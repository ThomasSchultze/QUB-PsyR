[
  {
    "objectID": "wrangling/wrangling1.html",
    "href": "wrangling/wrangling1.html",
    "title": "Extracting rows and columns from data frames",
    "section": "",
    "text": "The first step we will usually take after reading raw data in R is to extract the information we actually need for the analyses we are going to run on the data. Put differently, raw data usually has a lot of information we don’t really need and, thus, want to get rid off. This includes getting rid of cases we do not want to include in the data set but also variables we want to drop from the data.\nThe first question is: why should we remove cases or variables from the raw data, in the first place? There are several reasons why we may want to do so. It makes sense to look at dropping cases (rows of a data frame) and variables (columns of a data frame) separately.\nWe may want to drop cases from raw data if:\nLikewise, we may want to remove variables from the data frame if:\nIn theory, we already know how to drop rows and columns from a data frame, namely using logical or numerical indexing. While this is true, indexing using base R can become tedious if we have large data sets. The good thing is that there is an R package that makes our lives much easier: dplyr. The dplyr package is part of the tidyverse. We can install and load it separately, or we can install and load the whole tidyverse. If we do that latter, dplyr will be automatically installed and loaded because it is a dependency of the tidyverse package."
  },
  {
    "objectID": "wrangling/wrangling1.html#selecting-rows-with-dplyr",
    "href": "wrangling/wrangling1.html#selecting-rows-with-dplyr",
    "title": "Extracting rows and columns from data frames",
    "section": "Selecting rows with dplyr",
    "text": "Selecting rows with dplyr\nIn dyplr, we can select a subset of the rows of a data frame by using the function filter(). This function takes two arguments:\n\n.data (required): the name of a data frame we want to extract some rows from. Note the dot at the beginning of .data.\nan unnamed argument telling the function how to select the rows it should retain; a logical expression that will be tested, where TRUE means that the row is retained and FALSE means it will be dropped; we can use binary operators, functions, or a combination of the two to create the logical expression to be tested\n\nWhen creating the logical expression to be tested, we do not need to put variable names in quotation marks or refer to them using the $ operator. We can just write their name as is (exactly as we did when using ggplot2).\nLet’s make up some data (stored in df1) to demonstrate. We’ will create data for 100 participants of a fictional experiment. Here is an excerpt of the data:\n\n\n  ID gender age exp_finished manip_check condition dv1 dv2\n1  1   male  20         TRUE      passed treatment  52  47\n2  2 female  45         TRUE      failed   control  53  45\n3  3 female  31         TRUE      passed treatment  51  49\n4  4 female  28         TRUE      passed   control  54  55\n5  5 female  24         TRUE      passed treatment  52  50\n6  6 female  33         TRUE      passed   control  54  47\n\n\nWe can now use the filter() function to retain only some of the rows. Here are a few code examples:\n\n# create a subset of participants who passed the attention check and finished the experiment\ndf2 = filter(.data = df1, exp_finished == TRUE & manip_check == \"passed\")\n\n# extract a subset of df2 containing only participants who self-identify as male or female\ndf3a = filter(.data = df2, gender == 'male' | gender == 'female')\n\n# extract a subset of df2 containing data from participants with above-average scores of dv1\ndf3b = filter(.data = df2, dv1 &gt; mean(dv1))\n\nEach of the lines of code above creates a new data frame in our environment. While all data frames contain the same variables (i.e., we retain all columns, they will vary in the the number of rows).\n\nNote: Filtering the data with the filter() function from dplyr is quite similar to indexing using brackets, but the syntax is easier to parse. In particular, the function makes our life somewhat easier when combining several logical tests. The reason is that we do not need to to reference variables using the $ operator."
  },
  {
    "objectID": "wrangling/wrangling1.html#selecting-columns-with-dplyr",
    "href": "wrangling/wrangling1.html#selecting-columns-with-dplyr",
    "title": "Extracting rows and columns from data frames",
    "section": "Selecting columns with dplyr",
    "text": "Selecting columns with dplyr\nWhile selecting rows with dplyr is not too different from regular indexing, selecting columns with the select() function is where dyplr really shines. The reason is that select() is highly flexible. Similar to the filter() function, select() requires two arguments:\n\n.data (required): the data frame we want to select columns from.\nan unnamed argument telling the function how to select the rows it should retain.\n\nWe have several options to specify the unnamed argument of select(), which we will discuss below:\n\nSelecting variables by name\nIf we have specific variables in mind, we can just feed the select() function to name of the variable or variables. Again, we can pass the function the variable names as is (without quotation marks). In case, we want to select only one variable, we can simply pass the select() function this variable’s name as the second argument. If we want to select several variables, we need to pass their names separated by commas, or we can put them in a vector using the c() function. Finally, and this may remind you of indexing columns of a data frame using brackets, we can tell R to choose all but the specified variable or variables. we can do so by preceding the variable name or vector with either the - operator or the ! operator (logical “non”).\nExamples below:\n\n# extract participant gender from df2\ndf4a = select(.data = df2, gender)\n\n# extract condition, dv1, and dv2 from df2\ndf4b = select(.data = df2, c(condition, dv1, dv2))\n\n# extract all variables but the manipulation check from df2\ndf4c = select(.data = df2, !manip_check)\n\nEach of the lines of code above will create a new data frame, even if we only extract a single variable. Their number of variables will vary, but they will all have the same number of observations.\n\n\nSelecting variables using “from to”\nRemember that we used the : operator as an efficient (or lazy) way to tell R to create a sequence of number from \\(X\\) to \\(Y\\)? In dplyr we can do the same to select variables provided that we choose an uninterrupted section. All we need to do is pass the select() function the names of the first and the last variable in the sequence of variables we want to extract and combine them using the : operator. If we want to select multiple sequences, we can liste them separated by commas or combine them into a vector using the c() function. Of course, we can also use the - or ! operators to select all but a sequence of variables. The only thing to consider is that we need to put a single sequence of variables in parentheses if we want to exclude it. Examples as follows:\n\n# extract everything from participant ID to gender from df2\ndf5a = select(.data = df2, ID:gender)\n\n# extract ID and everything from condition, dv1, and dv2 from df2\ndf5b = select(.data = df2, ID, condition:dv2)\n\n# extract all variables but those from ID to the manipulation check from df2\ndf5c = select(.data = df2, !(ID:manip_check))\n\n\n\nSelecting variables using helper functions\nA very neat way to extract variables using the select() function is to specify the second argument using a helper function. Here, we will focus on semantic helper functions, that is, functions that allows us to select variables if their names satisfy certain conditions. For example, we can tell R to select all variables that names of which starts with a certain sequence. All semantic helper functions take a character value or a character vector as their function argument. The semantic helpers are listed below. As you will see, their names are self-explanatory:\n\nstarts_with: selects all variables the names of which start with the specified character string.\nends_with: selects all variables the names of which ends with the specified character string.\ncontains: selects all variables the names of which contain the specified character string.\n\nIf we feed the helper function a vector of character strings instead of a single character value, it will combine the selection criteria with a logical OR, that is, it will select all variables the names of which start with (or end with or contain) any of the specified character strings.\n\nThere is a forth semantic helper function called matches(). This function is very powerful because it lets us define so called regular expressions and then selects all variable the names of which match this expression. Regular expressions are a very flexible way of defining how a sequence of symbols must look like in order to be considered a match. For example, we can state that the variable name must consist of three letters followed by an underscore, which, in turn, is followed by two digits.\nWe will not cover regular expressions here because their syntax can be quite complex, but the interested reader is encouraged to research regular expressions on their own. There are several good guides available on-line.\n\nNow, why should we bother with semantic helpers, in the first place? Remember that most raw data comes in wide format? Wide format data can easily span hundreds of variables. Quite frequently, some of these variables will have similar (bot of course not identical) names. Consider, for example experiments, with multiple trials. There, the wide format data might contain variables such as “stimulus_X”, “reaction_time_X”, or “response_X” where \\(X\\) indicates the trial number. Alternatively, imagine a survey study where people respond to several personality questionnaires, where variable names consist of the questionnaire acronym and the item number. If we want to extract variables from such data sets, semantic helpers will we a gods-end.\nAs per usual, we can use semantic helper functions to drop variables by preceding the function call with a - or ! operator. Finally, it is worth noting that we can combine multiple semantic helpers helpers using the logical AND (&) and OR (|) operators. Below are a few examples of selecting columns with semantic helper functions:\n\n# extract all variables the names of which starts with \"d\" from df2\ndf6a = select(.data = df2, starts_with(\"d\"))\n\n# extract all variables the names of which end on either \"1\" or \"2\" from df2\ndf6b = select(.data = df2, ends_with(c(\"1\", \"2\")))\n\n# extract all variables the names of which contain the string \"dv\" or do not start with \"d\"\ndf6c = select(.data = df2, contains(\"dv\") | !starts_with('d'))"
  },
  {
    "objectID": "wrangling/wrangling1.html#extracting-rows-and-columns-in-one-go",
    "href": "wrangling/wrangling1.html#extracting-rows-and-columns-in-one-go",
    "title": "Extracting rows and columns from data frames",
    "section": "Extracting rows and columns in one go",
    "text": "Extracting rows and columns in one go\nWe can tell R to extract certain columns and rows of a data frame in one go, by combining the function calls for select() and filter(). We can call either function first. Let’s say we want to extract the experimental condition and the two dependent variables from our data frame, but restrict the data to female participants. Below are two ways to write the code:\n\n# extract the variables condition, dv1, and dv2 from df2, female participants only.\n# option A (select first):\ndf6a = select(.data = filter(.data = df2, gender == 'female'), condition:dv2)\n\n# option B (filter first):\ndf6b = filter(.data = select(.data = df2, c(gender, condition:dv2)), gender == 'female')\n\nAs we can see, the code differs slightly depending on which function we call first. If we use filter() first, we must include “gender” because we need this variable to select the rows. If we call select() first, there is no need to retain “gender”."
  },
  {
    "objectID": "wrangling/wrangling1.html#making-code-less-convoluted-using-pipes",
    "href": "wrangling/wrangling1.html#making-code-less-convoluted-using-pipes",
    "title": "Extracting rows and columns from data frames",
    "section": "Making code less convoluted using pipes",
    "text": "Making code less convoluted using pipes\nCombining multiple operations for data wrangling in one go can create code that is quite convoluted and, consequentially is hard to parse. The problem becomes more sever the more steps we include. However, we also don’t want to clog our environment with too many temporary data sets. Fortunately, dplyr comes with a special kind of syntax that can make our life easier by allowing us to combine multiple function calls in a more intuitive way. This syntax is called piping (originally, the pipes were contributed by the magrittr package, but if we load dplyr we can use them without magrittr).\nThe pipe is a special operator. It looks like this: %&gt;%. Using the pipe to separate functions calls allows us to call the functions in the order in which we want the respective operations to be performed. Importantly, it avoids code lines that end on a dozen closing parentheses. Think of it as telling R to take and object and first do A with it, then do B, then do C, and so on instead of telling R to do C with the outcome of doing B with the outcome of doing A with an object.\nTo illustrate, let’s go back to the most recent example where we combine the select() and filter() functions. Here is what the code would look like when we use pipes:\n\n## using pipes to organise code\n\n# extract the variables condition, dv1, and dv2 from df2, female participants only.\n# option A (select first):\ndf6a = df2 %&gt;% select(gender, condition:dv2) %&gt;% filter(gender == 'female')\n\n# option B (filter first):\ndf6b = df2 %&gt;% filter(gender == 'female') %&gt;% select(gender, condition:dv2)\n\nAs we can see, the code is a little less convoluted, and it is easier to spot what is being done in what order. The advantage of using pipes increases with the number of operations we combine. It is also worth mentioning that since piping changes the order of the function calls, we need to consider which variables we extract when using select(). In the original (convoluted) code, we had to retain “gender” when using filter() first. When using pipes, we need to retain “gender” only if we call select() first."
  },
  {
    "objectID": "working/working_x.html",
    "href": "working/working_x.html",
    "title": "Custom functions",
    "section": "",
    "text": "Sometimes, we may not find an R function that does exactly what we want it to do in base R, and we may also run out of luck when browsing additional R packages in search od that function. In those cases, we can resort to writing our own functions using a special function called function (yes, very creative, I know).\nWriting R functions is easy, once we understand how R functions work (and we do know that from looking at the documentation of different functions). What we need to do when defining a custom function is to: - give it a name (ideally a unique name to avoid conflicts with existing functions) - tell R what function arguments the function should have - tell R whether any of the arguments should have default values - tell R what operations it should perform when we call the function\nWhen defining the function, we need to use a specific Syntax. Just as with R objects, we write the name of our custom function to the left of an equal sign. To its right we call the function function. This function does not have fixed arguments that we specifiy in parentheses. Instead, we write those arguments in parentheses that we want our our custom function to have. If we would like there to be a default value for a function argument, we can use an equal sign following the argument name and enter the respective value to its right.\nFinally, we use braces to define what the function should do, that is, which operations to run on the arguments we fed it. Here, sky is the limit to our creativity. We can use any binary operator or function (even other custom functions) to tell R what it it should do. The final line of code within the braces should be a call of the function return, which we can use to define the output of our custom function. Let’s look at a few examples.\n\nExample 1\nIn the first example, we will write a custom function that computed the uncorrected standard deviation of a numeric vector (remember that the function sd that is built into base R computed the bias-corrected standard deviation, which may not always be what we are looking for). We will call this function sd_uncorrected (any name is fine, but it may be prudent to choose a name that is informative of what the function does).\nThe R code for defining this function could look as follows:\n\nsd_uncorrected = function(x, na.rm = TRUE){\n  \n  x = na.omit(x)  # excludes elements of x that are NA (if any)\n  n = length(x)   # sample size n is the length of the vector x\n  \n  var_x = 1/n * sum((x - mean(x))^2)  # compute uncorrected variance of x\n  sd = sqrt(var_x)                    # take square root to get sd\n  \n  return(sd)                          # return the object sd, which happens to be\n                                      # a single numeric value\n  \n}"
  },
  {
    "objectID": "working/working4.html",
    "href": "working/working4.html",
    "title": "Logical Indexing",
    "section": "",
    "text": "So far, we have learned how to obtain or overwrite parts of an R object using numerical indexing. We also know how to obtain a named element of a data frame or list, namely by using the $ operator. We will now turn to logical indexing, that is obtaining elements of objects that satisfy certain conditions.\nJust as with numerical indexing, logical indexing uses brackets. Within these brackets we can specify conditions that be be tested logically such that the result of the test is a TRUE or FALSE statement (i.e., a Boolean variable). R will then only show those elements of the object for which these conditions are true."
  },
  {
    "objectID": "working/working4.html#logical-indexing-using-binary-operators",
    "href": "working/working4.html#logical-indexing-using-binary-operators",
    "title": "Logical Indexing",
    "section": "Logical indexing using binary operators",
    "text": "Logical indexing using binary operators\nThe simplest form of logical indexing is to use a single statement involving a logical binary operator. For example, we could ask R to return to us all elements of a numeric vector (including, for example, a column of a data frame) that are greater then 3, or we could ask it to obtain all elements from a Boolean matrix that are TRUE.\nHere are a few examples:\n\n# create a numeric vector\nv1 = 1:6\n\n# create a Boolean 2x3 matrix\nm1 = matrix(\n  c(T, T, F, T, F, F),\n  nrow = 2\n)\n\n# obtain all elements of v1 that are greater than 3\nv1[v1 &gt; 3]\n\n# obtain all elements of m1 that are TRUE\nm1[m1 == T]\n\nHere is what the output in the console looks like:\n\n\n\n[1] 4 5 6\n\n\n[1] TRUE TRUE TRUE\n\n\n\nWe can also obtain elements by combining multiple logical statements using R’s AND and OR operators (& and |, respectively). For example, we could obtain all elements of the numeric vector we created above that exceed 2 AND are smaller than 5.\nIf we combine multiple logical tests in logical indexing, it is important that we tell R in each of the involved tests which variable to test. The following code would not work even though it may seem clear to us what it should mean:\n\n# obtain all elements of v1 that are greater than 2 and smaller than 5\nv1[v1 &gt; 2 & &lt; 5]\n\nWhile it looks as if we asked R to show us all elements that are greater than 2 AND smaller than 5, R will not know which variable the second part of the test refers to and will return an error message (in this message, R will tell us that the &lt; sign following the & operator was unexpected).\nHere is how the code needs to look like if we want to obtain elements of an object using a combination of multiple logical statements:\n\n# obtain all elements of v1 that are greater than 2 and smaller than 5\nv1[v1 &gt; 2 & v1 &lt; 5]\n\nThis code works as intended, which we can verify by looking at the output in the console.\n\n\n\n[1] 3 4\n\n\n\nOne neat thing about logical indexing is that we are not restricted to referring to the variable we want obtain elements from. As long as the outcome of the logical tests is an object of the same size as the object we want to obtain elements from, the code will work.\nWhat this means is that we can obtain all elements of an object that satisfy conditions in other equally sized objects. This is particularly useful when we want to obtain elements of a data frame. For example, we might want R to show us all elements of one variable that satisfy the condition that another variable equal a specific value. Let’s look at a few specific examples.\n\n# create a data frame containing some demographic variables and responses to three 5-point Likert-scale items\nmy_df = data.frame(\n  ID = 1:6,\n  age = c(23, 21, 18, 16, 19, 17),\n  gender = c('f', 'f', 'm', 'm', 'f', 'f'),\n  item1 = c(2, 5, 5, 4, 5, 3),\n  item2 = c(1, 2, 1, 3, 2, 1),\n  item3 = c(3, 4, 5, 5, 4, 2)\n)\n\n# obtain the responses to item 2 of all participants who are adults\nmy_df$item2[my_df$age &gt;= 18]\n\n# obtain the gender of all participants who scored below 4 on items 1 and 3\nmy_df$gender[my_df$item1 &lt; 4 & my_df$item3 &lt; 4]\n\nHere is what the output in the console looks like:\n\n\n[1] 1 2 1 2\n\n\n[1] \"f\" \"f\"\n\n\nJust as with numerical indexing, we can also use logical indexing to save the obtained elements as a new object of their own, or we can overwrite the obtained elements. Let’s focus on overwriting. Overwriting of values using logical indexing is particularly useful in situations, in which missing data is coded using specific numbers (e.g., -99).\nNumeric values for missing data can cause problems if we do not detect it such as distorting some measures of central tendency (e.g., the mean) or variability (e.g., standard deviation, range). Ideally, we want missing data to be represented by NA when working with R. That means that we may end up having to replace certain values that code missing data with NAs manually. Logical indexing makes this a walk in the park as the following example shows.\n\n# add two more items to the data frame created above, both of which contain missing data coded as -99\nmy_df$item4 = c(3, 5, 3, -99, 4, -99)\nmy_df$item5 = c(-99, 2, 3, 1, -99, 2)\n\n# turn all elements that are equal to -99 into NAs\n# variant A:\nmy_df$item4[my_df$item4 == -99] = NA\nmy_df$item5[my_df$item5 == -99] = NA\n\n# variant B:\nmy_df[my_df == -99] = NA\n\nBoth ways of overwriting the -99s with NAs work. The second one is more parsimonious. However, replacing all elements of a certain value in a data frame can become messy in large data sets. If the code for missing data has been chosen poorly (e.g., 99), there may be some variables for which 999 is an actual and valid value (e.g., a reaction time in milliseconds). Therefore, caution is advised when replacing values for a complete data frame."
  },
  {
    "objectID": "working/working4.html#logical-indexing-using-functions",
    "href": "working/working4.html#logical-indexing-using-functions",
    "title": "Logical Indexing",
    "section": "Logical indexing using functions",
    "text": "Logical indexing using functions\nInstead of - or in addition to - binary operators, we can also use functions for logical indexing. We can use functions in two ways here. First, we can use the output of a function call as a value of comparison. For example, rather than testing whether an element of an object exceeds a certain fixed value, we could test whether it exceeds the mean or the median of that object.\nSecond, there a several functions that conduct logical tests. For example, the function is.na() tests whether an object is NA, the function is.numeric() tests whether an object’s type is numeric (either integer or double), and the function is.finite() tests whether a numeric variable has a finite value.\n\nNote: R has a neat way of reversing functions such as is.na(), is.numeric(), and is.finite(). We can make R test the respective opposite by preceding the function call with an exclamation mark (similar to how the operator != mean not equal).\nFor example, calling !is.na() tests whether an object is not missing, !is.numeric() tests whether the object’s type is different from numeric, and !is.finite() tests whether a numeric value is infinite (or Inf in R terms).\n\nLet’s look at a few examples of logical indexing using functions. We will do so with the data frame we created and modified above. Here is a quick update on how the data frame looks currently.\n\n\n\n  ID age gender item1 item2 item3 item4 item5\n1  1  23      f     2     1     3     3    NA\n2  2  21      f     5     2     4     5     2\n3  3  18      m     5     1     5     3     3\n4  4  16      m     4     3     5    NA     1\n5  5  19      f     5     2     4     4    NA\n6  6  17      f     3     1     2    NA     2\n\n\n\nWe will now obtain and overwrite some of the elements of this data frame using the is.na() function and its inverse, !is.na(). Note that the complete function call including the opening and closing parentheses must be contained within the brackets.\n\n# obtain the age of all participants whose responded to item4\nmy_df$age[!is.na(my_df$item4)]\n\n# replace missing responses to items 4 with the average response (i.e., the mean of valid responses to that item across participants) \nmy_df$item5[is.na(my_df$item5)] = mean(my_df$item5, na.rm = T)\n\nRunning the first line of code above, yields the following output in the console:\n\n\n\n[1] 23 21 18 19\n\n\n\nAnd here is what the data frame looks like after we replaced the missing responses with the average response.\n\n\n\n  ID age gender item1 item2 item3 item4 item5\n1  1  23      f     2     1     3  3.00     2\n2  2  21      f     5     2     4  5.00     2\n3  3  18      m     5     1     5  3.00     3\n4  4  16      m     4     3     5  3.75     1\n5  5  19      f     5     2     4  4.00     2\n6  6  17      f     3     1     2  3.75     2\n\n\n\n\nNote how R now displays two decimals for item4. It does not display decimals for the other items, even though their object type is double - just as that of item4 (we can find out the object types by calling the function typeof() and feeding it the column of interest as the sole function argument).\nR generally only uses as many decimals as it has to. If a double type object contains only whole numbers, then R will simply omit the decimals."
  },
  {
    "objectID": "working/working2.html",
    "href": "working/working2.html",
    "title": "Functions",
    "section": "",
    "text": "Most of the things we will do when using R will be done using functions. We can think of a function as an (often) elaborate sequence of simple operations. As such, they can involve more than two objects. However, functions are not only more elaborate than operations using a binary operator; they are also more flexible, because we can modify what a function does or how it does it via its additional function arguments."
  },
  {
    "objectID": "working/working2.html#the-basic-setup-of-r-functions",
    "href": "working/working2.html#the-basic-setup-of-r-functions",
    "title": "Functions",
    "section": "The basic setup of R functions",
    "text": "The basic setup of R functions\nAll R function share the same set-up. The function has a unique name by which we can call it, and it has a number of function arguments that we need to specify in parentheses following the function name when calling the function.\nFunction arguments can be objects that we would like to feed to the function or parameters of the function that influence what the function does. Some of a function’s arguments are required arguments, that is, if we do not specify these arguments, the functions won’t run. Instead, R will complain by printing an error message telling us that we did not specify a required function argument.\nA function may also entail optional function arguments. Technically, these arguments are also required, but the person who wrote the function defined default values for the arguments. If a function argument has a default value, we do not need to specify it in order for the function to run. The function will simply run as if we had entered the respective argument’s default value manually.\n\nOccasionally, we might come across functions that have no arguments at all. These functions are quite rare. We can can them by writing nothing in the parentheses following the function’s name."
  },
  {
    "objectID": "working/working2.html#the-most-useful-function-in-r---help",
    "href": "working/working2.html#the-most-useful-function-in-r---help",
    "title": "Functions",
    "section": "The most useful function in R - help",
    "text": "The most useful function in R - help\nAll functions in R are documented in order to help users understand what the function does, what its arguments are and what they do. We can ask R to show us the documentation using the function help(). If we call this function and feed it the name of an R function as a function argument, R will display the documentation in the help tab of the utility & help section (bottom right of RStudio’s interface).\nNote that the function help() will accept the function name both if we enter it as is or if we enter it as a character string (i.e., we write it in quotation marks).\nAs an alternative to calling the function’s name in regular R syntax, we can type the functions name in the search bar of the help tab.If we use this way to learn about a function, we do not need quotation marks.\nOnce we call the help function, R will show us the documentation of an existing R function or a group of related functions. R will generally display a lot of information. What information is displayed exactly depends on the function.\nThe following information will be shown for all functions:\n\nthe name of the function\nwhat R package it is from\na description of the the function does\nhow to call the function (usage)\nthe functions’ arguments\na description of the type and format of the function’s output (value)\nfunctioning R code showing examples of how the function can be used\n\nThere may be additional information for some functions such as: - a description of the maths or logic underlying the function (details) - additional information of potential interest to the user (note) - references to literature the literature the function is based on - links to related functions (see also)\nLet’s look at an example by asking R to show us the documentation for the function median() using R syntax.\n\nhelp(median)\n\nR will now show us the following in the help tab.\n\n\n\nFig 1. Documentation for the function median\n\n\nLet’s first look at the description. Unsurprisingly, it states that this function computes the sample median.\nOne of most relevant sections of the documentation is the function’s usage. As we can see, the function median() has two function arguments, x and na.rm. The second argument, na.rm has a default value, indicated by the equal sign and a specific value (FALSE) to its right. If we do not specify na.rm, R will use the value FALSE as a default. Since the argument x has no default, we must specify it. Otherwise, the function won’t run.\nThe other highly relevant section is the arguments section. It tells us what the function arguments do and values are acceptable for each of the two function arguments. The argument x must be a numeric vector (we can ignore the first part of the sentence referring quite obscurely to ‘an object for which a method has been defined’). This is the set of numbers for which we will compute the median. The second function argument, na.rm, defines how missing values (represented by NA, meaning “not available”) should be handled. The function will not work when our numeric vector x contains at least one NA value. By setting na.rm to TRUE, we can tell R to remove the NA values prior to computing the median.\nFinally, let’s inspect the value section. From what the function is supposed to do, we would expect it to return a single numeric value. The documentation tells us that this value can be a double type value when the vector is of even length (in this case, the median is the mean of the two centre most values of x). It also informs us that the output of the function will be NA if x is either an empty vector of if x contains NA values while na.rm is set to FALSE.\n\nAt the top left of the documentation, R states in braces following the function’s name which R package it stems from. In case of the function median(), the package it belongs to is called stats. This package along with base is built into core R.\nCaveat: If a function does not belong to one of R’s built-in packages, R’s help() will not know the function unless the respective package is loaded (see the section on installing and loading additional packages for more information)."
  },
  {
    "objectID": "working/working2.html#finding-functions-if-their-name-is-unknown",
    "href": "working/working2.html#finding-functions-if-their-name-is-unknown",
    "title": "Functions",
    "section": "Finding functions if their name is unknown",
    "text": "Finding functions if their name is unknown\nSometimes, we might be looking for a function to perform a specific operation, but we do not know whether R has such a function and, if so, how it is called in R. In such situations, we can make use of the search bar on the top right of the help tab. If we input the term we are interested in there, R will return a list of related functions in the help tab.\nLet’s assume, for example, that we are looking for a function that computes the standard deviation of a numeric vector. Entering the term “standard deviation” in the search bar, yields the following output:\n\n\n\nFig 2. Search results for the term “standard deviation”\n\n\nAs we can see, R links to several help pages, each of which contains the documentation for a function that R thinks to relate to our search term. R displays the results of the search in a special format, namely package_name::function_name. For example, the first result of the search is the function devfun2() of the package lme4. Klicking on any of the links our search returned is equivalent to using the help() function for the respective function.\n\nNote that unlike the regular help() function, using the search bar is not restricted to currently loaded R packages. Any package that is installed on the computer will be included in the search. This handy feature is designed to maximize the chance of finding the function we are looking for.\n\nIn the example above, we can see that there is a function called sd() in the stats package that is part of basic R. The brief description of this function looks promising enough to warrant klicking on the function’s name. The documentation for the function that R now displays confirms that this is the function we are looking for (see below).\n\n\n\nFig 3. Documentation for the function sd\n\n\nAs we can see from the documentations usage and arguments sections, the function sd() requires a numeric vector x as a function argument. It has a second optional argument we are already familiar with, namely na.rm, which allows us to exclude NA values prior to computing the standard deviation.\nLet’s have a look at the details section of the function sd(). This section contains an important piece of information, namely that the function uses the denominator \\(n-1\\). What this means is that the function sd() computes an unbiased estimate of the population standard deviation by multiplying the (uncorrected) sample variance by \\(\\frac{n}{n-1}\\) prior to taking its square root.\nAn alternative to using the search bar of the help tab is to use R code to search for functions. We can do so using a special operator ?? followed by the term we are searching for. If our term consists of more than one word, we need to put it in quotation marks (putting a single word in quotation marks won’t do any harm).\nThe syntax looks like this:\n\n??\"standard deviation\"\n\nWe can verify that searching for our term of interest using R syntax yields the same results in the help tab.\n\nCaveat: There is one situation in which using the search bar of the help tab and using the operator ?? will yield different results. If the search term matches the name of an R function, either from base R or from a currently loaded R package, the help tab will always display the documentation for that function instead of providing us with a list of potentially relevant functions. Using the ?? operator will always display the list of results.\n\n\nIf our search for the desired function is not successful, Google (or any non-evil alternative search engine) is our friend. R has a very active and supportive community, and there are extensive resources for R users on-line."
  },
  {
    "objectID": "working/working2.html#combining-several-functions",
    "href": "working/working2.html#combining-several-functions",
    "title": "Functions",
    "section": "Combining several functions",
    "text": "Combining several functions\nWe will frequently find ourselves in situations, in which the operations we want to perform require calling not just one function but a sequence of functions.\nConsider, for example, a case, in which we want to create a list type object that contains descriptive statistics for two variables we observed. The first element of that list will contain the variables’ means while the second will contain their variance-covariance matrix.\nCreating the first element of that list is simple as it requires only computing the means whereas creating th 2x2 matrix requires computing the variances of the two variables as well as their covariance and then putting them in a matrix format.\nThe first way to go about this is to write R code such that we call each function separately and save its output as a new object. We then use this object as a function argument in the next function we need to call. Another possibility is to use a function call as the argument for the next function. Lets have a look at these two options.\n\nCalling functions separately.\n\nv1 = c(2, 3, 5, 7, 9, 11, 13)     # data for variable 1\nv2 = c(1, 1, 2, 3, 5, 8, 13)      # data for variable 2\n\n# compute means for v1 and v2 using the function 'mean'\nmean1 = mean(v1)\nmean2 = mean(v2) \n\n# create a vector of the means using the function 'c'\nmean_vector = c(mean1, mean2)\n\n# compute variances using the function 'var'\nvar1 = var(v1)\nvar2 = var(v2)   \n\n# compute the covariance using the function 'cov'\ncov12 = cov(v1, v2)\n\n# create the variance-covariance matrix using the\n# functions 'matrix' and 'c'\nvc_vector = c(var1, cov12, cov12, var2)\nvc_mat = matrix(vc_vector, nrow = 2)\n\n# now create the list using the function 'list'\nmy_list = list(means = mean_vector,\n               var_cov_matrix = vc_mat)\n\nWe can now have a look at the object we crated by calling the name of our newly defined list.\n\nmy_list\n\n\n\n\n$means\n[1] 7.142857 4.714286\n\n$var_cov_matrix\n         [,1]     [,2]\n[1,] 16.80952 17.04762\n[2,] 17.04762 19.57143\n\n\n\nAs we can see, the objects looks exactly as it should, and it contains the information we want it to contain. There is, however, a drawback to the piece-by-piece approach, we took in creating the list. It becomes apparent when we look at the Environment tab of the Memory section (top right).\n\n\n\nFig 1. Clogged Environment\n\n\nSince we defined the output of each function as an object of its own, our Environment contains not only the list we wanted to create, but also ten additional objects.Two of them are the original data, which we may want to keep, but the other eight objects are no longer required. Eight superfluous objects may not sound too much, but if we work on more complex tasks in R, the number of superfluous objects may increase rapidly.\nOne solution to solve the problem is to clean up afterwards using the function rm(). This function takes an arbitrary number of R objects as function arguments. It removes all listed arguments from the Environment. Here is what the code looks like in our example.\n\nrm(mean1, mean2, mean_vector,\n   var1, var2, cov12,\n   vc_vector, vc_mat)\n\nAfter we run this code, the Environment looks much tidier. It now contains only the original data vectors v1 and v2 as well as the list containing the descriptive statistics.\n\n\n\nFig 2. Unclogged Environment\n\n\n\n\nCalling multiple functions in one go\nAn alternative to calling each function separately and defining interim objects is to use function calls as arguments for other functions. In theory, it is possible to write one(very long) line of code to create our list of descriptive statistics from the two data vectors v1 and v2. Here is what the code would look like.\n\nv1 = c(2, 3, 5, 7, 9, 11, 13)     # data for variable 1\nv2 = c(1, 1, 2, 3, 5, 8, 13)      # data for variable 2\n\n# create the list using the function 'list'\nmy_list = list(\n  # create the first element of the list using the \n  # functions 'c' and 'mean'\n  means = c(mean(v1), mean(v2)),\n  \n  # create the second element using the functions \n  # 'matrix', 'c', 'var', and 'cov'\n  var_cov_matrix = matrix(\n    c(var(v1), cov(v1, v2), cov(v1, v2), var(v2)),\n    nrow = 2)\n)\n\nIn the example above, the R code is spread out between multiple lines for the purpose of better readability. We could have actually put it all into one line. If we now look at the Environment, we will only see the two original data vectors and the list we created. No interim objects were created, and there is not need to clean up afterwards.\nNote also how the code starts with the last step of the step-by-step approach. That is, if we use function calls as arguments for other functions, we need to think from the end to the start.\nOne downside of writing code containing convoluted function calls is that the code may become more difficult to parse for others or even for ourselves (especially if we revisit a script we wrote a while ago). For example, if we combine too many function calls, we may end up with a line of code that has a dozen closing brackets at its end. This is not only hard to read, but it may also lead to bugs in our code when we miss a closing bracket or include too many of them.\n\nNote: Either of the two ways of writing code shown above is viable. However, it is often advisable to choose a middle ground, where we define interim objects whenever the code would otherwise become to convoluted.\nAs always, readability of the code greatly benefits from making ample use of commenting."
  },
  {
    "objectID": "intro/intro5.html",
    "href": "intro/intro5.html",
    "title": "Saving and loading data",
    "section": "",
    "text": "At some point, we either want to load data into R or save a data frame that we have created or modified. Arguably, the best format to store data in tabular form (rows representing observations and columns representing variables) is the CSV (comma-separated variables) format.\nCSV files are ideal for Open Data as most software can read them, including generic text editors.\nIn order to load and save data in CSV format, we can use two functions:"
  },
  {
    "objectID": "intro/intro5.html#saving-data-frames",
    "href": "intro/intro5.html#saving-data-frames",
    "title": "Saving and loading data",
    "section": "Saving data frames",
    "text": "Saving data frames\nIf we save a data frame using the function write.csv(), we should specify three function arguments:\n\nx: the name of the object we want to save (a data frame, but works with matrices, too)\nfile: the directory where we want to save the file plus the file name as a character string\nrow.names: whether we want R to assign row names (default is TRUE, but we want to turn that to FALSE)\n\nIf we do not specify a directory, R will save the data frame in the current working directory.\nWe can ask R to tell us the current working directory using the function getwd(). This function is special in that it does not have any function arguments. Here is what the syntax looks like:\n\n# Obtain the current working directory\ngetwd()\n\nThe output in the console will look slightly different for each person due to differences in how their folders are named:\n\n\n\n[1] \"C:/Users/Thomas/Documents/R stuff\"\n\n\n\nHere is how the code for saving a data frame could look like.\n\n# save the object called my_data_frame\nwrite.csv(\n  x = my_data_frame,\n  file = \"C:/Users/Thomas/Documents/data.csv\",\n  row.names = FALSE\n)\n\nProvided the directory “C:/Users/Thomas/Documents” already exists on the computer, there will now be a file named “data.csv” in that directory."
  },
  {
    "objectID": "intro/intro5.html#loading-data-frames",
    "href": "intro/intro5.html#loading-data-frames",
    "title": "Saving and loading data",
    "section": "Loading data frames",
    "text": "Loading data frames\nWe can load a data frame (or matrix) saved in CSV format using the function read.csv().\nWe usually need to specify only two function arguments:\n\nfile: the directory and file name of the CSV file\nheader: a Boolean variable indicating whether the first row of file contains the variable names (default is TRUE)\n\n\nImportant: If we simply call the function, R will display the data frame we loaded in the console. If we want it to appear in the Environment we need to define it as a new object.\n\nHere is what the code would look like if we were to load the data frame saved in the previous example.\n\n# load data\nmy_data_frame = read.csv(\n  file = \"C:/Users/Thomas/Documents/data.csv\",\n  header = T)\n\nAssuming that the requested file exists in the specified directory, the Environment will now show the object called “my_data_frame” in the Environment."
  },
  {
    "objectID": "intro/intro3.html",
    "href": "intro/intro3.html",
    "title": "Objects and functions",
    "section": "",
    "text": "R revolves around objects and functions. Generally speaking, an object is a container for information, and a function is a piece of code that performs a specific task. We often use functions to manipulate or create objects."
  },
  {
    "objectID": "intro/intro3.html#r-objects",
    "href": "intro/intro3.html#r-objects",
    "title": "Objects and functions",
    "section": "R objects",
    "text": "R objects\nIn R, an object is information that we store in R’s memory. In order to define an R object, we need to choose a name and tell R what information the object should contain. The general syntax for defining objects looks like this:\n\nobject_name = information\n\nWe write the name of our new object to the left of an equal sign and define the information the object should contain to its right. Instead of using an equal sign to define an object, we can also draw an arrow using the less than sign and a hyphen, which looks like this:\n\nobject_name &lt;- information\n\n\nUsing the arrow instead of an equal sign dates back to an old programming language that R is derived from (APL). Whether you define objects using arrows or equal signs has no effect on what R does. It is simply a matter of preference.\n\nAs soon as we have defined an object (and assuming that we did everything correctly), our new object will appear in the Environment tab of RStudio’s Memory section (top right panel).\nWe will now cover the most important of the basic R objects, namely:\n\nsingle values\nvectors\nmatrices\n\n\nWe will later turn toward two of the more complex R objects that we are very likely to encounter frequently when working with R:\n\ndata frames\nlists"
  },
  {
    "objectID": "intro/intro3.html#functions-in-r",
    "href": "intro/intro3.html#functions-in-r",
    "title": "Objects and functions",
    "section": "Functions in R",
    "text": "Functions in R\nR functions are predefined pieces of code that we can call by writing the function name and telling R a number of function arguments. Function arguments are pieces of information a function needs to know in order to perform its task (very rarely, a function may not need any argument to perform its task). The general syntax for calling a function looks like this:\n\nfunction_name(argument1, argument2, ...)\n\nThat is, we first write the function’s name, and then define all required arguments in parentheses. If we do that correctly, the function will perform its task. Depending on the function, we will see an output in the console.\nWe will be using two functions below:\n\nThe function c(), which we will use to create vectors. The arguments the function c() requires are the elements we want to combine into a vector.\nThe function matrix(), which creates a matrix from a vector. This functions requires a vector, the number of rows and the number of columns as arguments.\n\n\nCore fact 1: Functions can have a lot of arguments, but it is not always necessary to specify all of them. Most functions have default values for some of their arguments. If we do not specify these arguments, R will just run the function as if we had entered the default value of the argument. If we do specify arguments with default values, we simply override the default.\n\n\nCore fact 2: It is impossible (for most people at least) to remember all functions and which arguments they require. Therefore, R contains a lifesaver in the form of a special function called help(). The function help() requires the name of a topic (including but not limited to functions) as its main argument. Calling help() with the name of a function will show the documentation for that function in the help tab of the Utility & Help section of RStudio’s interface (bottom right)."
  },
  {
    "objectID": "intro/intro3.html#object-types",
    "href": "intro/intro3.html#object-types",
    "title": "Objects and functions",
    "section": "Object types",
    "text": "Object types\nSingle values, vectors, and matrices can vary by type. The type of these objects refers to the nature of its content, for example whether it contains numbers, character strings, logical arguments etc. The most common types of types are:\n\ndouble: this means numbers with decimal points (although the decimals must not necessarily be displayed)\ninteger: whole numbers that can be negative positive or zero (rarely used because double type numbers are more practical)\ncharacter: a character string, which is essentially a sequence of text symbols (these symbols can include numbers, but these are interpreted as being part of the text and not as numbers)\nlogical: a logical TRUE or FALSE, also called a boolean value"
  },
  {
    "objectID": "intro/intro3.html#single-values",
    "href": "intro/intro3.html#single-values",
    "title": "Objects and functions",
    "section": "Single values",
    "text": "Single values\nSingle values are the simplest type of objects we can define in R. As their name suggests, they represent a single value, for example, the number 7. We could tell R to create a single value called \\(a\\) that consists of the number 7. We do this as follows:\n\na = 7\n\nAlternatively, we can use the arrow notation to define the object.\n\na &lt;- 7\n\nOnce we run either line of code, we should notice that our Environment now contains the object a. There, we can also see that a is the number 7.\nWe can make R show us an object by running its name as code (either via the console or a script). If we enter the name of our single value a as code, we will see the following output.\n\n\n\n[1] 7\n\n\n\nR tells us “7”, which is exactly the value that we specified as a. The “[1]” in front of the value “7” is not important at this point. It merely tells us that the “7” is the first (and only) element of our single value."
  },
  {
    "objectID": "intro/intro3.html#vectors",
    "href": "intro/intro3.html#vectors",
    "title": "Objects and functions",
    "section": "Vectors",
    "text": "Vectors\nVectors are objects that contain multiple values of the same type. Vectors cannot contain elements of different types. We can define vectors using the function c() with the function’s arguments being the values we want to combine into the respective vector. The syntax looks as follow:\n\nv1 = c(1,2,3)   # This line of code creates a double vector \n                # called \"v1\" containing the numbers 1 to 3.\n\nv2 = c('hello', 'purple', '11!')  # This creates a character vector\n                                  # called \"v2\" containing three strings.\n\nAfter executing this code, our Environment contains two more objects, v1 and v2. The vector v1 is denoted as a numeric vector (num) whereas v2 is labeled as a character vector (chr).\nJust as with the single value a, we can have a look at our vectors by entering their names as code. For example, we can enter v2 as code.\n\n\n\n[1] \"hello\"  \"purple\" \"11!\"   \n\n\n\nWe now see the three character strings that v2 consists of in the console. Again, there is a [1] at the front of the line. It indicates that “hello” is the first element of v2. If the vector was long enough to occupy multiple rows in the console, each line would start with a number in brackets, indicating which element of the vector is displayed at the beginning of that line.\n\nFun fact: We can think of single values as vectors of length 1.\n\n\nCaveat: If we try to create a vector containing different types of elements, R will not complain (i.e., we do not get an error message)! Instead, R will simply transform some of the elements to ensure that all elements share the same type.\nFor example, if at least one of the elements we want to include in the vector is a character string, R will transform all other element types to character strings and create a character vector.\nIf we try to combine numeric (double or integer) and logical values into one vector, R will interpret “FALSE” as 0 and “TRUE” as 1."
  },
  {
    "objectID": "intro/intro3.html#matrices",
    "href": "intro/intro3.html#matrices",
    "title": "Objects and functions",
    "section": "Matrices",
    "text": "Matrices\nMatrices are the two-dimensional cousins of vectors. They have \\(r\\) rows and \\(c\\) columns. Per convention, the number of rows is stated before the number of columns, that is, a 4x3 matrix has 4 rows and 3 columns (think roman-catholic as a mnemonic aid).\nAs with vectors, all elements of a matrix must share the same type (if we try to combine different types of elements, R will just transform some of the elements to create a homogeneous matrix). In fact, matrices in R are vectors that we break down into rows and columns. To create a matrix, we need to call the function matrix() and tell R three arguments: the vector we want to make into a matrix, the number of its rows, and the number of its columns. The syntax looks as follows:\n\nmatrix1 = matrix( # this tells R that we want to create a matrix\n  c(1,2,3,4),     # defines the vector that will turn into a matrix\n  nrow = 2,       # this tells R that our matrix should have 2 rows \n  ncol = 2)       # this tells R that our matrix should have 2 columns\n\nExecuting the code above creates a numerical matrix in our Environment. Other than with single values and vectors, we can click on the name of our matrix in the Environment to view how it looks like. Alternatively, we can enter the name of our matrix as code and have R print it in the console.\n\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\n\n\nR is convenient in the sense that it is technically sufficient to specify either the number of rows or the number of columns. R will just infer the missing piece of information. For example, if we want to turn a vector of length 6 into a matrix with 2 columns, R will know that the matrix must have 3 rows."
  },
  {
    "objectID": "intro/intro3.html#naming-objects-in-r",
    "href": "intro/intro3.html#naming-objects-in-r",
    "title": "Objects and functions",
    "section": "Naming objects in R",
    "text": "Naming objects in R\nIn order to define an object in R, we need to give it a name and then tell R how the object should look like. Generally speaking, we can get very creative when naming our objects, but there are a few rules we need to keep in mind.\n\nObject names must not start with numbers. The name “item2” is valid, but “2nd” is not (it will return a error message complaining about an “unexpected symbol”).\nObject names may not contain special characters with the exception of underscores and periods. For example, the names “item_2” or “item.2” are valid. Using other special character will often yield error messages because R interprets these characters as operators (e.g., “*” indicates multiplication, “&” is a logical conjunction, etc.).\nObject names must not contain spaces. While “item2” is a valid name, “item 2” is not.\n\n\nNot being allowed to use spaces in object names might prevent us from assigning meaningful and intelligible object names. To deal with the issue, there are three common types of notation:\n\nperiod notation: place periods where you would have placed a space, e.g. trial.number\nunderscore notation: replace spaces with underscores, e.g. trial_number\ncamelBack notation: instead of spaces write everything as one word, but write the beginning of new words with a capital letter, e.g. trialNumber\n\nWhich notation you use is purely a matter of preference. You will likely encounter all of them when using R because the people who created base R or additional packages each have their own preferences regarding notation."
  },
  {
    "objectID": "intro/intro1.html",
    "href": "intro/intro1.html",
    "title": "R and RStudio",
    "section": "",
    "text": "R is a software for statistical analysis (among others), but we will not work with R directly. Instead, we will be using R via a software called Rstudio, which serves as a user interface. While there is nothing wrong with using R on its own, RStudio makes the life of the users easier."
  },
  {
    "objectID": "intro/intro1.html#understanding-the-rstudio-interface",
    "href": "intro/intro1.html#understanding-the-rstudio-interface",
    "title": "R and RStudio",
    "section": "Understanding the RStudio interface",
    "text": "Understanding the RStudio interface\nYou have got the latest version of R and RStudio on your computer and are eager to get things going? Great!\nIf you open R Studio for the first time, it will look like this:\n\n\n\nFig 1. RStudio interace when opening for the first time\n\n\nThe interface has three parts, the R console, the memory, and the utility and help area. Once you open your first R script (you can do do by clicking on “File”, selecting “New File”, and choosing “R Script”), a fourth (and probably the most important) appears in the top left, the script area (note; when you close all scripts, the script area will disappear again until a new script is opened).\n\n\n\nFig 2. RStudio interface when at least one script is open"
  },
  {
    "objectID": "intro/intro1.html#the-rstudio-interface-components",
    "href": "intro/intro1.html#the-rstudio-interface-components",
    "title": "R and RStudio",
    "section": "The RStudio interface components",
    "text": "The RStudio interface components\n\nThe console\nThe console is the direct link between RStudio and R. It executes the R code we enter, either manually or by running (part of) a script. With the exception of plots, the output of the code we entered, is also displayed in the console. We will be using the console mostly in the early states of getting used to working with R. We can execute code in the console by typing it in and confirming it via the “Enter” key. Doing so will directly produce the result of the code we entered (if we did it correctly) or an error message (if we managed to botch it up).\n\n\nThe memory\nThe memory area is one of the many advantages of using RStudio as compared to R only. It has different subsections. The subsection labeled Environment displays all the objects that are currently in R’s memory. Examples of such objects include variables we defined, data we read into R, or models we fit on our data. The list of objects in the Environment is rather handy because we do not need to memorize the names of all our objects, and we can even have a closer look at our more complex objects (e.g., we can have a look at the individual variables in our data object).\nThe second subsection is the History of our R session. The history contains all the code we sent to the console (irrespective of whether we did it by manually entering code in the console or by running a script). Using the History is handy if we want to trace back what we did, for example to search for errors.\n\n\nUtility and Help\nThe Utility and Help area located on the bottom right has five subsections, which can be tremendously helpful, so let’s have a closer look at them.\n\nFiles: This section displays the content of the current working directory. We can use it like the windows explorer to search for and load files.\nPlots: If we create plots, they will be displayed in this subsection.\nPackages: The base version of R comes with a wide range of functions that we can use to handle and analyse data. However, we will frequently want to do things that base R cannot do. In those situations, we will require additional R packages. The Packages section shows, which packages are installed on our computer, and which ones are currently active (more on that later).\nHelp: The Help section is very important. We can use it to obtain information on all functions and objects contained in base R and all currently active packages.\nViewer: We will rarely be using the Viewer when starting to learn R. It can be used to display local html-files that we can create using R.\n\n\n\nThe Script\nThe Script area in the top left of the interface is where we do most of our actual work. Here, we can create, save, load, and modify R scripts. If we open more than one script, each one will have its own tab displaying the name of the script (or labeling it as “unnamed” if we have not saved it yet). Clicking on the tabs lets us switch between scripts."
  },
  {
    "objectID": "inference/inference8.html",
    "href": "inference/inference8.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "So far,w e have considered linear regression models with a single predictor. We will now turn to cases with two or more predictors. As soon as a regression model has more then one predictor, we speak of multiple regression.\nMultiple regression is a straightforward extension of the simple linear regression. Generally speaking the regression model looks like this:\n\\[Y = b_0 + b_1X_1+b_2X_2+...+b_kX_k +\\epsilon\\] Here, \\(b_0\\) is the intercept, \\(X_1\\) to \\(X_k\\) are the predictors, and \\(b_1\\) to \\(b_k\\) are their respective regression weights. As always, \\(\\epsilon\\) is the residual. We can test each parameter of our model for significance via a \\(t\\)-test:\n\\[\\frac{b_i - \\beta_i}{SE_{b_i}} \\sim t_{N-k}\\]\nJust as in simple linear regression, we compute the difference between the observed regression parameter and its expected value under \\(H_0\\) and divide this difference by the parameters’ standard error \\(SE_{b_i}\\). The resulting variable follows a \\(t\\)-distribution with \\(N-k\\) degrees of freedom where \\(k\\) is the number of parameters.\nJust as in simple linear regression, \\(\\hat{Y}\\) is the prediction of our model:\n\\[\\hat{Y} = b_0 + b_1X_1+b_2X_2+...+b_jXj\\] Since we can compute \\(\\hat{Y}\\), we can also compute the coefficient of determination \\(R^2\\) and its adjusted version using the exact same formulae. The only conceptual difference is that - since we now have more than one predictor - \\(R^2\\) is no longer the square of a ordinary correlation coefficient. For this reason \\(R\\) (note the capital R) is also referred to as the multiple correlation coefficient. Importantly, however, this does not change how we interpret \\(R^2\\). It is still our measure of the proportion of variance of your criterion \\(Y\\) explained by our regression model as a whole. The statistical test of \\(R^2\\) remains unchanged, as well, that is we test for significance of \\(R^2\\) using an \\(F\\)-distribution with \\(k-1\\) numerator degrees of freedom and \\(N-k\\) denominator degrees of freedom.\nThe function we will use to do multiple regression analysis is the same we used for simple linear regression, namely the function lm(). The only difference is that we now specify the formula argument of the lm() function such that it contains two or more predictors.\n\nExcurse: formula objects with multiple predictors\nBefore we delve into multiple regression in R, we first need to understand how to define the formula object if we have several predictors. Let’s first look at cases, in which we want to predict \\(Y\\) from two predictors \\(X_1\\) and \\(X_2\\)\n\n# formula treating y as an additive function of x1 and x2\ny ~ x1 + x2\n\nIn the example above, we have two additive effects (think of them as main effects) of \\(X_1\\) and \\(X_2\\) on \\(Y\\). But what if we wanted to add an interaction as well? In this case, we can combine the two predictors using the : operator. If we use this operator, R will compute the interaction term as the product of the two variables and use it as a predictor in the model. Here is what the syntax would look like:\n\n# formula treating y as a function of x1 and x2\n# as well as their interaction\ny ~ x1 + x2 + x1:x2\n\nIn the above example, we have the full model, that is, it contains all possible effects that a regression model with two predictors can have. However, it is also possible to feed a reduced model into the lm() function. For example, we might want to estimate a regression model that entails only the main effect of \\(X_1\\) and the interaction effect, but not the main effect of \\(X_2\\). In this case, the model would look like this:\n\n# formula treating y as an additive function of x1 \n# and the interaction of x1 and x2\ny ~ x1 + x1:x2\n\nR has a neat way of making regression model formulae more parsimonious, namely using the * operator. If we combine tow or more variables with this operator, R will include all main effects and interactions of these variables. Here are some examples:\n\n## formula for a full regression model with two predictors\n# this:\ny ~ x1*x2\n\n# reads as this:\ny ~ x1 + x2 + x1:x2\n\n\n## formula for a full regression model with three predictors\n# this:\ny ~ x1*x2*x3\n\n# reads as this:\ny ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\n\n\n## formula for a three-predictor models with the \n## third predictor as a purely additive effect\n# this:\ny ~ x1*x2 + x3\n\n# reads as this:\ny ~ x1 + x2 + x1:x2 + x3\n\nAs we can see, the * operator can make our lives a bit easier whenever models involve multiple predictors.\n\n\nMultiple Regression in R\nLet’s now look at the syntax for a multiple regression model. We will use some made-up data to run a regression model with two predictors \\(X_1\\)and \\(X_2\\). Here is what the data look like:\n\n\n  ID     Y    X1    X2\n1  1  1.29  0.36  0.92\n2  2  0.12 -0.10 -0.73\n3  3 -0.84 -0.83 -0.74\n4  4  2.37  1.66  0.99\n5  5 -0.21  0.17 -0.94\n6  6 -0.05 -0.70 -0.48\n\n\nWe now have to make a choice: we can either predict \\(Y\\) from \\(X-1\\) and \\(X_2\\) in a purely additive model, or we can include the interaction term \\(X_1 \\times X_2\\) as a third predictor in the model. Let’s first run the simpler model that omits the interaction effect. The formla looks like this:\n\\[Y = b_0 + b_1X_1 + b_2X_2+\\epsilon\\] Here is the syntax for this model:\n\n# predict Y from X1 and X2 in a multiple regression\nmod1 = lm(formula = Y ~ X1 + X2, data = df1)\n\n# display the results\nsummary(mod1)\n\nOnce we run that code, R will produce the following console output:\n\n\n# predict Y from X1 and X2 in a multiple regression\nmod1 = lm(formula = Y ~ X1 + X2, data = df1)\n\n# display the results\nsummary(mod1)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = df1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.09061 -0.55322  0.06585  0.48445  1.83926 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.17266    0.07949   2.172 0.032285 *  \nX1           0.30212    0.07707   3.920 0.000165 ***\nX2           0.41297    0.07834   5.271  8.2e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7923 on 97 degrees of freedom\nMultiple R-squared:  0.3289,    Adjusted R-squared:  0.3151 \nF-statistic: 23.77 on 2 and 97 DF,  p-value: 3.975e-09\n\n\n\nAs we can see, the section of the output names “coefficients” contains information on the intercept \\(b_0\\) and the regression weights for the two predictors, \\(b_1\\) and \\(b_2\\). In this example both predictors are significantly related to the criterion \\(Y\\). Note that since we have three parameters in the model and the total sample size \\(N\\) is 100, the \\(t\\)-tests for each parameter have 97 degrees of freedom, and the \\(F\\)-test of \\(R^2\\) against zero has 2 numerator and 97 denominator degrees of freedom.\nHow do we interpret the regression model? The answer is: pretty much like a simple linear regression. The intercept \\(b_0\\) indicates the level of \\(\\hat{Y}\\) (that is, the predictor level of \\(Y\\)) when both \\(X_1\\) and \\(X_2\\) are fixed at zero. Since we did not model and interaction of the two predictors, the regression weights \\(b_1\\) and \\(b_2\\) are straightforward to interpret: \\(b_1\\) tells us much \\(\\hat{Y}\\) increases when we increase \\(X_1\\) by one unit, irrespective of the current level of \\(X_2\\); \\(b_2\\) tells us the same for increases in \\(X_2\\).\nLet’s now look at the full model that includes the interaction of \\(X_1\\) and \\(X_2\\) as a third predictor. The model looks like this:\n\\[Y = b_0 + b_1X_1 + b_2X_2 + b_3X1X2\\] As explained above, there are two ways to define the formula object:\n\n# predict Y from X1, X2, and their interaction\n# Version A:\nmod2 = lm(formula = Y ~ X1 + X2 + X1:X2, data = df1)\n\n# Version B:\nmod2 = lm(formula = Y ~ X1*X2, data = df1)\n\n# display the results\nsummary(mod2)\n\nRunning the code above yields the following output in the console:\n\n\n\n\nCall:\nlm(formula = Y ~ X1 * X2, data = df1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.11870 -0.54380  0.03453  0.47744  2.05139 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.18874    0.07795   2.421   0.0173 *  \nX1           0.31211    0.07541   4.139 7.50e-05 ***\nX2           0.39870    0.07677   5.193 1.16e-06 ***\nX1:X2       -0.14833    0.06252  -2.373   0.0197 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.774 on 96 degrees of freedom\nMultiple R-squared:  0.3661,    Adjusted R-squared:  0.3463 \nF-statistic: 18.48 on 3 and 96 DF,  p-value: 1.527e-09\n\n\n\nThe output now shows four parameters in the “coefficients” section. Adding a third predictor cost us one degree of freedom for both the individual \\(t\\)-tests of the model parameters and and \\(F\\)-test of \\(R^2\\) when compared with the previous model.\nThe interpretation of the model parameters is the same as before for the main effects of \\(X_1\\) and \\(X_2\\). Since the interaction term is simply another predictor from the vantage point of the model, \\(b_3\\) indicates how much \\(\\hat{Y}\\) increases if we increase the product of \\(X_1\\) and \\(X_2\\) by one unit. While this statement is mathematically true, it is often difficult to grasp what these products represent conceptually (e.g., what is the product of one’s experienced stress and available resources?). The important thing to keep in mind about interaction terms in multiple regression is the following: if this interaction is statistically significant, we know decide to believe that the magnitude of the relation of \\(X_1\\) and \\(Y\\) depends on the level of \\(X_2\\) (and that the strength of the relation of \\(X_2\\) and \\(Y\\) depends on the level of \\(X_1\\)).\n\n\nHierarchical regression\nAs soon as we have more than one predictor variable, we can think about comparing nested regression models. We speak of nested models if one model is an extension of another. To be precise, a regression model \\(A\\) is nested in another model \\(B\\), when model \\(A\\) contains only some of the predictors of model \\(B\\) without adding new ones. In hierarchical linear regression, we compare nested models such that we start with a simple model and add predictors in each step.\nAs long as the earlier models in a hierarchical regression are nested in the later models, we can test whether adding a predictor (or a set of predictors) improves the overall fit of the model. We already know how to measure the overall predictive power of a regression model, namely via the variance explained by a regression model. The respective measure is the coefficient of determination \\(R^2\\). Since we expand models in a hierarchical regression analysis, the proportion of the variance of \\(Y\\) explained by the second model is always equal to or greater than that explained by the first model. The question is whether the increase in \\(R^2\\) is large enough to justify the addition of predictors.\nHow can we test whether an increase in \\(R^2\\) between two nested models is statistically significant? We first need to remember that sums of squares follow \\(\\chi^2\\)-distributions. Next, we need to remember that the difference between two \\(\\chi^2\\)-distributed variables is itself \\(\\chi^2\\)-distributed. The degrees of freedom of the difference variables equals the difference of the degrees of freedoms of the two \\(\\chi^2\\)-distributed variables it was computed from. If we, for example computed the difference between a \\(\\chi^2\\)-distributed variable \\(A\\) with 10 degrees of freedom and another \\(\\chi^2\\)-distributed variable \\(B\\) with 8 degrees of freedom, the resulting variable \\(A-B\\) would follow a \\(\\chi^2\\)-distribution with 2 degrees of freedom.\nTo test whether the difference between the \\(R^2\\) of two nested models is different, we now need to remember that \\(R^2\\) is computed from sums of squares, which are \\(\\chi^2\\)-distributed.\n\\[R^2 = \\frac{S_{\\hat{Y}}}{S_{Y}} = \\frac{SS_{regression}}{SS_{total}} = 1-\\frac{SS_{residual}}{SS_{total}}\\]\nIf we compare two nested models, the denominator will be the same because it represents the variance of the observed criterion \\(Y\\). What will differ is the variance of the model prediction \\(S_{\\hat{Y}}\\). Therefore, the models will also differ regarding their \\(SS_{regression}\\) and \\(SS_{residual}\\). In order to compare the two models’ respective fit, we first need to compute the difference between their respective \\(SS_{regression}\\) or their \\(SS_{residual}\\) to obtain a \\(\\chi^2\\)-distributed variable (since \\(SS_{residual} = SS_{total} - SS_{regression}\\), the difference will be the same). We will call this variable \\(SS_\\Delta\\). Since the \\(SS_{regression}\\) have \\(N-k\\) parameters, we can easily compute the degrees of freedom of \\(SS_\\Delta\\):\n\\[df_{SS_\\Delta} = N-k_{model1} - (N-k_{model2})= N-k_{model1}-N+k_{model2}=k_{model2}-k_{model1}\\]\nNow that we have the difference of the sums of squares \\(SS_\\Delta\\) and its degrees of freedom, the last step is to compute an \\(F\\)-statistic to test whether it represent a statistically significant increase in explained variance. This statistic looks as follows:\n\\[\\frac{ \\frac{SS_\\Delta}{df_{SS_\\Delta}} }{\\frac{SS_{residual_{model2}}}{df_{residual_{model2}}}} \\sim F_{df_{SS_\\Delta}; df_{residual_{model2}}}\\]\nNow that we know ho to test for a significant \\(\\Delta R^2\\) on a conceptual level, the question is how to run this test in R. We can do so easily using the function anova(). The anova() function is similar to the summary() function in that it can be used on a range of different models an will perform different operations and produce different outputs based on the objects exact nature. If we call the anova() function and feed it two or more nested regression models, it run a significance test of \\(\\Delta R2\\) between the first and second model, between the second and third model, and so on.\nLet’s now have look at the syntax. We will use the two regression models we looked at above because they are nested (mod1 is nested within mod2 because it lacks the interaction effect). It is important to enter the models in ascending order of complexity (simplest model first). The code will run even if we enter more complex model first, but we may add up with negative values for the test statistic, which makes no sense mathematically.\n\n# test for a significant increase in variance explained\n# between the less complex mod1 and the more complex mod2\nanova(mod1, mod2)\n\nHere is the console output:\n\n\n\nAnalysis of Variance Table\n\nModel 1: Y ~ X1 + X2\nModel 2: Y ~ X1 * X2\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     97 60.889                              \n2     96 57.517  1    3.3724 5.6288 0.01966 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe output first displays the formulae of the two models we compared. More important is the table following it. As we can see, the anova() function computes \\(SS_{\\Delta}\\) as the difference between the two models’ \\(SS_{residual}\\). It also shows the models’ respective degrees of freedom as well as the degrees of freedom for the model difference \\(SS_{\\Delta}\\). Since we only added one parameter in model 2 (the regression weight for the interaction term), this difference has one degree of freedom. Finally, the function computes the \\(F\\)-statistic for the significance test and reports the associated \\(p\\)-value. in our case, the test is significant, meaning that the second model explains a significantly greater proportion of the variance of \\(Y\\).\n\nIf you look at the \\(p\\)-value for \\(\\Delta R^2\\), you will notice that it is identical to the \\(p\\)-value of the significance test of the regression weight for the interaction term in the summary of mod2. Since we only added one variable, and this variable is a significant predictor of \\(Y\\), it follows that the model must explain a significantly larger proportion of the variance of \\(Y\\).\nIn other words, if we add a single predictor in a hierarchical regression analysis, we can test its significance using either the \\(t\\)-test on its regression weight or the \\(F\\)-test on \\(\\Delta R^2\\). The result will be the same.\n\n\n\nMultiple regression with categorical predictors\nJust as in a simple linear regression, our predictors can be categorical instead of continuous. Here, we will focus on the case with one continuous predictor \\(X_1\\) and one categorical predictor \\(X_2\\). Let’s first assume that \\(X_2\\) is dichotomous and that we use it as a dummy-coded predictor (coded 0 vs. 1). Here is some made-up data.\n\n\n  ID     Y    X1        X2\n1  1  3.19  7.59   control\n2  2 14.97 10.55 treatment\n3  3  3.89 12.17   control\n4  4  9.30  5.31 treatment\n5  5  4.84 10.86   control\n6  6 16.57 11.01 treatment\n\n\nFirst, we will run a regression model containing only the two main effects. The syntax is the same as for continuous predictors:\n\n# run the regression analysis\nmod3a = lm(formula = Y ~ X1 + X2, data = df2)\n\n# show the results\nsummary(mod3a)\n\nHere is the console output:\n\n\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = df2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0924 -1.0868 -0.2693  1.3977  3.2981 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.7094     1.1741   0.604 0.548623    \nX1            0.4705     0.1220   3.858 0.000347 ***\nX2treatment   9.1844     0.4274  21.491  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.5 on 47 degrees of freedom\nMultiple R-squared:  0.9079,    Adjusted R-squared:  0.904 \nF-statistic: 231.7 on 2 and 47 DF,  p-value: &lt; 2.2e-16\n\n\n\nAs we can see, both predictors are significant. Time to interpret the model parameters! The intercept \\(b_0\\) tells us the estimated value \\(\\hat{Y}\\) for observations in the reference category (\\(X_2\\) takes the value “treatment”) when \\(X_1\\) is zero. The regression weight of \\(X_1\\), \\(b_1\\) indicates that for each increase in \\(X_1\\) our estimate \\(\\hat{Y}\\) increases by roughly 0.47 units, while the regression weight \\(b_2\\) states that moving from the reference category to the other category (\\(X_2\\) takes the value “control”) is associated with an increase in \\(\\hat{Y}\\) of roughly 9.18 points.\nLet’s now compute a second model by adding the interaction term:\n\n# run the regression analysis\nmod3b = lm(formula = Y ~ X1 * X2, data = df2)\n\n# show the results\nsummary(mod3b)\n\nLet’s have a look at the console output:\n\n\n\n\nCall:\nlm(formula = Y ~ X1 * X2, data = df2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.13476 -0.79688 -0.03927  0.57664  2.41743 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      6.3654     1.1500   5.535 1.43e-06 ***\nX1              -0.1371     0.1215  -1.129    0.265    \nX2treatment     -1.7729     1.5842  -1.119    0.269    \nX1:X2treatment   1.2046     0.1710   7.044 7.83e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.051 on 46 degrees of freedom\nMultiple R-squared:  0.9557,    Adjusted R-squared:  0.9528 \nF-statistic: 330.8 on 3 and 46 DF,  p-value: &lt; 2.2e-16\n\n\n\nWe now notice something interesting: not only is there a significant interaction effect, but the two main effects are no longer significant in the extended model. Let’s first try to make sense of the model parameters. As in the previous model, the intercept \\(b_0\\) represents the model prediction \\(\\hat{Y}\\) when \\(X_1\\) is zero and an observation stems from the reference category (meaning that, due to the dummy-coding R uses for factors, \\(X_2\\) is also zero). The interaction term also takes the value of zero in this case because the interaction is the product of the two predictors, and they are both zero. The regression weight \\(b_2\\) is the shift of the model intercept when we switch from the category “treatment” to the second category “control” (i.e., \\(X_2\\) takes the value 1 due to dummy-coding, but \\(X_1\\) is still fixed at zero). The interpretation of \\(b_1\\) changes slightly now when compared with the previous model. Instead of telling us, in general, how \\(\\hat{Y}\\) changes if we increase \\(X_1\\) by one unit, it now tells us how \\(\\hat{Y}\\) changes in the reference category if we increase \\(X_1\\) by one unit. That means that the regression weight for \\(X_1\\), \\(b_1\\) is the slope of a regression line conditional on the categorical predictor \\(X_2\\) being zero. Finally, the regression weight of the interaction term \\(b_3\\) tells us how the slope of the regression line represented by the regression weight of \\(X_1\\) changes if we switch to the second category. In other words, the slope of the regression line for the second category is the sum of \\(b_1\\) and \\(b_3\\).\nWhat we might be interested in now is whether the new model outperforms the old one in terms of variance explained. We already know how to test this, namely using the anova() function and feeding it models mod3a and mod3b (in that order) as function arguments.\n\n# test for a significant increase in variance explained between mod3a and mod3b\nanova(mod3a, mod3b)\n\n\n\n\nAnalysis of Variance Table\n\nModel 1: Y ~ X1 + X2\nModel 2: Y ~ X1 * X2\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     47 105.700                                  \n2     46  50.851  1    54.848 49.616 7.832e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAs we can see, the full model outperforms the one containing only the main effects.\n\nThis is a nice example because it shows us that a model can be wrong but still provide a very good fit to the data. Once we computed mod3b and confirmed that it explains a substantially greater part of the variance of \\(Y\\), we can - with the usual confidence - discard mod3a as ‘wrong’. However, if we look at the model output for mod3a, we can see that it was able to explain a whopping 90% of the variance of \\(Y\\). Just because a model is very good does not mean that it is accurate!\n\nOn a final note, a convenient aspect about multiple linear regression with one continuous and one categorical predictor is that we can visualise the data quite effectively. Specifically, we can draw one regression line for each level of the categorical predictor. If, for example, we look at the model summary above, we can derive the regression line for the first category by simply looking at the intercept \\(b_0\\) and the regression weight for the continuous predictor \\(b_1\\). For the second category, we now that the intercept is \\(b_0 + b_2\\) (the intercept for the reference category plus its increase if we switch to the second category), and the slope is \\(b_1 + b_3\\) (the slope for the reference category plus its adjustment when we switch to the second category). Here is what it looks like for our model mod3b."
  },
  {
    "objectID": "inference/inference6.html",
    "href": "inference/inference6.html",
    "title": "Repeated measures and mixed ANOVAs",
    "section": "",
    "text": "So far, we have covered ANOVAs for designs, in which the explanatory variables varied between subjects. We will now turn to cases, in which all or at least some of these factors vary within subjects. Although these cases are a little more complicated, they follow the same logic of taking the overall variance of the outcome of interest and partitioning it.\nIn a repeated measures ANOVA (also sometimes referred to as within-subjects ANOVA), we are interested in whether there are mean differences between two or more variables measured among the same participants. This could be the same variable we measure at different times, for example when we are interested in changes over time, or responses to different questions we asked our participants, for example how much they like various colours."
  },
  {
    "objectID": "inference/inference6.html#one-factorial-repeated-measures-anova",
    "href": "inference/inference6.html#one-factorial-repeated-measures-anova",
    "title": "Repeated measures and mixed ANOVAs",
    "section": "One-factorial repeated measures ANOVA",
    "text": "One-factorial repeated measures ANOVA\nThe one-factorial case of the repeated measures ANOVA is an extension of the paired \\(t\\)-test. The difference to the one-factorial between-subjects ANOVA is that each participant provides a response at all levels of the within-subjects factor (if at least one of the responses is missing, that participant is lost for the analysis). The question now is how to partition the variance of the observations or - to be more precise - the corresponding sums of squares.\nIn a first step, we can partition the total sum of squares into variability between participants and variability within participants:\n\\[SS_{total} = SS_{betweenP} + SS_{withinP}\\]\nHere, \\(SS_{betweenP}\\) represents variability that we can attribute to idiosyncrasies of our participants, meaning that participants differ systematically in their responses. Depending on what we are investigating the reasons for such systematic differences could be inter-individual differences in terms of personality, gender, education, or general cognitive ability. They could also stem from differences in task-related aptitude or current motivational or affective states. Whatever the reasons, if there is variation between participants, we can state that, averaged across all levels of our factor, some participants have higher or lower levels in our outcome variable than others.\nIn most cases, we are more interested in variability within participants represented by the \\(SS_{withinP}\\). The reason is that this variability contains not only the measurement error (i.e., unsystematic or unexplained variability of responses each participants provides), but also systematic variability between the levels of our within-subjects factor. This means that we can further partition the \\(SS_{withinP}\\) as follows:\n\\[SS_{withinP} = SS_{betweenC} + SS_{error}\\] Here, \\(SS_{betweenC}\\) represent the part of the total variability of \\(X\\) that we can attribute to the variation of our within-subjects factor. This is what we are ultimately interested in when we run a repeated measures ANOVA. The error term \\(SS_{error}\\) accordingly includes all variability that we cannot attribute to the variation in the within-subjects factor. In sum, we can partition the total sum of squares as follows:\n\\[SS_{total} = SS_{betweenP} + SS_{betweenC} + SS_{error}\\] The first step is to compute the total sum of squares. We do so by collapsing across all participants \\(i\\) and all \\(j\\) levels of the within-subjects factor \\(A\\). Thus, our total sum of squares looks as usual:\n\\[SS_{total} = \\sum_{i=1}^{N} \\sum_{j=1}^{J} (x_{ij} - \\bar{x})^2\\]\nWe next compute the \\(SS_{betweenP}\\), for the sake of completeness. Technically, we do not really need it because the information we are actually interested in lies completely within participants. In order to compute the \\(SS_{betweenP}\\), we pretend that there is no more variability within participants. We can so by replicate each participants’ responses by their mean across all factor levels. That is, we pretend that a participant always responds with the exact same score on all measurements.\n\\[SS_{betweenP} = J\\sum_{i = 1}^{N}(\\bar{x}_i-\\bar{x})^2 \\]\nHere, \\(J\\) is the number of factor levels and \\(N\\) is the sample size.\nWe next compute the \\(SS_{withinP}\\). Since we are only interested in how much variation there is in the various responses participants provided, we must pretend that there is no more variability between participants. We do so by substituting the grand mean \\(\\bar{x}\\) with each participant’s own mean response across all factor levels \\(\\bar{x_i}\\):\n\\[SS_{withinP} = \\sum_{i=1}^{N}\\sum_{j=1}^{J}(x_{ij}-\\bar{x}_{i})^2\\] Here, \\(N\\) is again the sample size, \\(J\\) is the number of factor levels, \\(X_{ij}\\) is the response of the \\(i\\)th participant to the \\(j\\)th level of our factor, and \\(\\bar{x_i}\\) is the mean response of the \\(i\\)th participant across all factor levels.\nNow that we have computed the \\(SS_{withinP}\\), it is time to compute the variability in our data that is attributable to the variation of our factor, namely \\(SS_{betweenC}\\). We compute is in the same fashion in which we computed the \\(SS_{between}\\) in the one-factorial case, that is, we substitute each individual response within one factor level by the respective mean response.\n\\[SS_{betweenC} = N\\sum_{j=1}^{J}(\\bar{x}_j-\\bar{x})^2\\] Here, \\(N\\) is the sample size, \\(J\\) is the number of factor levels, \\(\\bar{x}\\) is the grand mean, and \\(\\bar{x_j}\\) is the cell mean for the \\(j\\)th factor level.\nNow that we know both the variability within participants \\(SS_{withinP}\\) and that part of it that we can attribute to variability in our factor \\(SS_{betweenC}\\), we can compute \\(SS_{error}\\) via simple subtraction:\n\\[SS_{error} = SS_{withinP} - SS_{betweenC}\\].\nWe can now compute the \\(F\\)-statistic that will tell us whether the variability between the factor levels is large enough to constitute a significant effect. To do so, we compute the \\(MS_{betweenC}\\) and \\(MS_{error}\\) by dividing the respective sums of squares by their degrees of freedom and then take their ratio:\n\\[\\frac{MS_{betweenC}}{MS_{error}} = \\frac{\\frac{SS_{betweenC}}{J-1}}{\\frac{SS_{error}}{(J-1)\\times(N-1)}} = F_{J-1;(J-1)\\times{(N-1)}}\\] Just as in the between-subjects case, we can now test whether the within-subjects factor, let’s call it \\(A\\), has an effect on the outcome variable. Remember that:\n\\(\\alpha_j = \\mu_j - \\mu\\)\nThis means that the effect of the \\(j\\)th level of factor \\(A\\), \\(\\alpha_j\\) is the difference between the true group mean \\(\\mu_j\\) and the true grand mean \\(\\mu\\).\nJust as in the between-subjects ANOVA, the Null hypothesis is that all \\(\\alpha_j\\) are zero (i.e., all cell means are equal). The alternative hypothesis is that not all group means are equal or, put differently, that at least one of the \\(\\alpha_j\\) is non-zero.\n\\(H0:\\alpha_j = 0 \\quad \\forall j\\)\n\\(H1: \\lnot H_0\\)\n\nRunning a repeated measure ANOVA in R\nSince we already know how to run between-subjects ANOVAs in R, it is fairly easy to run a repeated measures ANOVA. We will use the same package and function we used for the between-subjects ANOVAs, namely the aov_ez() function from R package * afex*. The only difference is that we will use the function argument wihtin instead of between to tell R the name of our within-subjects factor.\nAs always, we need some data first. Let’s assume that we ran a study with a within-subjects design, in which 30 participants worked three memory tasks that only differ in difficulty (let’s assume that we counterbalanced the order so we do not need to concern ourselves with order effects). In other words, we have a within-subjects factor called “difficulty” with three levels, and we can now test whether participants’ task performance differs according this factor. We will call the data frame containing the data df1. Here is what the data looks like.\n\n\n\n  ID difficulty performance\n1  1       easy   2.1646466\n2  1   moderate   0.7448755\n3  1       hard   1.3471731\n4  2       easy   2.0451407\n5  2   moderate  -0.2245538\n6  2       hard  -1.2001469\n\n\n\nWe can now run the repeated measures ANOVA using the following syntax:\n\n# load the library afex\nlibrary(afex)\n\n# run the ANOVA and save it as a new object\naov1 = aov_ez(id = 'ID', dv = 'performance', \n              within = 'difficulty', data = df1)\n\n# return the result of the ANOVA\naov1\n\nIf we run the code above, R will produce the following output in the console:\n\n\n\nAnova Table (Type 3 tests)\n\nResponse: performance\n      Effect          df  MSE         F  ges p.value\n1 difficulty 1.91, 55.25 0.49 27.22 *** .255   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\nSphericity correction method: GG \n\n\n\nThe information is similar to the one we got in the between-subjects case, that is, R displays the name of our factor, \\(F\\)-statistic and its degrees of freedom, an estimate of the effect size \\(\\eta_2\\), and the \\(p\\)-value.\nNote that the analysis yields fractional degrees of freedom that are slightly lower then the ones we would have expected based on the descriptions above. The reason is that the aov_ez() function corrects for violations of the sphericity assumption per default using the Greenhouse-Geisser method.\n\nAs you may recall, sphericity is one of the assumptions we make in order for repeated measures ANOVAs to provide meaningful results. Sphericity means that the variances of all pairwise differences of our factor levels are equal. In a design with a three-level factor, this means that we have three possible differences: \\(\\Delta_{1;2}\\) is the difference between factor levels 1 and 2, \\(\\Delta_{1;3}\\) is the difference between factor levels 1 and 3, and \\(\\Delta_{2;3}\\) is the difference between levels 2 and 3. The assumption of sphericity in this case states that if we computed the variances of each of the \\(\\Delta\\)a, they would all be equal.\nOften, people test whether this assumption is violated and decide only to control for its violation when the respective test does not justify retaining the Null hypothesis that the variances of the pairwise differences between factor levels are equal. However, similar to the paired \\(t\\)-test, R takes a different approach and corrects the degrees of freedom for non-sphericity unless we explicitly ask it not to.\n\nGenerally speaking, there is no reason to turn off the default Greenhouse-Geisser correction. However, in some cases, we may want to run the ANOVA without correcting for egocentricity (for example, when trying to reproduce results of published studies), or we may want to change the correction method to the Huynh-Feldt method.\nWe can change the default for the correction of non-sphericity using function argument of the aov_ez() function called anova-table. This argument is a list that allows us to change several aspects of the ANOVA, but for now we only need to define one parameter in that list called correction, which is a character string indicating how we want to correct for non-sphericity in an aNOVA with at least one within-subjects factor. It defaults to “GG” (Greenhouse-Geisser method), but we can alternative set it to “none” (no correction) or HF(Huynh-Feldt method).\nLet’s say we want to turn off the non-sphericity correction. Here is what the syntax looks like:\n\n# load the library afex\nlibrary(afex)\n\n# run the ANOVA and save it as a new object;\n# also turn off correction for non-sphericity\naov1b = aov_ez(id = 'ID', dv = 'performance', \n              within = 'difficulty', data = df1,\n              anova_table = list(correction = 'none'))\n\n# return the result of the new ANOVA\naov1b\n\nAs we can see in the console, the degrees of freedom now match the ones stated above. Since the violation of sphericity was not severe in the first place (we can see this from the corrected degrees of freedom being very close to the uncorrected ones), the results remain qualitatively similar.\n\n\n\nAnova Table (Type 3 tests)\n\nResponse: performance\n      Effect    df  MSE         F  ges p.value\n1 difficulty 2, 58 0.47 27.22 *** .255   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\n\n\n\nDisentangling significant effects in repeated measures ANOVAs\nAs in the between-subjects case, a significant effect in a repeated measures ANOVA becomes difficult to interpret without further analysis once we have three or more factor levels. The simple conclusion that “not all means are equal” is rarely satisfactory. Therefore, we need to investigate where exactly the mean differences originate using post-hoc comparisons.\nThe good news is that it works pretty much in the same fashion as it does for between-subjects ANOVAs. That is, we can use the emmeans() function from the emmeans package to test pairwise comparisons or custom contrasts for statistical significance. If we were, for example, interested in the pairwise comparisons, the syntax would look as follows:\n\n# load the library emmeans\nlibrary(emmeans)\n\n# compute pairwise comparisons for the within-subjects factor\nemmeans(aov1, specs = 'difficulty', \n        contr = 'pairwise', correction = 'tukey')\n\nNote that since ‘tukey’ is the default for the correction argument, we could have omitted it. Now let’s have a look at the console output.\n\n\n\n$emmeans\n difficulty  emmean    SE df lower.CL upper.CL\n easy        1.1221 0.148 29    0.820    1.424\n hard       -0.0738 0.177 29   -0.435    0.288\n moderate    0.9778 0.181 29    0.608    1.347\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast        estimate    SE df t.ratio p.value\n easy - hard        1.196 0.160 29   7.472  &lt;.0001\n easy - moderate    0.144 0.194 29   0.743  0.7402\n hard - moderate   -1.052 0.175 29  -6.006  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\nOne thing is worth noting here: other than in the between-subjects case, the post-hoc contrasts do not benefit from increased degrees of freedom. Instead, they are equivalent to simple paired \\(t\\)-tests. The reason is that, here, we already use information from all available participants when estimating the error term of the pairwise comparisons."
  },
  {
    "objectID": "inference/inference6.html#repeated-measures-anovas-with-multiple-factors",
    "href": "inference/inference6.html#repeated-measures-anovas-with-multiple-factors",
    "title": "Repeated measures and mixed ANOVAs",
    "section": "Repeated measures ANOVAs with multiple factors",
    "text": "Repeated measures ANOVAs with multiple factors\nJust as in the between-subjects case, we can run a repeated measures ANOVA with two or more within-subjects factors. Here, we will focus on the two-factorial case, but from there, we can easily extrapolate to more complex designs.\nIf we have two within-subjects factors \\(A\\) and \\(B\\), systematic variance between conditions can stem from the main effects of these factors or their interaction. As such, we can decompose the variability between conditions represented by \\(SS_{betweenC}\\) as follows:\n\\[SS_{betweenC} = SS_A + SS_B + SS_{A \\times B}\\] The test logic is, again, similar to that of the two-factorial between-subjects ANOVA, that is, we can test the two main effects and the interaction effect for statistical significance using \\(F\\)-ratios. To do so, we first need to compute the \\(MS\\) of the effect we are interested in as the ratio of its \\(SS\\) and its degrees of freedom, and the divide this \\(MS\\) by the \\(MS_{error}\\) which we compute in the exact same way as we did in the one-factorial repeated measures ANOVA (see above).\nUsing that approach, we can test: \\(H_{0_A}\\) (all marginal means of factor A are equal), \\(H_{0_B}\\) (all marginal means of factor B are equal), and \\(H_{0_{A \\times B}}\\) (the combination of \\(A\\) and \\(B\\) has no effect beyond the the respective main effects of \\(A\\) and \\(B\\)).\n\nRunning two-factorial repeated measures ANOVAS in R\nBefore we can look at the code for a two-factorial repeated measures ANOVA, we need some data. Let’s assume we ran an experiment studying 50 participants’ willingness to forgive a transgression by another person in a fictional situation on a scale from 1 (not at all) to 6 (certainly). Let’s further assume that we manipulate two variables: the severity of the transgression (low, moderate, or high) and the apparent socio-economic status (SES) of the other person (low vs. high). This leaves us with a \\(2 \\ times 3\\) within-subjects design. Here is an excerpt of some simulated data for this study which are contained in the data df2.\n\n\n\n  ID severity  SES forgive\n1  1      low high       3\n2  2      low high       3\n3  3      low high       3\n4  4      low high       3\n5  5      low high       3\n6  6      low high       4\n\n\n\nWe can now run the two-factorial repeated measures ANOVA using the aov_ez() function. Since we have two within-subjects factors, we need to feed the function a character vector containing both factor’s names as the within argument. here is what the code looks like:\n\n# run a two-factorial within-subjects ANOVA\naov2 = aov_ez(id = 'ID', within = c('severity', 'SES'), \n              data = df2, dv = 'forgive')\n\n# display the ANOVA results\naov2\n\nOnce we run the code above, here is what appears in the console as output:\n\n\n\nAnova Table (Type 3 tests)\n\nResponse: forgive\n        Effect          df  MSE         F  ges p.value\n1     severity 1.93, 94.57 0.53 10.57 *** .032   &lt;.001\n2          SES       1, 49 0.34      1.40 .001    .243\n3 severity:SES 1.55, 76.13 0.81    4.47 * .017    .022\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\nSphericity correction method: GG \n\n\n\nAs we can see, the ANOVA table R returns contains information on the two main effects and the interaction. For the two significant effects, the main effect of the severity of the transgression and the interaction of severity and SES, we can also see that the aov_ez() function adjusted the degrees of freedom to adjust non-sphericity. There is no such adjustment for the main effect of SES because it has only two levels (if we wanted to turn the adjustment off or set it to the Huynh-Feldt method, we could do it using the anova_table argument, see above).\n\n\nDisentangling effects in a two-factorial repeated measure ANOVA\nSimilar to the between-subjects case, how we go about disentangling the effects in a two-factorial repeated measure ANVOA depends critically on whether the interaction was statistically significant. Remember that, as general rule, we should only interpret main effects if there is either no evidence of an interaction in our analysis or if the interaction is ordinal.\nLet’s have a look at the results using a violin plot (the black dots show the means for each condition while the lines behind them represent the 95% confidence intervals):\n\n\n\n\n\n\n\n\n\nThe graphical inspection already suggests that we do not have an ordinal interaction, which means that we should focus on the interaction when interpreting the results and not interpret the main effect of severity. Of course, we could also run a statistical analysis instead of the visual inspection by checking for each level of the severity factor whether there is a significant effect of SES in the same direction.\nWe can do that using the emmeans() function. However, we will use the specs argument in a slightly different way. Instead of defining it as a character vector containing our two factors, we will define it as a formula. Specifically, we will tell the emmeans() function to compare the means of the factor for which the ANOVA yielded a significant main effect (severity) separately for each level of the other factor (SES). Here is that the syntax looks like:\n\n# obtain pairwise comparisons for SES for each level of severity\nemmeans(object = aov2, specs = ~ severity|SES, \n        contr = 'pairwise', adjust = 'tukey')\n\nHere, R will know that we enter a formula when specifying the specs argument because we lead with a tilde operator (~). The rest of the definition of specs reads as SES by severity. Now let’s have a look at the output:\n\n\n\n$emmeans\nSES = high:\n severity emmean    SE df lower.CL upper.CL\n low        3.78 0.162 49     3.45     4.11\n moderate   3.68 0.152 49     3.37     3.99\n high       3.64 0.151 49     3.34     3.94\n\nSES = low:\n severity emmean    SE df lower.CL upper.CL\n low        3.96 0.148 49     3.66     4.26\n moderate   3.72 0.134 49     3.45     3.99\n high       3.18 0.139 49     2.90     3.46\n\nConfidence level used: 0.95 \n\n$contrasts\nSES = high:\n contrast        estimate    SE df t.ratio p.value\n low - moderate      0.10 0.188 49   0.532  0.8559\n low - high          0.14 0.157 49   0.894  0.6463\n moderate - high     0.04 0.143 49   0.280  0.9577\n\nSES = low:\n contrast        estimate    SE df t.ratio p.value\n low - moderate      0.24 0.166 49   1.450  0.3237\n low - high          0.78 0.129 49   6.061  &lt;.0001\n moderate - high     0.54 0.115 49   4.694  0.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\nAs usual, calling the emmeans() function yields two tables, one containing the means (in this case the means of each level of SES for each level of severity), and another containing the contrast tests we asked for. In our case, there are three contrasts for each level of SES since we run pairwise comparisons on a three-level factor (severity). As we can see, the severity of the transgression has statistically significant effects only when SES is low. Here, the willingness to forgive is higher when severity is low as compared with highly severe transgressions or when it is moderately severe instead of highly severe. Since the corresponding comparisons are not significant for high SES, we cannot say whether there is a true main effect of severity. That is, the statistical analysis confirms the graphical analysis and tells us not to interpret the main effect of severity.\n\nIn the example above, we chose the Tukey’s method to adjust for multiple comparisons. Note, however, that the emmeans() function did not adjust for six test even though we ran six tests in total.\nInstead, the function adjusted for three tests in each fo the two sets of tests. The same would have happened had we chosen a different adjustment method such as Bonferroni’s method.\nWhat this means is that we need to consider that this way of adjusting for multiple comparisons is more liberal than correcting for all tests we ran post-hoc.\nIf we wanted to adjust for all tests, the most straightforward approach would be to specify the contrasts we are interested in manually by feeding the contr argument a list of vectors containing the contrast weights.\n\nNow that we know that there is an interaction in our data that is not ordinal, we know that we should not interpret the main effects. We do, however, need to understand the nature of the interaction effect. We already know something about the interaction because we ran pairwise comparisons of the three levels of our severity factor for each level of SES.\nWe can now run the complementary analysis by testing for differences between the two levels of SES for each level of severity of the transgression. If we do not want to adjust for multiple comparisons, we can use the formula-based approach to specifying the specs argument (since there is only one comparison for each level of severity, the function will not adjust the \\(p\\)-values). Here is what the syntax looks like:\n\n# run pairwise comparisons of SES for each level of severity\nemmeans(object = aov2, specs = ~ SES|severity,\n        contr = 'pairwise', adjust = 'none')\n\nLet’s have a look at the output:\n\n\n# run pairwise comparisons of SES for each level of severity\nemmeans(object = aov2, specs = ~ SES|severity,\n        contr = 'pairwise', adjust = 'none')\n\n\nAs we can see, only one of the comparisons is statistically significant. That is, while we cannot say based on the data whether SES affects the willingness to forgive slightly or moderately severe transgressions, SES makes a difference when transgressions are severe.\nFinally, let’s consider the scenario, in which there is no significant interaction between the two within-subjects variables or in which the interaction turns out to be ordinal. In this case, we can actually interpret main effects. This means that we also need to disentangle main effects if the respective variable has more than two levels. We can, once again, do that using the emmeans() function by running pairwise comparisons on the marginal means. To this end, we need to specify the specs argument as the name of the factor we are interested in and omitting the other factor. Let’s do that using the example data (neglecting for a moment that the interaction was not ordinal). Here is what the code would look like:\n\n# run pairwise comparisons on the marginal means of severity of transgressions by collapsing across the two levels of SES\nemmeans(object = aov2, specs = \"severity\",\n        contr = 'pairwise', adjust ='tukey')\n\nNow let’s have a look at the output:\n\n\n\n$emmeans\n severity emmean    SE df lower.CL upper.CL\n low        3.87 0.129 49     3.61     4.13\n moderate   3.70 0.125 49     3.45     3.95\n high       3.41 0.132 49     3.14     3.68\n\nResults are averaged over the levels of: SES \nConfidence level used: 0.95 \n\n$contrasts\n contrast        estimate     SE df t.ratio p.value\n low - moderate      0.17 0.1100 49   1.541  0.2808\n low - high          0.46 0.0978 49   4.705  0.0001\n moderate - high     0.29 0.0949 49   3.057  0.0099\n\nResults are averaged over the levels of: SES \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\nR tells us that the results of the pairwise comparisons are averaged across the levels of SES, just as we intended. We can see that willingness to forgive is higher for transgression of low or moderate severity when compared with severe transgressions. If there was no interaction in our data or if the interaction was ordinal, we could then conclude that this is a general pattern."
  },
  {
    "objectID": "inference/inference6.html#mixed-anovas",
    "href": "inference/inference6.html#mixed-anovas",
    "title": "Repeated measures and mixed ANOVAs",
    "section": "Mixed ANOVAs",
    "text": "Mixed ANOVAs\nWe speak of a mixed ANOVA when the research design contains at least one between-subjects factor and at least one within-subjects factor. A very prominent type of studies that fall into this category is the randomised control trial with pre, post, and follow-up measures.\nWe already know how to partition the variability of our outcome variable \\(x\\) in both the between-subjects and the within subjects-case. In a mixed ANOVA, we do both of these things. It is almost like running two separate ANOVAs.\nTo test between-subjects effects, we first compute each subjects average score across all within-conditions. We then compute the sums of squares for all effects and the respective error term in the same fashion as we would in a pure between-subjects design.\nIn order to test the effects of main effects or interactions of within-subjects effects and within-between-interactions, we focus solely on the variability within participants. We partition this variability just as we would if this were a pure within-subjects design. That is, we treat a within-between-interaction as if it was just another within-subjects factor.\nWhen computing mean squares from the different sums of squares, we need to consider that we have two error terms. For between-effects, this is the variability within the different between-subjects conditions (collapsed across all levels of our within-subjects factors). For within-effects, it is that part of the variability within participants that we cannot account for with our within-effects and within-between-interactions. You will, thus, see two different mean squared errors (MSE) in the ANOVA output of a mixed ANOVA.\n\nRunning mixed ANOVAs in R\nRunning mixed ANOVAs in R is easy. All we need to do feed the aov_ez() function both the between and within arguments. As always, we first need some data for the mixed ANOVA. Let’s say we ran study testing an intervention using a \\(2 \\times 2\\) design with experimental condition (treatment vs. control) as a between-subjects factor and measurement time (pre treatment vs. post treatment) as a within-subjects factor. Here is an excerpt of the data:\n\n\n  ID      cond time score\n1  1 treatment  pre     4\n2  2 treatment  pre     7\n3  3 treatment  pre     7\n4  4 treatment  pre     4\n5  5 treatment  pre     7\n6  6 treatment  pre     7\n\n\nHere is what the data looks like if we were to visualise it:\n\n\n\n\n\n\n\n\n\nNow that we have some data, we can actually run the mixed ANOVA. Here is the syntax:\n\n# run a mixed 2 by 2 ANOVA and save it as an object\naov2 = aov_ez(id = 'ID', between = 'cond',\n              within = 'time', dv = 'score',\n              data = df2)\n\n# show the results in the console\naov2\n\nOnce we run that code, we will get the following console output:\n\n\n\nContrasts set to contr.sum for the following variables: cond\n\n\nAnova Table (Type 3 tests)\n\nResponse: score\n     Effect    df  MSE         F   ges p.value\n1      cond 1, 78 1.26      0.04 &lt;.001    .833\n2      time 1, 78 0.56 12.12 ***  .046   &lt;.001\n3 cond:time 1, 78 0.56    6.96 *  .027    .010\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\n\nAs we can see, the output contains three effects, the two main effects and the within-between interaction. Note also that the between-subjects effect of “condition” uses a different error mean squares (based on the variability between subjects) that the within-subjects effect os “time” and the interaction term (which is based on the within-subject variability).\n\n\nDisentangling effects in mixed ANOVAs\nLet’s quickly think back on the two-factorial between-subjects ANOVA. Remember what we said about disentangling effects when ANVOAs have more than one factor? The bottom line was: which effects we are going to interpret and, thus, need to disentangle, depends on the interaction effect. We should only interpret significant main effects if a) there is no evidence of an interaction, or b) the interaction is ordinal.\nIf we have a significant interaction, we first need to find out how exactly it looks. One way to go about this is using pairwise comparisons on all cells. However, this can get messy with large designs simply because the number of possible pairwise comparisons increases exponentially with the number of a design’s cells. Alternatively, we could manually specify post-hoc contrasts to get exactly those contrasts we are interested in. Of course, this can be tedious with large designs, and it is somewhat error prone if we need to enter large numbers of vectors containing contrast weights.\nThe good news is that there is another elegant way to run analyses of the simple effects behind an interaction, namely providing the emmeans() function with a formula when defining the specs argument.\nGenerally speaking, when we use a formula to specify specs we are telling the function to run certain contrasts on one variable (or even more than one) for each level of another (or for each combination of several others). Here is what the syntax looks like in the case of simple effects:\n\n# run pairwise comparisons for the between-factor \n# 'cond' for each level of the within-factor 'time'\nemmeans(object = aov2, specs = ~ cond | time,\n        contr = 'pairwise', adjust = 'none')\n\nYou can read the formula we fed the emmeans() to define its spec argument as follows: run contrasts on “cond” for each level of “time”. Which contrasts to run is, once again, specified using the contr argument, in our case pairwise comparisons.\nLet’s have a look at the console output:\n\n\n\ntime = pre:\n contrast            estimate    SE df t.ratio p.value\n treatment - control   -0.275 0.242 78  -1.136  0.2593\n\ntime = post:\n contrast            estimate    SE df t.ratio p.value\n treatment - control    0.350 0.180 78   1.944  0.0555\n\n\n\nAs we can see, the output tells us which effect the experimental condition (treatment vs. control) has for each time point (pre. vs. post). Here, we can only confirm an effect after the treatment (post), but our data do not allow conclusions before treatment (pre).\nIn order to get the full picture of the interaction, we also need to inspect the effect of time by condition. To this end, we simply switch the two variables in our formula specification of specs (see syntax below).\n\n# run pairwise comparisons for the between-factor \n# 'cond' for each level of the within-factor 'time'\nemmeans(object = aov2, specs = ~ time | cond,\n        contr = 'pairwise', adjust = 'none')\n\n\n\n\ncond = treatment:\n contrast   estimate    SE df t.ratio p.value\n pre - post   -0.725 0.168 78  -4.327  &lt;.0001\n\ncond = control:\n contrast   estimate    SE df t.ratio p.value\n pre - post   -0.100 0.168 78  -0.597  0.5523\n\n\n\nAs we can see, there is only an effect of the time at which we measure the dependent variable when there is a treatment in between (which we would probably expect in this kind of research design). We cannot say, however, whether there is any change in the dependent variable for participants in the control group since the respective simple effect is not statistically significant.\n\nCaveat: Doing post-hoc contrasts in this fashion imposes a restriction on the adjustment of \\(p\\)-values for multiple comparisons. R will not adjust for all tests, but for all comparisons within a level of the grouping variable.\n\nIn our example, we would not interpret the main effect of “time” because we have a semi-disordinal interaction. That is, we cannot say that “time” has a general impact on the dependent variable, and we would also not claim that experimental condition has a general effect. Whether they have an effect seems to depend on the level of the respective other variable (thus, the significant within-between interaction in the ANOVA).\nIn addition, since we have only two factor levels for “time”, there would be no need to disentangle the effects (we could just look at the means to infer the direction of the effect). If we were interested in disentangling a main effect, however, we could do it using emmeans() just like we did it in the case of the two-factorial between-subjects ANOVA.\nIn order to disentangle a main effect, we need to run pairwise comparisons on its marginal means (i.e., the means when collapsing across the levels of other factors). We can do so by feeding the emmeans() function only the factor of interest when defining specs (omitting the other factors).\n\n# disentangle the main effect of time\nemmeans(object = aov2, specs = 'time',\n        contr = 'pairwise', adjust = 'none')\n\nHere is wehat appears in the console once we run that code:\n\n\n\n$emmeans\n time emmean    SE df lower.CL upper.CL\n pre    5.84 0.121 78     5.60     6.08\n post   6.25 0.090 78     6.07     6.43\n\nResults are averaged over the levels of: cond \nConfidence level used: 0.95 \n\n$contrasts\n contrast   estimate    SE df t.ratio p.value\n pre - post   -0.412 0.118 78  -3.482  0.0008\n\nResults are averaged over the levels of: cond \n\n\n\nWhen looking at the effect of time, the analysis reveals a difference between the two marginal means pre and post treatment (this is equivalent to the main effect of time in the ANOVA since we have only got two factor levels). Conveniently, the output also reminds us that these are marginal means by stating that results are averaged across the levels of our other factor “cond”."
  },
  {
    "objectID": "inference/inference4.html",
    "href": "inference/inference4.html",
    "title": "The one-factorial ANOVA",
    "section": "",
    "text": "The Analysis of Variance (ANOVA) is a generalisation of the \\(t\\)-Test. It is used to test the difference between two or more means against zero. When comparing groups using ANOVA, we partition the total variance inherent to the data (collapsed across all groups) into the variance between groups (i.e., the variance of the group means) and the variance within groups."
  },
  {
    "objectID": "inference/inference4.html#the-foundations-of-anova",
    "href": "inference/inference4.html#the-foundations-of-anova",
    "title": "The one-factorial ANOVA",
    "section": "The foundations of ANOVA",
    "text": "The foundations of ANOVA\nA key concept in ANOVA is that of the sum of squares. To understand the sum of squares, let us consider the formal definition of the variance \\(\\sigma^2\\) of a variable, which looks as follows:\n\\[\\sigma^2_x = \\frac{1}{n} \\times \\underbrace{\\Sigma{(x_i - \\bar{x})^2}}_\\text{sum of squares}\\]\nFrom this equation, we can see that the variance of the variable \\(x\\) is the product of two components. One is the sum of squares, that is, the sum of the squared deviations of each observation \\(x_i\\) from their mean \\(\\bar{x}\\). The second factor simply divides this sum of squares by the number of observations \\(n\\).\nWhen we estimate the true variance \\(\\sigma^2_x\\) from data we collected, using the formula above produces a slight bias. Specifically, our estimate of the true variance will be systematically too low. To arrive at an unbiased estimate, we need to divide the sum of squares by \\(n-1\\) instead of \\(n\\), leading to the following formal definition of the estimate of the variance \\(\\hat{\\sigma}^2_x\\):\n\\[\\hat{\\sigma}^2_x = \\frac{1}{n-1}\\Sigma{(x_i - \\bar{x})^2}\\]\n\nFun fact: Dividing a sum of squares by its degrees of freedom yields the so called mean squares. The mean squares follow a \\(\\chi^2\\)-distribution with the respective degrees of freedom."
  },
  {
    "objectID": "inference/inference4.html#the-logic-of-the-anova",
    "href": "inference/inference4.html#the-logic-of-the-anova",
    "title": "The one-factorial ANOVA",
    "section": "The logic of the ANOVA",
    "text": "The logic of the ANOVA\nThe whole idea of ANOVA rests on the fact that the variance can be partitioned into the sum of its parts. This is also true for the the sum of squares. As a very basic rule, we can state that:\n\\[SS_{total} = SS_{between} + SS_{within}\\]\nHere, \\(SS_{total}\\) is a measure of the total variability of a variable, \\(SS_{between}\\) represents that part of the total variability that is due to differences of the group means, and \\(SS_{within}\\) represents the part of the total variability that results from heterogeneity within the groups.\nSimilar to the \\(t\\)-test, ANOVA is a signal to noise ratio, where we treat variability between groups as the signal and variability within groups as the noise. The test statistic we use to test for significant mean differences between groups is the \\(F\\)-value, which is formally defined as:\n\\[F = \\frac{VAR_{between}}{VAR_{within}}=\\frac{\\frac{SS_{between}}{df_{between}}}{\\frac{SS_{within}}{df_{within}}} = \\frac{MS_{between}}{MS_{within}}\\]\nIn the formula above, \\(MS\\) is the corresponding mean squares.\n\nFun fact: Based on the formula above, we can easily derive that \\(F\\) is the ratio of two \\(\\chi^2\\)-distributed variables. Both the numerator and the denominator constitute a sum of squares divided by its degrees of freedom. That is also the reason why the \\(F\\)-distribution has two degrees of freedom, one for the numerator and one for the denominator.\n\nHow we compute the mean squares depends on the type of ANOVA we run and the number of groups we compare in it.\nHowever, we can state generally how we compute the different sum of squares. First of all, let’s remember how we compute the total sum of squares, \\(SS_{total}\\):\n\\[SS_{total} = \\sum_{i=1}^{N} (x_i - \\bar{x})^2\\]\nThe total sum of squares represent the sum of the squared deviation of all \\(N\\) observed data points from the grand mean \\(\\bar{x}\\). Now lets look at the formula for \\(SS_{between}\\).\n\\[SS_{between} = \\sum_{j=1}^{J} n_j \\times (\\bar{x}_j - \\bar{x})^2\\]\nHere, \\(J\\) is the number of groups we compare, and \\(n_j\\) is the sample size of group \\(j\\). For the \\(SS_{between}\\) we pretend that there is no variance within the \\(J\\) groups at all. Each observation is represented by its group’s mean \\(\\bar{x}_j\\), and we compute the variability as the difference of these group means from the grand mean \\(\\bar{x}\\). Therefore, the \\(SS_{between}\\) isolates the between-group part of the total variability of \\(x\\). Let’s now turn to the \\(SS_{within}\\).\n\\[SS_{within} = \\sum_{j=1}^{J} \\sum_{i = 1}^{n_j} (x_{ij} - \\bar{x}_j)\\]\nAgain, \\(J\\) is the number of groups we compare, and \\(n_j\\) is the sample size in group \\(j\\). The \\(x_{ij}\\) refers to the \\(i\\)th observation in group \\(j\\). For the \\(SS_{within}\\), we pretend that there is no variance between groups at all. We do so by substituting the grand mean for the respective group means \\(\\bar{x}_j\\). Thus, the \\(SS_within\\) isolates the within-group variability of \\(x\\).\nThe final thing we need to understand before we can delve into the actual ANOVAs is the \\(F\\)-statistic. As we have seen above, we compute \\(F\\) as a ratio of the \\(MS_{between}\\) and the \\(MS_{within}\\). This ratio is interesting in several ways:\n\nSince the \\(SS_{between}\\) usually has much fewer degrees of freedom than the \\(SS_{within}\\), the variability between groups does not have to be nearly as large as the variability within groups to produce a large \\(F\\)-value.\nThe more groups we compare, the lower the \\(F\\)-ratio will be, ceteris paribus. However, this does not necessarily mean that it becomes more difficult to detect significant mean differences. The more groups we compare, and the more numerator degrees of freedom our test has, the lower the critical \\(F\\)-value past which we consider a result statistically significant.\nThe larger our total sample, the more denominator degrees of freedom we have, and the smaller the noise becomes in our signal-to-noise ratio, ceteris paribus. This makes intuitive sense: as the samples size increases, our measurement becomes more precise, making it easier to detect differences between group means.\n\n\nFun fact: If we use an ANOVA to compare two means, we effectively run a \\(t\\)-test but discard its ability to indicate the direction of the effect.\nIn such cases \\(F = t^2\\), and the \\(p\\)-value of both tests will be identical if we run a \\(t\\)-test assuming equal variances.\n\nANOVAs come in various flavours. In the following, we will look at some of them, namely:\n\none-factorial ANOVAs (between-subjects)\ntwo-factorial ANOVAs (between-subjects)\nrepeated-measures ANOVAs (within-subjects)\nmixed ANOVAs (at least one between and one within factor)"
  },
  {
    "objectID": "inference/inference4.html#one-factorial-anova",
    "href": "inference/inference4.html#one-factorial-anova",
    "title": "The one-factorial ANOVA",
    "section": "One-factorial ANOVA",
    "text": "One-factorial ANOVA\nIn a one-factorial ANOVA, we compare two or more group means such that we consider each group to represent one level of the same grouping variable or factor \\(A\\). The underlying model assumes that the true mean of each of the \\(j\\) groups \\(\\mu_j\\) is the sum of the true grand mean \\(\\mu\\) and the effect of the \\(j\\)th level of factor \\(A\\) on that mean. We call these effects \\(\\alpha_j\\).\n\\(\\mu_j = \\mu + \\alpha_j\\)\nThis is equivalent to:\n\\(\\alpha_j = \\mu_j - \\mu\\)\nIn other words, the effect of the \\(j\\)th level of factor \\(A\\) is the difference between the true group mean \\(\\mu_j\\) and the true grand mean \\(\\mu\\). Accordingly, if \\(\\mu_j\\) exceeds \\(\\mu\\), the respective \\(\\alpha_j\\) is positive.\nThe Null hypothesis is that all group means are equal. This equivalent to stating that all \\(\\alpha_j\\) are zero. The alternative hypothesis is that not all group means are equal or, put differently, that at least one of the \\(\\alpha_j\\) is non-zero.\n\\(H0:\\alpha_j = 0 \\quad \\forall j\\)\n\\(H1: \\lnot H_0\\)\nRunning ANOVAs in base R tends to be very clunky because R is more centered around classic regression models. Therefore, we won’t be using base R to run ANOVAs, but instead use an R package called afex (Analysis of Factorial Experiment). That means, we need to install and load afex first.\n\n# install.packages(\"afex\")\nlibrary(afex)\n\nOnce we have done that, we can start doing ANOVAs with one of several functions:\n\naov_car()\naov_4()\naov_ez()\n\nThe three functions serve the same goal and do the same things, but they differ in terms of the syntax. The function aov_car() uses syntax that is most closely related the the (clunky) base R version of ANOVA. In contrast, aov_4() uses syntax based on the popular lme4 package that is widely used for the analysis of generalised mixed models. Thus, this function is ideally suited for users who are already familiar with lme4. Finally, aov_ez() uses a completely string-based format, that is, it does not require a formula type object as a function argument. The advantage of aov_ez() is that it is very convenient and easy to handle (thus the suffix “ez”). This comes at the cost of flexibility. The lack of a formula means that we are stuck with a full ANOVA model. The other two functions technically allow us to specify models that omit certain main effects or interactions. However, since we will rarely want to run these incomplete models, this is a drawback that usually causes no hassles. In the following, we will focus on the aov_ez() function and leave exploration of the other functions to the discretion of the reader.\nHere are the most important function arguments of aov_ez():\n\nid (necessary): a character value indicating the name of the variable that contains our subject ID\ndv (required): another character value; the name of the variable containing the data the group means of which we want to compare\ndata (required): an R object of type data frame containing the data we want to analyse\nbetween (optional): a character string or character vector indicating the name(s) of the variable(s) constituting the between-subjects factor(s) of our design; default is NULL meaning that there are no between-subjects factors\nwithin (optional): a character string or character vector indicating the name(s) of the variable(s) constituting the within-subjects factor(s) of our design; default is NULL meaning that there are no within-subjects factors\n`covariate*: a character value or vector indicating the name(s) of the covariate(s) in our analysis\nfactorize (optional): a logical value; determines if the between- and within-subject variables are turned into factors prior to the analysis; default is TRUE; if our design has at least one covariate, we need to set this to FALSE and make sure that all factors are defined as such manually\nanova_table (optional): a list of further arguments passed to the function; the ones we may be interested in are es (effect size; default is ‘ges’, which yields \\(\\eta^2\\) as an effect size measure, but we can switch it to ‘none’ or to ‘pes’, which yields \\(\\eta^2_p\\)) and correction (non-sphericity correction method; default is ‘none’, but we can switch it to ‘GG’ for the Greenhouse-Geisser or ‘HF’ for the Huynh-Feldt correction)\n\nFor the purpose of running a one-factorial between-subjects ANOVA, we can disregard some of the function arguments shown above. The only ones we need are id, dv, between, and possibly anova_table in case we want to obtain the effect size.\n\nNow that we want to do ANOVAs, it is time to talk about factors. In R, factors are a special type of vector that contain both values and labels for those values. The different values of vectors are considered to be categories. Factors are important because the ANOVA-function we use here requires its between- and within-subject variables to be factors.\nWe can create a factor using the factor() function by feeding it the following function arguments:\n\nx: a vector we want to turn into a factor\nlevels: a vector containing all possible values the factor can take\nlabels: a character vector assigning a label to each level of the factor\n\n\nLets look at an example, in which we test whether the means of three groups are equal or not. First, we need to create some data.\n\n# create a data frame containing data from 30 subjects in three groups of 10 each; here, \"id\" is the subject identifier, \"cond\" is the between-subjects grouping variable, and \"dv\" contains the outcome variable we want to compare between groups\n\nmy_df = data.frame(\n  # subject ID\n  ID = 1:30,      \n  # between-subjects-factor 'cond'\n  cond = factor(  \n    rep(x = 1:3, each = 10),\n    levels = 1:3,\n    labels = c('control', 'treatment1', 'treatment2')),\n  # outcome variable 'dv'\n  dv = c(\n    c(10, 12,  9, 14, 11, 15, 13, 15, 18, 12), # dv data for control\n    c( 9,  7, 15, 14,  8,  7, 16, 13, 11, 16), # dv data for treatment1\n    c(12, 11, 11,  9,  8, 13,  8,  6, 14,  7)) # dv data for treatment2\n)\n\nWe can inspect how the data frame looks using the head function.\n\n\n\n  ID    cond dv\n1  1 control 10\n2  2 control 12\n3  3 control  9\n4  4 control 14\n5  5 control 11\n6  6 control 15\n\n\n\nNow that we have some data, we can run the ANOVA. The syntax looks as follows:\n\naov_ez(id = 'ID', between = 'cond', dv = 'dv', data = my_df)\n\nHere is what the output in the console looks like:\n\n\n\nContrasts set to contr.sum for the following variables: cond\n\n\nAnova Table (Type 3 tests)\n\nResponse: dv\n  Effect    df  MSE    F  ges p.value\n1   cond 2, 27 9.27 2.44 .153    .106\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\n\nAs we can see, R displays an ANOVA table in the console along with some additional information. The output is preceded by a message (this is NOT an error message; the code ran properly). This message informs us that the aov_ez() function set the contrast type for our factor ‘cond’ to ‘contr.sum’. What this means is that the contrast underling our factor was forced into effect coding because that is the format that ANOVAs use. We don’t need to concern ourselves with that.\nNext, R will tell us that this output is an ANOVA table based on type-3 sum of squares. Type-3 sums of squares are what most statistics packages use. If your knowledge of statistics is so advanced that you can make an informed decision that you would prefer type-2 sums of squares, you can change it by setting the type function argument to 2.\nNow for the important bits. R shows us what the response variable in our model is, namely ‘dv’. Below that information, it displays the ANOVA table. Because we ran a one-factorial ANOVA; this table contains only one row. Here, we can see the name of the between-subjects factor (effect), the numerator and denominator degrees of freedom for its \\(F\\)-value (df), the mean squares of the effect (MSE), the \\(F\\)-value, the generalised \\(\\eta^2\\) as a measure of the effect size, and the \\(p\\)-value.\nIn our example, the mean difference is not statistically significant, which means that we cannot reject the null hypothesis. In other words, we cannot say whether the true means between the three groups differ.\n\nSimilar to the \\(\\chi^2\\)-test, the \\(F\\)-test we use in an ANOVA is always a one-tailed test because it is based on squared variables. Therefore, the test has no ‘direction’ as a \\(t\\)-test would.\nWhy is this important? Sometimes, we might encounter a scientific article, in which the authors state that they ran a ‘one-tailed’ ANOVA test, but what they do in those articles is simply the divide their \\(p\\)-value by 2. This practice (often encountered when the regular \\(p\\)-value lies between .05 and .10) rests on the erroneous belief that all statistical tests are - per default - two-tailed and can, therefore, also be run as a one-tailed test with slightly greater power."
  },
  {
    "objectID": "inference/inference4.html#disentangling-significant-effects-in-anovas",
    "href": "inference/inference4.html#disentangling-significant-effects-in-anovas",
    "title": "The one-factorial ANOVA",
    "section": "Disentangling significant effects in ANOVAs",
    "text": "Disentangling significant effects in ANOVAs\nIn the example above, we had to retain the Null hypothesis because the analysis did not show evidence that the three means were different from each other. In those cases, there is no need for further analyses. However, things look a bit different when the ANOVA yields a significant result. Let’s look at an example.\nIn this example, we will compare two treatments and one control condition with data for 30 participants in each condition. The data is stored in a data frame called my_df2. Here is what the data looks like (we use the function head() on the data frame to have R show us the first 6 lines).\n\n\n\n  ID    cond  dv\n1  1 control  88\n2  2  treat1 119\n3  3  treat2 119\n4  4 control 103\n5  5  treat1 103\n6  6  treat2 137\n\n\n\nWe now run the ANOVA on the data. Other than before, we will define the result of the analysis as a new R object. This will make it easier for us to disentangle the effect later on. Here is the syntax:\n\n# run a one-factorial ANOVA and save its results as a new R object\nmodel1 = aov_ez(id = 'ID', dv = 'dv', between = 'cond', data = my_df2)\n\nWe will now see a new object called “model1” in the Environment. R will tell us that this object is a list. Entering the new object’s name as syntax will show us the result of the ANOVA just as if we had called the function as is instead of defining it as a new object. Here is the console output:\n\n\n\nAnova Table (Type 3 tests)\n\nResponse: dv\n  Effect    df   MSE         F  ges p.value\n1   cond 2, 87 89.48 22.91 *** .345   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\n\nAs we can see, the effect of the factor “cond” in our fictional data is statistically significant. We can now state - with the usual confidence - that the means of the three groups are not equal. However, we cannot say anything else. Since an ANOVA uses the squared test statistic \\(F\\), we have no information on the direction of the mean differences, nor do we know which means differ. Thus, we need further analyses to get a clear picture of the mean differences the ANOVA detected.\n\nAs a useful - if somewhat brutish - metaphor, think of an ANOVA as firing a shotgun into think fog. A statistically significant effect means that we hit something, but we do not know what we hit. In order to find that out we need to venture into the fog and have a closer look.\n(Credit for this metaphor goes to Prof. Dieter Heyer)\n\nIn order to disentangle a significant effect in an ANOVA with three or more groups, we need to run post-hoc analyses. There are many different ways to run such post-hoc analyses. Here, we will focus on post-hoc contrasts.\n\nPost-hoc t-tests\nBefore we delve into post-hoc contrasts, it is worth mentioning that one easy way to disentangle significant effects in an ANOVA is running \\(t\\)-tests to compare the means of the groups. We already know how to do this, namely by using the t.test() function.\nHowever, there are three downsides to using simple \\(t\\)-tests. First, they are less powerful than contrasts because their estimate of the error variance is based on only a part of the total sample. Second, if we want to control the type-I error rate by adjusting for multiple comparisons, we need to do so manually (this may be easy for Bonferroni’s method, but more challenging for less conservative adjustment methods). Third, as the number of groups we compare in an ANOVA increases, so does the number of possible pairwise comparisons. In fact, the number of comparisons is \\((k^2-k)/2\\), that is, it grows exponentially. Therefore, the amount of code we need to write also increases accordingly.\n\n\nPairwise comparisons using contrasts\nAn alternative to running individual \\(t\\)-tests is to do pairwise comparisons using post-hoc contrasts. A contrast is a weighted average of the group means \\(\\bar{x}_j\\) of the \\(J\\) groups we compare.\n\\[\\hat{\\psi} = \\sum_{j=1}^{J} c_j \\bar{x}_j\\]\nIt must satisfy the condition that the weights \\(c_j\\) add to zero:\n\\[\\sum_{j=1}^{J} c_j = 0\\]\nThe standard error of a contrast \\(\\hat{\\psi}\\) is computed based on all data. That is why contrasts are usually more powerful than run-og-the-mill \\(t\\)-tests. The standard error is defined as:\n\\[S_{\\hat{\\psi}} = \\sqrt{MS_{within} \\sum_{j=1}^{J} \\frac{c_j^2}{n_j}}\\]\nDividing a contrast \\(\\hat{\\psi}\\) by its standard error \\(S_{\\hat{\\psi}}\\) yields a variable that follows a \\(t\\)-distribution with \\(N-J\\) degrees of freedom, where \\(N\\) is the total sample size, and \\(J\\) is the umber of groups.\n\\[\\frac{\\hat{\\psi}}{S_{\\hat{\\psi}}} \\sim t_{N-J}\\] Testing a contrast comes down to running a one-sample \\(t\\)-test to test (for example):\n\\(H_0: \\hat{\\psi} = 0\\)\n\\(H_1: \\hat{\\psi} \\ne 0\\)\nIf we want to analyse contrasts in R, we need another R package called emmeans. This package contains a function called emmeans() (yes, it has the same name as the package) that allows us to run a contrast analysis. Let’s have a look at the emmeans() function. When we call it to analyse contrasts, we need to specify a few arguments:\n\nobject (required): an R object containing the results of an ANOVA (good thing we saved our ANOVA as an R object).\nspec (required): a character string or vector containing the name(s) of the grouping variable(s) in our data (in our example, the grouping variable is ‘cond’).\ncontr (optional): a character value or list indicating the type of post-hoc comparisons; the default is NULL, but we can set it to “pairwise”, or we can define it as a list of vectors representing custom contrasts.\nadjust (optional): a character value indicating which method should be used to control for type-I error inflation due to multiple comparisons; default is “tukey” for Tukey’s method, but we can set it to “none” if we don’t want to adjust our \\(p\\)-values, or we can choose a different adjustment method such as “holm” (the the Holm-Tukey method) or Bonferroni (for the Bonferroni correction).\n\nLet’s dive in an run pairwise comparisons on our ANOVA. We can tell the emmeans() function to run pairwise contrasts by setting the contr argument to “pairwise”. The function will then set up the following contrasts:\n\n\n\n\ncontrol\ntreatment 1\ntreatment 2\n\n\n\n\ncontrast 1\n1\n-1\n0\n\n\ncontrast 2\n1\n0\n-1\n\n\ncontrast 3\n0\n1\n-1\n\n\n\nIn the first example, we will go with the unadjusted \\(p\\)-values. Let’s have a look at the code using the ANOVA model we saved as an R object above (“model1”). Here is the syntax.\n\n# uncomment the next line to install the package emmeans\n# install.packages(\"emmeans\")\n# load emmeans\nlibrary(emmeans)\n\n# pairwise comparisons without type-I error adjustment\nemmeans(object = model1, specs = 'cond',\n        contr = 'pairwise', adjust = 'none')\n\nOne we run the code above, R will return some information in the console. Here is what it looks like:\n\n\n\nWelcome to emmeans.\nCaution: You lose important information if you filter this package's results.\nSee '? untidy'\n\n\n$emmeans\n cond    emmean   SE df lower.CL upper.CL\n control   97.1 1.73 87     93.7      101\n treat1   102.4 1.73 87     98.9      106\n treat2   113.3 1.73 87    109.9      117\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast         estimate   SE df t.ratio p.value\n control - treat1    -5.23 2.44 87  -2.143  0.0349\n control - treat2   -16.20 2.44 87  -6.633  &lt;.0001\n treat1 - treat2    -10.97 2.44 87  -4.490  &lt;.0001\n\n\n\nThe information we are interested in, is contained in the lower half of the output. There, we can see three contrasts, each comparing two of the three levels of our between-subjects variable ‘cond’. The output contains an estimate of the respective mean difference (this is the contrast value \\(\\hat{\\psi}\\)) and its associated standard error (this is \\(S_{\\hat{\\psi}}\\)). It also shows us the empirical \\(t\\)-value for the contrast (dividing \\(\\hat{\\psi}\\) by \\(S_{\\hat{\\psi}}\\) yields this exact value) and its degrees of freedom. Here, we can see that the contrast has more degrees of freedom than a simple \\(t\\)-test would have. Finally, the output contains the \\(p\\)-value for each contrast. Based on this analysis, we would conclude that the effect of condition on our ANOVA was significant because all three means differ from one another.\nNow let’s rerun the analysis while correcting for multiple comparisons. As mentioned above, we can choose from several methods that control type-I inflation. Let’s say we want to go with a classic and opt for Bonferroni’s method. This method adjusts the threshold at witch we consider an effect to be statistically significant. We now accept \\(H_1\\) only if \\(p &lt; \\frac{\\alpha}{k}\\), where \\(\\alpha\\) is our tolerated type-I error level (e.g., \\(\\alpha = .05\\)), and \\(k\\) is the number of post-hoc contrasts we test. What R does instead, is multiplying the unadjusted \\(p\\)-values by \\(k\\). This makes it somewhat more convenient because we can keep comparing the now corrected \\(p\\)-values to our usual tolerated type-I error level.\n\nAre wondering what would happen if the unadjusted \\(p\\)-value is already so large that multiplying it by the number of tests \\(k\\) would propel it above 1? The simple answer is: nothing terrible: R caps the adjusted \\(p\\)-values at 1 in order not to violate the laws of probability. While it is technically ‘wrong’, it is of little concerns to because the \\(p\\)-values it concerns are so large they we would not accept \\(H_1\\) in either the adjusted or unadjusted case.\n\nHere is the code for the pairwise comparisons using Bonferroni’s method for controlling type-I error inflation (the only thing we need to change is the adjust argument).\n\n# pairwise comparisons with Bonferroni adjustment\nemmeans(object = model1, specs = 'cond',\n        contr = 'pairwise', adjust = 'bonferroni')\n\nLet’s have a look at the output in the console:\n\n\n\n$emmeans\n cond    emmean   SE df lower.CL upper.CL\n control   97.1 1.73 87     93.7      101\n treat1   102.4 1.73 87     98.9      106\n treat2   113.3 1.73 87    109.9      117\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast         estimate   SE df t.ratio p.value\n control - treat1    -5.23 2.44 87  -2.143  0.1048\n control - treat2   -16.20 2.44 87  -6.633  &lt;.0001\n treat1 - treat2    -10.97 2.44 87  -4.490  0.0001\n\nP value adjustment: bonferroni method for 3 tests \n\n\n\nThe information looks very similar to what we saw when we ran the uncorrected pairwise comparisons. The are only two differences: first, R will state at the bottom of the table that the \\(p\\)-values were adjusted using Bonferroni’s method for three tests. Second, the \\(p\\)-values differ. From looking at the first \\(p\\)-value (the one for the contrast comparing treatment 1 to the control group) we can confirm that R has tripled the original \\(p\\)-value (the difference at the fourth decimal is due to rounding). If we choose to use a Bonferroni correction for the post-hoc comparisons, our conclusions about the result of the ANOVA change slightly. Now, we would state - with the usual confidence - that the significant effect the ANOVA is due to the mean of the second treatment group differing from both the control group’s mean and that from treatment 1. However, we cannot say whether scores in treatment 1 differ from the control group.\nFinally, let’s consider using a less conservative adjustment method, namely Tukey’s method, also known as Tukey’s honestly significant difference (HSD). In technical terms, HSD computes the critical mean difference for which a pairwise comparison is considered significant, and then judges each comparison by that difference. The test statistic \\(q\\) follows the Studentised range distribution, the shape of which depends on the number of groups \\(k\\) and the sample size \\(N\\) (via the distribution’s degrees of freedom). Because the distribution of the \\(q\\)-statistic considers the number of groups, the HSD adjusts for multiple comparisons. Again, the oly theing we need to change about our syntax is the adjust argument (see below):\n\n# pairwise comparisons using Tukey's method\nemmeans(object = model1, specs = 'cond',\n        contr = 'pairwise', adjust = 'tukey')\n\nLet’s again inspect the console output\n\n\n\n$emmeans\n cond    emmean   SE df lower.CL upper.CL\n control   97.1 1.73 87     93.7      101\n treat1   102.4 1.73 87     98.9      106\n treat2   113.3 1.73 87    109.9      117\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast         estimate   SE df t.ratio p.value\n control - treat1    -5.23 2.44 87  -2.143  0.0872\n control - treat2   -16.20 2.44 87  -6.633  &lt;.0001\n treat1 - treat2    -10.97 2.44 87  -4.490  0.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\nOnce more, the output remains largely the same, the only difference being the \\(p\\)-values and the statement about how the \\(p\\)-values were adjusted. We can see that - similar to the Bonferroni - correction, R uses a multiplier on the \\(p\\)-values rather than lowering the tolerated type-I error level, which makes life a little easier for us because we can still use our conventional \\(\\alpha\\)-level to asses the statistical significance of the tests. Although the Tukey correction is less severe than the Bonferroni correction, the result is qualitatively similar: we can confirm that treatment 2 scores differ from those of the other two conditions, but we cannot say whether scores in treatment 1 differ from those in the control group.\n\n\nCustom post-hoc contrasts\nThe final way to run post-hoc tests we will consider here is to run custom contrasts. What this means is that we define ourselves how we want to compare the group means in case of a significant effect in the ANOVA. When using custom contrasts, we can choose how many comparisons we run, which means we want to compare, and how to compare them.The only thing we need to keep in mind is that the contrast weights must add to zero (R does not check this for us; the code will run even if the contrast sum does not add to zero, but the result may be nonsense).\nFor example, we could compare two group means (just as we did with the pairwise comparisons), but we could also test whether two groups differ from a third.\nLet us go back to our ANOVA example and assume that we want to run two post-hoc tests. The first tests whether having any treatment leads to different scores than being in the control condition. The second tests whether the effectiveness of the two treatments differs. Here is what the two contrasts would look like:\n\n\n\n\ncontrol\ntreatment 1\ntreatment 2\n\n\n\n\ncontrast 1\n-1\n+0.5\n+0.5\n\n\ncontrast 2\n0\n1\n-1\n\n\n\nLet’s dissect that! For contrast 1, we compare the control group with the mean of the two treatment groups. In contrast 2, we compare only the two treatment groups; the 0 weight for the control condition means that its mean does not play any role in this contrast.\nHow do we tell R that we want to run custom contrasts? We can do so using the contr argument of the emmeans() function. To be specific, we need to define this argument as a list in which each element is one contrast.\nWhen defining the list of contrasts, each contrast must be a numeric vector of length \\(k\\), where \\(k\\) is the number of groups in the grouping variable that we fed into the emmeans() function (in our case 3). If we chose the wrong vector length, we will receive an error message. We also need to make sure that the sum of each contrast vector is zero (see above). Finally, we can (but do not have to) create these vectors as named elements of the list. Naming the vectors may lead to output that is slightly easier to make sense of.\nHere, we need to decide whether we save the list of contrasts as a separate object or whether we feed the emmeans() function a call of the list() function (the result will be the same, so it is simply a matter of preference). Now let’s look at the R code.\n\n## Alternative 1: separate objects\n\n# create a list containing the custom contrasts\nmy_contrasts = list(\n  treat_vs_control = c(-1, 0.5, 0.5),\n  treat1_vs_treat2 = c(0, 1, -1)\n)\n\n# run the custom post-hoc tests\nemmeans(object = model1, specs = 'cond',\n        contr = my_contrasts, adjust = 'none')\n\n## Alternative 2: don't create a separate list\n\n# run the custom post-hoc tests\nemmeans(object = model1, specs = 'cond',\n        contr = list(\n          treat_vs_control = c(-1, 0.5, 0.5),\n          treat1_vs_treat2 = c(0, 1, -1)), \n  adjust = 'none')\n\nRunning either version of the code will prompt R to test the contrasts and adjust the \\(p\\)-values according to Bonferroni’s method (of course we could select a different adjustment method or turn if off by changing the adjust argument accordingly). We will receive the following console output:\n\n\n\n$emmeans\n cond    emmean   SE df lower.CL upper.CL\n control   97.1 1.73 87     93.7      101\n treat1   102.4 1.73 87     98.9      106\n treat2   113.3 1.73 87    109.9      117\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast         estimate   SE df t.ratio p.value\n treat_vs_control     10.7 2.12 87   5.066  &lt;.0001\n treat1_vs_treat2    -11.0 2.44 87  -4.490  &lt;.0001\n\n\n\nAs we can see, R now returns a table with two contrast tests. Since we named the contrasts properly, it is easy to see what these contrasts tested. For each contrast, R will tell us the estimated mean difference, the associated standard error, and the degrees of freedom (\\(n-k\\), similar to the pairwise comparisons described above). It will also show us the \\(t\\)-value and \\(p\\)-value for each contrast.\nIn our example, both contrasts are significant. This means, we can state - with the usual confidence - that the ANOVA was significant because a) getting a treatment leads to different scores than getting no no treatment (control condition), and b) the treatments differ in effectiveness.\nBut what about multiple comparisons? So far, we have not corrected our custom contrasts for multiple comparisons, but we can do so if we want to. The only thing we need to consider is that we cannot use Tukey’s method as it is specific to pairwise comparisons (oddly enough, Tukey’s method won’t work even if we manually specify pairwise comparisons - if we want to use the HSD method,m we need to set the contr argument to “pairwise”).\nTo adjust for multiple comparisons, we can again specify the argument adjust. If we use custom contrasts, the default for this argument is actually “none”, but we can set it to “bonferroni” (for the classic Bonferroni correction) or “holm” (for the slightly lees conservative Holm-Bonferroni method) instead. Let’s go with Bonferroni. Here is the code:\n\n# run the custom post-hoc tests using \"contrast\" and correct\n# for multiple comparisons using Bonferroni's method\nemmeans(object = model1, specs = 'cond',\n        contr = list(\n          treat_vs_control = c(-1, 0.5, 0.5),\n          treat1_vs_treat2 = c(0, 1, -1)), \n  adjust = 'bonferroni' )\n\nNow let’s look at the console output:\n\n\n\n$emmeans\n cond    emmean   SE df lower.CL upper.CL\n control   97.1 1.73 87     93.7      101\n treat1   102.4 1.73 87     98.9      106\n treat2   113.3 1.73 87    109.9      117\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast         estimate   SE df t.ratio p.value\n treat_vs_control     10.7 2.12 87   5.066  &lt;.0001\n treat1_vs_treat2    -11.0 2.44 87  -4.490  &lt;.0001\n\nP value adjustment: bonferroni method for 2 tests \n\n\n\nAs we can see, not much has changed in terms of statistical significance, which makes sense because the unadjusted \\(p\\)-values were already very low. However, R now tells us at the bottom of the table that \\(p\\)-values were adjusted using the Bonferroni method.\n\nCustom contrasts are really useful because of their flexibility. They allow us to test very specific hypotheses. In fact, if we can state in advance which means in our design should differ (for example, because we have a strong theory to base our hypotheses on), we can technically skip the ANOVA altogether and instead run only the contrast tests. In those cases, we speak of a-priori contrasts.\n\n\n\nA concluding remark on post-hoc comparisons\nWe started the journey into post-hoc analyses using the metaphor of the ANOVA as firing a shotgun into thick fog which - in case we hit something - necessitates wandering into the fog in order to find out what exactly we hit. As the examples above show, the answer to that crucial question depends on which post-hoc comparisons we run (simple \\(t\\)-tests, pairwise comparison contrasts, or custom contrasts) and whether and how we control for type-I error inflation due to multiple comparisons.\nWhen trying to disentangle a significant effect in an ANOVA, we may find ourselves faced with multiple options, each of which is valid and can be argued for convincingly. However, even subtle differences between these options may lead to qualitatively different conclusions. The practical advice here is to preregister exactly which tests we will run in case an ANOVA yields a significant result (ideally by providing the R code for the post-hoc analysis)."
  },
  {
    "objectID": "inference/inference2.html",
    "href": "inference/inference2.html",
    "title": "The Chi² Test",
    "section": "",
    "text": "The \\(\\chi^2\\)-test allows us to test for deviations between observed and expected frequencies. The \\(\\chi^2\\) distribution is formalised as follows:\n\\(\\chi^2 = \\frac{(o_i - e_i)^2}{e_i}\\)\nHere, \\(o_i\\) denotes the observed frequencies in category \\(i\\) whereas \\(e_i\\) represents the expected frequencies given some assumed distribution. In brief, the larger the discrepancies between the observed and expected frequencies, the larger the \\(\\chi^2\\)-value becomes. If it exceeds a certain threshold (defined by the degrees of freedom of the specific test), we reject the Null hypothesis and decide to believe that the deviations of the observed from the expected frequencies are systematic.\nThe most common analyses for which the \\(\\chi^2\\)-test is used are:\nIf we want to run any of these \\(\\chi^2\\)-tests, we can use an R function called chisq.test(). Which function argument the chisq.test() function needs depends on what we want to do. Therefore, we will look at each case separately in the following."
  },
  {
    "objectID": "inference/inference2.html#test-of-observed-frequencies-against-chance",
    "href": "inference/inference2.html#test-of-observed-frequencies-against-chance",
    "title": "The Chi² Test",
    "section": "Test of observed frequencies against chance",
    "text": "Test of observed frequencies against chance\nThis is the simplest form of the \\(\\chi^2\\)-test. To run this test, we need only feed the chisq.test() function a single function argument x, namely a numeric vector of length 2 or greater, containing the frequencies we want to analyse.\nThe function has another argument called p which specifies the expected probabilities against which to test the observed frequencies. The default value of this argument is equi-probability. That is, if we do not specify it, R will automatically test whether the observed frequencies systematically differ from a chance distribution.\nHere is an example of how the syntax for this test looks:\n\n# create a vector containing frequencies for three categories\nv1 = c(19, 33, 21)\n\n# test for deviation from equal probabilities\nchisq.test(x = v1)\n\nIf we run the syntax above, R will conduct the \\(\\chi^2\\)-test by comparing the observed frequencies shown in v1 against chance (expected frequencies are computed using a probability of one third for each of the three categories). The output looks as follows:\n\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  v1\nX-squared = 4.7123, df = 2, p-value = 0.09478\n\n\n\nAs we can see, R tells us which object it ran the test on (v1). It also tells us the empirical \\(\\chi^2\\) value (named X-squared), the degrees of freedom of this test (2 because v1 has three categories), and the associated \\(p\\)-value. In our example, and given that we run the test using the conventional \\(\\alpha\\) level of 5%, the differences in the observed frequencies do not warrant rejecting the Null hypothesis. That is, we cannot state that some of the categories in v1 are more likely than others."
  },
  {
    "objectID": "inference/inference2.html#test-of-observed-frequencies-against-a-predefined-probability",
    "href": "inference/inference2.html#test-of-observed-frequencies-against-a-predefined-probability",
    "title": "The Chi² Test",
    "section": "Test of observed frequencies against a predefined probability",
    "text": "Test of observed frequencies against a predefined probability\nLet’s assume that we want to compare v1 not against chance but against unequal probabilities for the three categories. For example, our Null hypothesis could state that the first and second category should occur in 25% of cases each while category three should account for the remaining 50%. To run this test, the only change we need to make to the way we called the chisq.test() function in the example above is to specify the function argument p and define it as a numeric vector containing the probabilities.\nThis vector of probabilities needs to be of the same length as the vector of observed frequencies. In addition, since we are dealing with probabilities, all elements need to range between 0 and 1, and their sum must equal 1. Let’s have a look at the syntax.\n\n# test for deviation from pre-defined non-equal probabilities\nchisq.test(x = v1, p = c(0.25, 0.25, 0.50))\n\nThe output of the test looks very much like the one in the previous example. Again, R shows us the empirical \\(\\chi^2\\)-value, the degrees of freedom, and the \\(p\\)-value (see below).\n\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  v1\nX-squared = 18.534, df = 2, p-value = 9.448e-05\n\n\n\nThis time, the empirical \\(\\chi^2\\) value is greater, and the corresponding \\(p\\)-value is small enough to reject the Null hypothesis. That is, we can state that v1 was not drawn from a population, in which category 3 is twice as frequent as categories 1 and 2."
  },
  {
    "objectID": "inference/inference2.html#tests-for-systematic-covariation-in-contengency-tables",
    "href": "inference/inference2.html#tests-for-systematic-covariation-in-contengency-tables",
    "title": "The Chi² Test",
    "section": "Tests for systematic covariation in contengency tables",
    "text": "Tests for systematic covariation in contengency tables\nIn some cases, we may not be interested in comparing observed frequencies against a (theoretical) distribution, but rather to compare observed frequencies across two variables (i.e., in a two-dimensional contingency table). Conceptually, this boils down to testing - row-by-row or column-by-column - whether the underlying distribution of one variable is equal across all levels of the other variable.\nTo do so, we need to feed the chisq.test() function a matrix of frequencies as the function argument x. We no longer need to specify the function argument p because the expected frequencies are computed from the marginal frequencies of the contingency table assuming stochastic independence.\nLets look at an example. First, we need some data in table format. We can create such data using the function matrix() or by combining several vectors of equal length using either the function cbind() or rbind() (these functions stack equally long vectors of the same type by column or row). Let’s go with the rbind() version for now.\n\n# create a 2x3 contingency table by stacking \n# two vectors containing frequencies row-wise\ndata1 = rbind(c(762, 327, 68), \n        c(484, 239, 77))\n\n# run a chi² test on the data\nchisq.test(data1)\n\nRunning the code above will perform the \\(\\chi^2\\)-test on the contingency table we created. Here is what the data looks like.\n\n\n\n     [,1] [,2] [,3]\n[1,]  762  327   68\n[2,]  484  239   77\n\n\n\nThe output in the console when running the \\(\\chi^2\\)-test looks as follows:\n\n\n\n\n    Pearson's Chi-squared test\n\ndata:  data1\nX-squared = 11.525, df = 2, p-value = 0.003143\n\n\n\nOnce more, R tells us the empirical \\(\\chi^2\\)-value, the degrees of freedom (for \\(n\\times m\\) tables, the test has \\(n-1\\times m-1\\) degrees of freedom, which makes 2 in our example), and the \\(p\\)-value of the test. From the \\(p\\)-value, we can see that there is systematic co-variation, that is, some cells are more and others less frequent than we would expect if the two variables were unrelated."
  },
  {
    "objectID": "inference/inference2.html#extracting-frequencies-from-data-frames",
    "href": "inference/inference2.html#extracting-frequencies-from-data-frames",
    "title": "The Chi² Test",
    "section": "Extracting frequencies from data frames",
    "text": "Extracting frequencies from data frames\nThe data we want to feed into \\(\\chi^2\\)-tests is usually contained in data frames. We can extract frequencies and even turn them into contingency tables using the table() function. Let’s consider an example, in which we have data from one thousand simulated nerds participants who told us their attitudes toward Star Trek and Star Wars (love, hate, indifferent).\nLet’s say the data is contained in a data frame called my_df. We can use the function head() to inspect the first six rows of the data frame to get a feeling for what the data looks like.\n\n# inspect the top of the data frame\nhead(my_df)\n\nHere is what appears in the console:\n\n\n\n  ID    starTrek    starWars\n1  1 indifferent        love\n2  2        hate indifferent\n3  3 indifferent        love\n4  4        hate        love\n5  5        love indifferent\n6  6        love        love\n\n\n\nWe can now create a contingency table using the table() function and define it as a new object. To this end, we need to feed the table function the two dimensions by which we want to organise the table as separate function arguments. Here is what the code looks like:\n\n# create a 3x3 contingency table of attitudes toward\n# Star Trek and Star Wars\nmy_table = table(my_df$starTrek, my_df$starWars)\n\n# assign dimension names to the table (Star Trek goes first because\n# we entered it first into the table function above)\n# this step is optional but it makes the table more readable\ndimnames(my_table) = list(StarTrek = c('hate', 'indifferent', 'love'),\n                          StarWars = c('hate', 'indifferent', 'love'))\n\nOnce we have defined the new object, we can inspect the contingency table in the console by running its name as R code. Here is what it looks like:\n\n\n\n             StarWars\nStarTrek      hate indifferent love\n  hate          28          49   71\n  indifferent   44          99  142\n  love          81         212  274\n\n\n\nNow that we have created the contingency table from the data frame, we can feed it into the chisq.test() function as its sol argument to test whether there is systematic co-variation between attitudes toward Star trek and Star Wars. Here is what the code looks like.\n\nchisq.test(my_table)\n\nHere is the console output that results from running the test using the syntax above:\n\n\n\n\n    Pearson's Chi-squared test\n\ndata:  my_table\nX-squared = 2.5325, df = 4, p-value = 0.6388\n\n\n\nRunning the test shows us the usual information in the console. In this specific example, the \\(p\\)-value does not allow rejecting the Null hypothesis. In other words, there is no evidence in our simulated data indicating that whether one’s attitude toward Star Trek is in any way related on one’s attitude toward Star Wars."
  },
  {
    "objectID": "inference/inference0.html",
    "href": "inference/inference0.html",
    "title": "The (frequentist) Inference Game",
    "section": "",
    "text": "Before we have a look at specific types of statistical analyses, let’s take a step back and remember what we learned about frequentist statistics when we were undergraduate students."
  },
  {
    "objectID": "inference/inference0.html#the-general-logic-of-frequentists-tests",
    "href": "inference/inference0.html#the-general-logic-of-frequentists-tests",
    "title": "The (frequentist) Inference Game",
    "section": "The general logic of frequentists tests",
    "text": "The general logic of frequentists tests\nIn frequentist statistics, our inferences follow the same logic, irrespective of the kind of test we run. Specifically, we:\n\nchoose a test that is appropriate for the data (e.g., a \\(\\chi^2\\)-test for the analysis of frequencies, or an independent samples \\(t\\)-test for comparing continuous data between two groups)\ndefine a Null hypothesis \\(H_0\\) (usually that the effect is zero) and an alternative hypothesis \\(H_1\\) (usually that the effect is non-zero)\nchoose a tolerated type I error level \\(\\alpha\\) (usually 5%)\nrun the test and inspect the \\(p\\)-value it returns\ntinker with the data and analyses until \\(p\\) drops below .05\nmake a statement about the effect of interest based on the \\(p\\)-value\n\nThe last part is the tricky bit because many people seem to forget that there is an asymmetry in how informative results are when \\(p &lt; .05\\) as opposed to \\(p &gt; .05\\). Let’s have a look at them separately."
  },
  {
    "objectID": "inference/inference0.html#inference-when-p-.05",
    "href": "inference/inference0.html#inference-when-p-.05",
    "title": "The (frequentist) Inference Game",
    "section": "Inference when p < .05",
    "text": "Inference when p &lt; .05\nIf the \\(p\\)-value of our test is smaller than our tolerated \\(\\alpha\\) level (e.g., .05), we consider the effect so be statistically significant. The reason is that the chance of obtaining a result at least as extreme as the one we observed is reasonably low if the effect did, in fact, not exist and \\(H_0\\) were true. Therefore, we accept \\(H_1\\), which states that the effect exists.\nLet’s paraphrase that: If p &lt;.05, we decide to believe that the empirical result cannot be a chance finding, all the while knowing that it can be a chance finding! We even know what the probability of encountering that chance finding are if \\(H_0\\) is true, namely our tolerated \\(\\alpha\\)-level.\nIn essence, we know that there are two possible scenarios that can result in a statistically significant test result:\n\nThe effect does not exist, but we drew an unrepresentative sample.\nThe effect does exist, and the sample accurately represents it.\n\nHowever, if we accept the fact that we may sometimes (namely in \\(\\alpha\\)% of cases) make a mistake when interpreting significant findings as evidence of \\(H_1\\) being true, we can essentially get rid of option 1 when interpreting our results.\nThis laves us with the following interpretations whenever \\(p &lt; .05\\):\n\nThe effect does not exist, but we drew an unrepresentative sample.\nThe effect does exist, and the sample accurately represents it.\n\nAccepting that we may sometimes err (without knowing where we erred), we have now created a situation, in which a statistically significant test result is unambiguous, that is, it has only one valid conclusion: the effect of interest exists (\\(H_0\\) is true)."
  },
  {
    "objectID": "inference/inference0.html#inference-when-p-.05-1",
    "href": "inference/inference0.html#inference-when-p-.05-1",
    "title": "The (frequentist) Inference Game",
    "section": "Inference when p > .05",
    "text": "Inference when p &gt; .05\nLet’s now turn to the other side of the coin, namely situations, in which \\(p &gt; .05\\). Our intuition may tell us that we can conclude that \\(H_1\\) must be wrong and that we can accept \\(H_0\\) to be true. This would lead us to state that the effect we were investigating does not exist. However, this case is a bit more complicated.\nIf we want to conclude that an effect does not exist, we must consider the type II error (overlooking an effect that actually exists). Logic would state that if we can reject \\(H_0\\) when we tolerate a certain type I error level (e.g., \\(\\alpha = .05\\)), then we can also reject \\(H_1\\) if we tolerate the same type II error level (i.e., \\(\\beta = .05\\)).\nSo far so good. The problem is that the type II error level depends on the true size of the effect we are studying, and we do not know the true effect size (if we did, we would not have to run a test, in the first place).\nWhat complicates things further is that the classic \\(H_1\\) (the effect is non-zero) is a composite hypothesis. What does that mean? It means that \\(H_1\\) comprises an infinite number of specific hypotheses, let’s call them \\(H_{1_{i}}\\), which differ from each other regarding the assumed true effect size. If any of the \\(H_{1_{i}}\\) is true, then \\(H_1\\) as a whole is true.\nIf we run a statistical test on our data, there will be some \\(H_{1_{i}}\\) postulating a true effect size that is large enough for the associated type II error level to be smaller than the one we are willing to tolerate. However, there will always be some of \\(H_{1_{i}}\\), for which the postulated effect size so small that the associated type II error level exceeds the one we are willing to accept.\nWhat this means is that we have three possible scenarios that can result in a statistically non-significant test result:\n\nThe effect does not exist, and the sample accurately represents it.\nThe effect exists, and its size is so large that the associated type II error level is below the level we are willing to tolerate.\nThe effect exists, but its size is so small that the associated type II error level exceeds the level we are willing to tolerate.\n\nUsing the same logic as above, we can eliminate option 2 because we are willing to accept a certain number of errors during our academic career. However, we cannot eliminate option 3 in the same fashion because the associated type II error chance is higher than what we are willing to tolerate. That leaves us with two possible scenarios:\n\nThe effect does not exist, and the sample accurately represents it.\nThe effect exists, and its size is so large that the associated type II error level is below the level we are willing to tolerate.\nThe effect exists, but its size is so small that the associated type II error level exceeds the level we are willing to tolerate.\n\nBecause we do not know which of the remaining two scenarios led to our non-significant result, we must acknowledge that it is uninformative. We cannot decide whether the effect is actually zero or whether it is non-zero but too small for us to detect reliably. In other words, when interpreting a non-significant result, we must not conclude that we have shown the effect no to exist. Instead, our answer needs to be: we don’t know (yet) whether the effect exists or not.\n\nThere is an elegant solution to the problem of interpreting non-significant \\(p\\)-values. We can determine the effect size \\(x\\) for which the type II error \\(\\beta\\) is equal to our tolerated type I error level \\(\\alpha\\).\nWe can then state - with the same confidence with which we decide to reject \\(H_0\\) if \\(p &lt; .05\\) - that there is no effect greater than or equal to \\(x\\).\nHow useful that statement is, critically depends on the sample size of our study. With small samples, we may only be able to rule out huge effects. However, when our sample size is large enough, we may get to the point where we can rule out any non-trivial effect (what this means is that effect sizes we cannot rule out with the required confidence are too small to be of practical concern)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Space for QUB-Psy",
    "section": "",
    "text": "Welcome to the website. I hope you enjoy it!"
  },
  {
    "objectID": "graphs/graphs2.html",
    "href": "graphs/graphs2.html",
    "title": "Scatterplots",
    "section": "",
    "text": "Scatterplots are the most common way to plot two continuous or ordinal variables against each other. These two variables translate to coordinates on the x- and y-axis in a coordinate system, and we plot points (in the broad R sense) on these coordinates.\nAs always, we need data before we can plot. We will use the same made-up data we used to plot histograms. As a reminder, the data is stored in df1, and it contains five variables, \\(ID\\), \\(X\\), \\(Y\\), \\(Z\\), and \\(W\\). Here, \\(X\\) and \\(Y\\) are of special interest because they are continuous variables. Here is a brief reminder of what the data looks like.\nID W     X     Y Z\n1  1 1 -1.21 -0.30 0\n2  2 2  0.28  0.31 0\n3  3 3  1.08  1.65 0\n4  4 4 -2.35 -2.35 0\n5  5 5  0.43  0.10 0\n6  6 1  0.51  1.18 0\nIf we want to create a scatterplot, we first need to create a base plot with the appropriate aesthetics. For now, we are content with a simple plot of the two continuous variables in our data set, \\(X\\) and \\(Y\\). The syntax for the base plot would look like this:\n# create a base plot for a scatterplot of X and Y\np1 = ggplot(data = df1, mapping = aes(x = X, y = Y))\nRunning this code will create a new object in our environment. Calling the objects name will show the empty plot in the plot tab. This is how it would look:\nR will choose the limits of the axes to fit the data, and it will display the variable names on the respective axis. It will also show white horizontal and vertical grid lines but nothing else because we have not yet added a layer with a geom.\nWe will now add this second layer using the function geom_point(), which will plot points at the coordinates determined by the values of \\(X\\) and \\(Y\\) in each row of our data frame. The function has several arguments we can use to customise the colour, size, and shape of the points that are applied to all points. The shape of the ‘points’ determines whether they only have one colour value or whether they have a filling colour that is separate from their border colour. Let’s look at the function arguments in more detail:\nAs a reminder, here are the different point shapes and their respective numbers. In this list, colour is blue, and fill is light blue, allowing us to infer which point shapes have only the colour argument and which have both the colour and fill argument.\nLet’s say we want the points in our scatterplot to be purple squares (shape 22) of size 2 with a black border. The following syntax creates the new plot as an object of its own. Calling that new object’s name will show the new plot in the plot tab:\n# add a layer with points to the base plot such that the points\n# are purple squares with a black border\np2 = p1 +\n  geom_point(shape = 22, colour = 'black', fill = 'purple', size = 2)\np2\nHere is what the plot would look like:\nWe can now spice up the plot a bit by adding a third variable that determines the appearance of our points. We will use \\(Z\\) as this third variable. Adding this third variable requires us to create a new base plot, in which we define which aspects of our points appearance should depend on the value of \\(Z\\). Let’s go nuts and and make the points differ according to \\(Z\\) in terms of shape, colour, and fill (we could also let them differ in terms of size, but this is not recommended for discrete variables). Here is what the syntax for this base plot would look like:\n# create a new base plot for a scatterplot with different points for each level of Z\np3 = ggplot(data = df1, mapping = aes(x= X, y = Y,\n                                      shape = Z,\n                                      colour = Z,\n                                      fill = Z))\nThere is one very important thing to note, now that we told the base plot that our the way our geoms look like should vary by a third variable: we can no longer define the respective aesthetics via function arguments of the function adding the geom (we can, however, still enter function arguments that are constant for all points, such as, in our example, their size).\nAs a first step, we can call the geom_point() function without any arguments. This will result in a plot, in which the aesthetics differ according to some default values. That is, ggplot2 comes with a default colour palette from which it chooses the colours, and it has a list of default values for point shapes, too. Let’s give the default version a spin and see how the plot looks like. Here is the syntax:\n# create a new base plot for a scatterplot with different points for each level of Z\np4 = p3 + \n  geom_point()\np4\nOnce we run this code above, this is what appears in our plot tab:\nLet’s inspect that plot. What we can see is that we now have points that differ in terms of their colour and shape. The plot also has a legend showing us which value of \\(Z\\) is associated with which type of point. However, since ggplot2 chooses single-colour point shapes per default, the fill argument had no effect. In addition, the default colours are not ideal because they are difficult to discern for people who have red-green colour blindness. The question is how we can manually change the colours, shapes, and filling of the points."
  },
  {
    "objectID": "graphs/graphs2.html#changing-the-appearance-of-points-manually",
    "href": "graphs/graphs2.html#changing-the-appearance-of-points-manually",
    "title": "Scatterplots",
    "section": "Changing the appearance of points manually",
    "text": "Changing the appearance of points manually\nWhen using ggplot2, we can change the appearance of geoms by adding further layers to the plot overwriting the defaults. The functions we will use the change the appearance of the points are called scale_shape_manual(), scale_colour_manual(), and scale_fill_manual(). As their names suggest, these function allow us to assign the shape, colour, and fill attributes to the points in our plot manually. The only function argument each of the functions needs is called values. Since our grouping variable \\(Z\\) has two levels, we need to make sure that the values argument for each function is a vector of length 2. The nature of these values depends on the respective aesthetics we aim to change (numbers between 1 and 25 for the shape, RGB odes or colour words for colour and fill).\nLet’s say we want our points to be filled circles if \\(Z=0\\) or diamonds if \\(Z=1\\) (shapes 21 and 23, respectively). The circles should be light blue with dark blue borders whereas the diamonds should be orange with red borders. To produce these points, we can simply take the current version of our plot and add three layers using the functions mentioned above. The order in which we add these functions does not matter. The only thing we need to keep in mind is that we assign the values in each of these functions in the correct order. Since \\(Z\\) has two levels (0 and 1), the first value corresponds to the first level of \\(Z\\) (0) and the second value to its second level (1). Here is what the syntax would look like:\n\n# change the appearance of the points by manually assigning shapes, border colours and filling colours\np5 = p4 +\n  scale_shape_manual(values = c(21,23)) + \n  scale_colour_manual(values = c('darkblue', 'red')) +\n  scale_fill_manual(values = c('lightblue', 'orange'))\np5\n\nRunning the code above will create a new object containing the updated plot. Calling the new object’s anme will produce the following plot in the plot tab:\n\n\n\n\n\n\n\n\n\nAs we can see, the points now differ in terms of shape, filling colour and border colour as intended."
  },
  {
    "objectID": "graphs/graphs2.html#adding-regression-lines-to-scatterplots",
    "href": "graphs/graphs2.html#adding-regression-lines-to-scatterplots",
    "title": "Scatterplots",
    "section": "Adding regression lines to scatterplots",
    "text": "Adding regression lines to scatterplots\nA nifty feature of ggplot2 is that it allows us to add regression lines to a scatterplot. All we need to do is add a layer to our plot that contains a new geom, the line we want to add. For the sake of simplicity, we will first do so with the simple scatterplot that does not differentiate between the two levels of \\(Z\\) (p2). The function that allows us to add a line geom for a regression line fitted to the data is called geom_smooth(). What this function does is subject the data to a statistical transformation in the sense of a regression model and then plot the predicted values of that model.\nWe need to define three function arguments for the geom_smooth() function:\n\nmethod (optional): a character value defining the type of regression model being fit to the data; default depend on the sample size, ‘loess’ (local polynomial regression) for fewer than, 1,000 observations, ‘gam’ (generalized additive model) otherwise; we can choose ‘lm’ (linear model) or ‘glm’ (generalised linear model) instead; ‘lm’ will fit an ordinary least squares regression to the data.\nformula (optional): a formula type object telling R how the regression model looks like; default is “y ~ x”, but R will return an ugly message if we do not specify it manually.\nse (optional): a boolean value determining whether the regression line should be accompanied by a confidence band; default is TRUE.\n\nLet’s say we want our scatterplot to contain an ordinary regression line with confidence bands. Here is what the syntax would look like:\n\n# add regression line with confidence band to a scatterplot\np6 = p2 + \n  geom_smooth(method = 'lm', formula = y ~ x, se = T)\np6\n\nRunning the code above will yield the following plot:\n\n\n\n\n\n\n\n\n\nAs we can see, we now have the regression line for a simple regression model in the plot, and the grey band around the line represents the 95% confidence interval around the model predictions. It is quite typical for these confidence bands to be slightly narrower at the extremes of our predictor because we usually have fewer extreme data points. This simply means that the prediction errors are slightly larger at the extremes.\nIf we do not like the colours ggplot2 assign to the regression line per default, we can change it manually inside the geom_smooth() function. Keep in mind that defining colours inside the function for a geom only means that we assign this colour to all geoms created by that function. Since we only have one regression line here, that is not an issue. We can change the colour of the regression line using the colour argument while fill allows us to change the colour of the confidence band. Let’s try to set the colour of the regression line to black and the colour of the confidence band to purple so the aesthetics align better with those of the points. Here is the syntax:\n\n# add regression line with confidence band to a scatterplot\np6 = p2 + \n  geom_smooth(method = 'lm', formula = y ~ x, se = T,\n              colour = 'black', fill = 'purple')\np6\n\nHere is how the plot looks with the customised colours:\n\n\n\n\n\n\n\n\n\nThe final question we will address here is what happens if we have split the points in our scatterplot according to a third variable as in the examples above. Can we plot separate regression lines in this case? The answer is yes. Let’s take the plot with the customised appearance of the points depending on the values of \\(Z\\) (p5) as the basis and add a new layer containing regression lines. Remember that we already defined both colour and fill to vary as a function of \\(Z\\) and that we also specified which colours correspond to which level of \\(Z\\)? The same colours will be applied to the regression lines and confidence bands. What this means is that the colours we specify manually using the scale() function and the rules as to how the colours vary are applied to all geoms. In our case, this is convenient because it means that we do not need to define those colour again. Instead, we can just call the geom_smooth() function without specifying either colour or fill. Syntax as follows:\n\n# add regression lines for each level of Z with confidence bands\np7 = p5 + \n  geom_smooth(method = 'lm', formula = y ~ x, se = T)\np7\n\nHere is the resulting plot:\n\n\n\n\n\n\n\n\n\nNotice how the legend of the new plot not only shows the points for each level of \\(Z\\) but also includes information about the colour of the regression lines and the confidence bands."
  },
  {
    "objectID": "graphs/graphs0.html",
    "href": "graphs/graphs0.html",
    "title": "Making graphs in R",
    "section": "",
    "text": "There are many ways to make graphs in R. Base R already contains functions that allow us to create plots, and there is a plethora of R packages that either contain some functions for plotting related to the core content of the package or even focus on plotting exclusively. As always, there is no right or wrong choice. How you make your graphs is very much a matter of preference.\nOn a very general note, base R provides us with very basic plots that do not necessarily look nice. However, it also gives us the most flexibility when it comes to modifying and enriching our graphs. Given enough experience, we can create very sophisticated graphs, line-by-line, point-by-point, and polygon-by-polygon in R using just the basic plotting functions included in base R. However, using base R is not necessarily efficient, so we may want to consider using specialised R packages for plotting.\nThe package that is arguably the most frequently used for visualisation of scientific data is ggplot2, and this is the package we will be using here. This package has been developed on the basis of the “Grammar of Graphics” (Wilkinson, 2005) and used a special syntax to reflect the core ideas of this grammar. In order to make sense of this syntax - and to better understand how to create plots using ggplot2 - is it important to get a basic grasp of this grammar of graphics."
  },
  {
    "objectID": "graphs/graphs0.html#a-grammar-of-graphics",
    "href": "graphs/graphs0.html#a-grammar-of-graphics",
    "title": "Making graphs in R",
    "section": "A Grammar of Graphics",
    "text": "A Grammar of Graphics\nThe idea underlying the grammar of graphics is graphics have a common principle. They are mappings from data (something abstract represented by numbers or categories) to aesthetics (things we can perceive visually, in this case). The aesthetics include the position in a coordinate system, but also information such as the shape, size, or colour of whatever object we want to plot. The different objects we can plot are geometric objects (geoms). They could be lines, bars, dots, to name just a a few. Plots in In ggplot2 have layers, that is, we build our plots, step-by-step, starting with an empty base plot to which we gradually add geoms until it looks the way we envision it.\nSometimes, it is sufficient to know which aspects of our data to make visible (aesthetics) and how to visualise them (geoms). However, formally, we also need to define the statistical transformation of our data for the purpose of plotting (stats). In some cases, we will use untransformed data (the so called “identity” transformation), while in other cases, we want to transform the data (e.g., by taking its logarithm), and in yet others we may want to summarise the data by taking its mean or median.\nIn sum, when we use ggplot2, the aestheteics, geoms, and stats are the ingredients of our plots. Now all we need to do is learn how to define the aesthetics, which geoms there are at our disposal (we won’t be using half of them), and which stats we need to use to make informative plots.\n\nIn ggplot2, we will use either functions of type geom_ that come with built-in and modifiable stats arguments or stats_ functions that come with built-it in and modifiable geom arguments. This illustrates that we always need to define both stats and geoms when plotting with ggplot2.\nFun fact: The corresponding stats_ and geom_ functions of ggplot2 are interchangeable. Which one we want to use is up to us."
  },
  {
    "objectID": "graphs/graphs0.html#creating-plots-with-ggplot2",
    "href": "graphs/graphs0.html#creating-plots-with-ggplot2",
    "title": "Making graphs in R",
    "section": "Creating plots with ggplot2",
    "text": "Creating plots with ggplot2\nIrrespective of what type of plot we want to create, the first step is always to create a base plot. We do so by calling the ggplot() function from ggplot2. The ggplot() function requires two function arguments:\n\ndata (required): a data frame. We need to tell the function where our data is stored.\nmapping (optional): this function argument tells the function the aesthetics. It requires a function call of another function, aes() (for aesthetics); if specified, defines the default mapping for all layers we will add to the plot; if not specified, we need to specify the aesthetics separately for each layer.\n\nThe aes() function that we need to feed into ggplot() for its mapping argument, takes the following arguments:\n\n\\(x\\) (required): the variable that goes on the x-axis of our plot.\n\\(y\\) (depending on plot type): the variable that goes on the y-axis unless the plot type predefines what goes there (e.g., frequency in a histogram).\n\\(colour\\) (optional): the variable that determines the colour of some geoms (points, lines); for some geoms such as bars, this arguments translates to the border colour.\n\\(fill\\) (optional): the variable that determines the colour of some geoms (bars, violins); if the geom is a two-dimensional object, this is the colour in which we fill the shape, as opposed to its border colour (this is defined by colour).\n\\(size\\) (optional): the variable that determines the size of some geoms (points, lines).\n\\(shape\\) (optional): the variable that determines the shape of points.\n\\(linetype\\) (optional): the variable that determines the type of lines.\n\nThat base plot is an empty plot. While we already define which variables will go on the axes for our plots and which variable we will use to define aesthetics such as the size, colour, or shape of potential geoms, the plot itself will be empty. Why? Because we are yet to add a layer containing a geom. That said, what information we put into the code for the base plot will already depend on the type of plot and how we want to represent the variables of our data. For example, we need to specify what goes on the x-axis and the y-axis for scatterplots and line diagrams, but not for histograms (for them, we only need the specify what goes on the x-axis). Likewise, whether we specify a third variable to determine the colour, size or shape in a scatterplot very much depends on whether we have a third variable to classify data by or whether we consider it sensible to include it.\nPut differently, while the first step is always to create an empty base plot, the specific code for this plot differs slightly depending on which information we want to represent (data) and how we want to represent it (geoms and stats).\n\nA note on colours in R\nBefore we delve further into the realm of plots, it is useful to talk about colours, and how they are implemented in R. In fact, R has several ways to specify colours. Here we will look the the two most commons ones: we can specify colours using colour names as character strings or via RGB-values (Red-Green-Blue values).\nR has a list of colour names that are represented as character strings. This includes the basic colours such as “blue” and “green”, but also differently shaded (and oddly named) variants such as “blanchedalmond” and “darkseagreen”. Here is a list of the colours implemented in this fashion:\n\nThe alternative to calling colours by name is to call them by their RBG-values. In the RGB model, a colour is expressed in terms of its red, green, and blue component. In R, these components are defined as hexadecimal numbers ranging from 00 (a decimal zero) to FF (a decimal 255). The RGB code in R is a character string, in which we precede the RGB values with a hash (#).\nFor example the character string ‘#FF0000’ translates to RGB values of 255, 0, and 0, which means 100% red, 0% green, and 0% blue. This would be equivalent to calling the colour “red” in R. If we called the colour ‘#00FFFF’, the colour would have 0% red component and 100% of each green and blue. We call this colour cyan. Setting all three components to zero will yield black, setting them all to FF will yield white, and setting them, to equal values in between will produce various shades of grey. Using RGB codes to specify colours in R may take some getting used to, but the good news is that there are tons of resources on the internet showing different colours along with their RGB codes (for example, QUB has a corporate branding site that defines which colours we should use in official materials along with the respective RGB codes).\nRStudio has a convenient feature when we write R scripts: it immediately displays the colour for a character string that specifies RGB values (see below).\n\nA final thing to note is that some geoms in R have only one colour while others have two, a border colour and a filling colour. Geoms that have only one colour include lines and most types of points. We can determine their colour via the colour function argument. Other geoms such as bars, circles, and polygons have a border colour which is separate from their filling colour. For those geoms, we can define the border colour via the colour argument and the filling colour via the fill argument.\n\n\nPoint shapes in R\nWhen plotting in R, the term “point” is a broad category of symbols that can, but do not have to be, actual points. We can choose from a range of symbols that are implemented n the points() function from base R. Most R packages that revolve around plotting, and that includes ggplot2, use those exact symbols, so it is good to know what they are. We can determine which symbols to plot via the shape argument. Each symbol is represented by a numeric value as shown in the list below. For example a shape value of 19 (the default value) is a large point. Note that the colour of the ‘points’ in the list below is set to blue (‘#0000FF’ or ‘blue’ in R terms) whereas the fill argument is set to light blue (‘#ADD8E6’ or ‘lightblue’). If the respective shape has only one colour in the list below, we can control this colour with the colour argument of geom_points(); if it has two colours (shapes 21 to 25), we can toggle the border colour with the colour argument and the filling colour with fill.\n\n\n\n\n\n\n\n\n\n\n\nA note on line types\nA final thing to consider is that we can draw different types of lines in R. Whenever we add geoms that consist of or include lines, we can toggle the type of the line via the linetype argument. Line types are represented as numeric values as shown below."
  },
  {
    "objectID": "graphs/graphs0.html#references",
    "href": "graphs/graphs0.html#references",
    "title": "Making graphs in R",
    "section": "References",
    "text": "References\nWilkinson, L. (2005). The Grammar of Graphics. 2nd ed. Statistics and Computing. Springer."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site",
    "section": "",
    "text": "This site contains resources for the upskilling of staff from QUB’s School of Psychology in R and RStudio.\n\nThis site is work in progress. I will continuously add or update content\n\nContributors: - Thomas Schultze-Gerlach (t.schultze(at)qub.ac.uk)"
  },
  {
    "objectID": "graphs/graphs1.html",
    "href": "graphs/graphs1.html",
    "title": "Histograms",
    "section": "",
    "text": "The first type of plot we are going to look at are histograms. Histograms are a common way to visualise the distribution of numerical variables. The variable values are grouped into categories called bins on the x-axis, and the frequency of the data in each bin is represented on the y-axis. Specifically, we plot bars for each bin with the height of the bars corresponding to the frequency of the data in the respective bins.\nThat said, we cannot plot without data, so let’s make up some data that we can use for simple plots. The data contains five variables, \\(ID\\), \\(X\\), \\(Y\\), \\(Z\\), and \\(W\\). \\(ID\\) is simply a subject identifier. Both \\(X\\) and \\(Y\\) are continuous variables, whereas \\(Z\\) is a two-level factor. Finally, \\(W\\) is a numeric variable with four levels. Here is an excerpt of the data, which is stored in a data frame called df1.\nID W     X     Y Z\n1  1 1 -1.21 -0.30 0\n2  2 2  0.28  0.31 0\n3  3 3  1.08  1.65 0\n4  4 4 -2.35 -2.35 0\n5  5 5  0.43  0.10 0\n6  6 1  0.51  1.18 0\nLet’s say we want to plot a histogram of variable \\(X\\) in our data set. We need to know that for histograms, we only need to define what goes on the x-axis in our aesthetics. The reason is that the y-axis is always the frequency of the data, so we do not need to specify that. It is prudent to define the base plot as an R object, so this is what we will do. Here is what the syntax would look like:\n# load library ggplot2, which we need to create the plots\nlibrary(ggplot2)\n\n# create an empty base plot for a histogram of variable X in df1\np1 = ggplot(data = df1, mapping = aes(x = X))\nOnce we run this line of code, our environment will show a new list-type object called “p1”. We can display the base plot by running its name as code. If we do so, R will display the empty base plot in the Plots tab of the Utility and Help section of RStudio’s interface (bottom right). Here is what this plot would look like:\nAs we can see, the variable \\(X\\) appears on the x-axis of the plot. The ggplot() function automatically chooses the range of x-values based on the data. Since we did not specify the \\(y\\)-argument of the aes() function, there is nothing on the y-axis yet. Note that, per default, ggplot() creates a light grey background with white grid lines for its plots. We will learn how to change that later on.\nNow that we have a base plot, it is time to add a new layer with the desired geom. We can do so using the function geom_histogram(). This function has several optional arguments, including the mapping argument we already know from the ggplot() itself. What this means is that we can overwrite the mapping of our base plot for specific geoms we add to it. For now, we will not do that. In this case, the function “inherits” the mapping from the base plot.\nWe will, however, use either of two optional function arguments that define how our histogram looks like, namely:\nOne important aspect to the ggplot() syntax is that we add layers to a plot using the + operator. We can either save the new plot with the added layer as a new object, overwrite the old object with the new plot, have the new plot shown in the plot tab by not defining it as an object. Let’s say we want to group our data into 10 bins in the histogram. Let’s have a look at how the syntax would look like:\n## create a histogram of X with 10 bins\n# update the plot object by overwriting it\np1 = p1 + \n  geom_histogram(bins = 10)\n\n# create the updated plot as a new object\np2 = p1 + \n  geom_histogram(bins = 10)\n\n# have the new plot shown in the plot tab\np1 + \n  geom_histogram(bins = 10)\nIf we chose the third option above or otherwise called the name of the newly defined object, the new plot will be shown in the plot tab. Here is how it would look:\nAs we can see, the plot now has a proper y-axis. The reason is that we chose a geom that determines what goes on this axis (frequency of the data in each bin). We can also verify that the data has been grouped into 10 bins. Finally, now that we have a proper y-axis, there are also horizontal grid lines.\nSo far, so good. However, we may want to change two aspects of the histogram: the colour of the bars (because the dark grey is depressing) and their border (because it looks tidier if there is a border separating the bars). We can do so by specifying two arguments of the geom_histogram() function:\nLet’s say we prefer our bars to appear in light blue with a dark blue border. We can ask R to do that by specifying the colour and fill arguments as shown below:\n# change the filling and border colours of all bars to\n# light blue and dark blue, respectively\np1 = p1 + \n  geom_histogram(bins = 10, colour = 'darkblue', fill = 'lightblue')\nHere is what the plot looks with the new colours assigned:"
  },
  {
    "objectID": "graphs/graphs1.html#interchangeability-of-stats-and-geoms",
    "href": "graphs/graphs1.html#interchangeability-of-stats-and-geoms",
    "title": "Histograms",
    "section": "Interchangeability of stats and geoms",
    "text": "Interchangeability of stats and geoms\nAs mentioned in the introductory section on data visualisation using ggplot2, each geom_ type function comes with a stats argument and vice versa. In case of the geom_histogram() function, the default for that stat argument is “bin”, which divides the data specified as x in the aes() function call of the base plot into several equally spaced bins.\nKnowing this, we can create a histogram using the stat_bin() function instead (see code example below):\n\n# create an empty base plot for a histogram of variable X in df1\np1 = ggplot(data = df1, mapping = aes(x = X))\n\n# add a layer with a histogram using the stat_bin function\np1 + \n  stat_bin(bins = 10, colour = 'darkblue', fill = 'lightblue')\n\nAs we can see below, the resulting graph is identical to the one we created abnove using ’geom_histogram()`."
  },
  {
    "objectID": "graphs/graphs3.html",
    "href": "graphs/graphs3.html",
    "title": "Violin Plots",
    "section": "",
    "text": "We often find ourselves in situations, in which we want to plot a continuous variable against one or more categorical variables. It is quite common for people in such situations to use bar plots with the height of the bars corresponding to the mean for one category. Those bar plots are sometimes (but not always) accompanied by error bars indicating confidence intervals or stand errors to convey a sense of the variability of the data. However, bar plots may not be the best choice…\n\nAs this meme illustrates, bar plots can mask substantial differences between categories because they reduce the displayed information to means and some measure of variability. They do not contain information about the distribution of the data.\nTherefore, it is worthwhile to consider violin plots as an alternative. A Violin plot is essentially a graphical representation of the density of a continuous variable. In order for such a plot to exceed a bar plot in terms of its informational value, we need to supplement the information about the distribution of the continuous variable with the information that bar plots usually provide: means and some measure of variability. In the following, we will learn how to do that using ggplot2.\n\nViolin plots with a single grouping variable\nWe will first look at cases where we have a single categorical variable that groups our data. Before we can plot, we need some data, so let’s make some up. Let’s pretend we ran an experiment on creativity where we compare the creativity of ideas generated by people between three conditions: a condition where 102 people work work on the task sitting in a cardboard box, a condition in which they sit outside the box, and control condition without any boxes. The dependent variable is a rating of participants’ originality across all generated ideas. Here is what the data (stored in df1) would look like:\n\n\n   ID condition creativity\n1 102   control       5.17\n2 102   control       5.42\n3 102   control       3.78\n4 102   control       3.09\n5 102   control       5.21\n6 102   control       0.36\n\n\n\n\n\n\n\n\n\nIf we want to plot the data using violin plots, we first need to set up the base plot. Because we want to plot creativity by condition, we need to define both the x and y argument in the aes() function call when setting up the base plot. Here is the syntax:\n\n# create an empty base plot for a violin plot\np0 = ggplot(data = df1, mapping = aes(x = condition, y = creativity))\n\nAs we can see below, this base plot has a categorical x-axis and a continuous y-axis with the range of the latter being determined by the range of the empirical data of the y-variable.\nNext, we want to add the violins. We can do so using the function geom_violin(), which has “ydensity” as a default stat argument. Due to the interchangeability of stats and geoms in ggplot2, we could also add the layer using the stat_ydensity() function, which has a geom argument with “violin” as the default. See the syntax below:\n\n# add a layer with a violin to the empty base plot\n# Option A: use geom_violin\np1 = p0 +\n  geom_violin()\n\n# Option B: use stat_ydensity\np1 = p0 +\n  stat_ydensity()\n\nEither version of the code will yield the same plot, namely one that contains one violin for each condition. These violins are axis-symmetric depictions of the density of the y-variable, that is, the wider the violin is at a given level of the y-axis, the more frequently we would expect to observe that value of the y-variable. The plot looks as follows:\n\n\n\n\n\n\n\n\n\nAs with all geoms in ggplot2, we can customise their appearance. Violins are polygons, and as such, we can separately define their fill and colour argument to toggle their filling and line colours, respectively. Since we did not specify either of these argument in the base plots function call of aes(), we can define them in the function call of geom_violin() (or stat_ydensity()), and the values we specify there, will be applied equally to all violins. For example, we could ask R to fill the violins in Queen’s vibrant red and colour their borders in Queen’s dark red. Fully opaque filling colours are not necessariyl practical if we want to add more layers to the plot, so we will use the ggplot2 function alpha() to make the filling colour semi-transparent. Here is how the code would look:\n\n# violin plot with custom filling (50% transparency) and border colour  \np1 = p0 +\n  geom_violin(fill = alpha(\"#D6000D\", alpha = 0.50), colour = \"#8F0E20\")\n\nThe resulting graph looks as follows:\n\n\n\n\n\n\n\n\n\nSo far, so good. We now have three beautiful violins, but we still miss information about the means and dispersion of the data that a run-of-the-mill bar plot would convey. We can add this information by adding additional layers. One option is to use the geom_pointrange() function, which will create a point as well as a vertical line for each category of our x-variable. The geom_pointrange() function requires three bits of information for each point range to be drawn:\n\na measure of central tendency (usually the mean, but we can choose the median if we prefer that) that will be represented by the point\nthe lower limit and upper limit of the lines to be drawn, defined by some measure of dispersion (e.g., 95% confidence intervals, standard errors, etc.)\n\nIn order to convey this information to the geom_pointrange() function, we need to define two of its arguments. First, we need to tell it, which stat to use. Instead of the default value for the stat argument (“identity”), we need to set it to “summary”. That means, we tell the function that we will not use the data as is, but instead compute some summary statistics from it. If we do that, we also need to specify the function that does these computations by defining the argument fun.data. We can define this argument by stating the name of the function we want to use (just the name of the function without parentheses). If we want to display means and 95% confidence intervals, we can either use the function mean_cl_normal() (computes a mean plus the parametric 95% confidence interval) or the function mean_cl_boot() (computes the mean and a bootstrapped 95% confidence interval). let’s have a look at the syntax:\n\n# add point ranges depicting means and parametric 95% confidence intervals to the violin plot\np2 = p1 +\n  geom_pointrange(stat = 'summary', fun.data = mean_cl_normal)\n\nHere is what the plot looks like:\n\n\n\n\n\n\n\n\n\nThe function geom_pointrange() allows us to customise the the appearance of the points using the argument shape (see the section on scatterplots for details). Depending on which shape we choose, we can then toggle the filling and/or border colour of the points via the function’s fill and colour arguments.\nThe point ranges already convey the information we are interested in (central tendency and dispersion of the data in each category). However, the lines indicating the 95% confidence intervals look a bit different from the commonly used error bars because they miss the little horizontal bars at the end. If we want, we can add them by adding another layer to the plot using the geom_errorbar() function. Similar to the geom_pointrange() function, we need to define the stat argument as “summary” and provide a function that returns the desired information as the fun.data. We can use the same function as above, namely mean_cl_normal() (or mean_cl_boot() if we prefer non-parametric confidence intervals). A final think to consider is how wide we want to error bars to be. We can toggle their width using the width argument (the default is 1, which creates very wide error bars; we will set it to 0.1 instead).\nSyntax as follows:\n\n# add error bars around the means for each category\np3 = p2 + \n  geom_errorbar(stat = 'summary', fun.data = mean_cl_normal, width = 0.1)\n\nLet’s have a look at the result:\n\n\n\n\n\n\n\n\n\n\nDue to the interchangeability of stats and geoms in ggplot2, we could add point ranges and error bars using the stat_summary() function instead. We would simply need to define the geom argument as “pointrange” and “errorbar”, respectively. The rest of the function calls would be identical to those of the geom_pointrange() and geom_errorbar() functions as shown above.\n\nA final thing we may want to consider is adding the individual data points in the plot. An elegant way to do so is the geom_jitter() function. When we do plots, jittering means to randomly shift the position of a point by a small margin (jittering is useful when we have overlapping data points). The geom_jitter() function lets us define in which range we want to randomly move points horizontally and vertically using the width and height arguments. Each of these arguments must be defined a numeric value. These values correspond to the units of the x- or y-axis. For example, a value of 0.3 means that the points are shifted randomly by up to 0.3 axis units in either direction (up/down or left/right, respectively). When plotting a continuous y-variable against a categorical x-variable, it suffices to shift the points horizontally. Since the default for the height argument is 0, it is sufficient to define the width argument. We will set it to 0.1, here. Similar to geom_point(), we can also change the shape of the points using the shape argument, and we can toggle the fill and/or colour arguments depending on the respective character. In the example below, we use the default points that only have the colour argument. We will use semi-transparent points in order not to distract the viewers from the point ranges and error bars.\n\n# add jittered points to the plots\np4 = p3 +\n  geom_jitter(width = 0.1, colour = alpha('black', alpha = .20))\n\nHere is the resulting plot:\n\n\n\n\n\n\n\n\n\nAs we can see, we now have a rather informative plot, showing each individual data point, the distribution of the data in each condition, as well as information about means and confidence intervals. Much better than bar plots, wouldn’t you agree?"
  },
  {
    "objectID": "inference/inference.html",
    "href": "inference/inference.html",
    "title": "Correlations",
    "section": "",
    "text": "If we want to test whether there is a linear relation between two continuous variables \\(x\\) and \\(y\\), we can compute the correlation coefficient \\(r\\) as a measure of the strength of that relation. The correlation coefficient is defined as follows:\n\\(r_{x,y} = \\frac{cov(x,y)}{\\sigma_x\\sigma_y} = \\frac{(x_i-\\bar{x})(y_i-\\bar{y})}{\\sigma_x\\sigma_y}\\)\nAs we can see, the correlation coefficient is the standardised covariance of \\(x\\) and \\(y\\). The standardisation (dividing the covariance by the product of the standard deviations of \\(x\\) and \\(y\\)) ensures that - unlike the covariance - the correlation coefficient is limited to the range from -1 to 1. Here, -1 indicates a perfect negative linear relationship, 1 indicates a perfect positive linear relationship, and 0 means that there is no linear relationship between \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "inference/inference.html#computing-correlations-in-r",
    "href": "inference/inference.html#computing-correlations-in-r",
    "title": "Correlations",
    "section": "Computing correlations in R",
    "text": "Computing correlations in R\nWe can compute the correlation coefficient \\(\\rho\\) in R using the function cor. We can use the cor function in two ways:\nThe first way is to feed the function two numerical vectors of the same length as two separate function arguments, x and y. R will then compute the correlation between x and y. Here is what the syntax looks like:\n\n# create a numeric vector\nv1 = -10:10\n\n# create a second numeric vector (square of v1)\nv2 = v1^2\n\n# compute the correlation of v1 and v2\ncor(x = v1, y = v2)\n\nIf we run this code, R will return the correlation coefficient a single number in the console (see blow).\n\n\n\n[1] 0\n\n\n\nIn our example, the correlation is zero despite y being a function of x (\\(y = x^2\\)). The reason is that the relationship of x and y is non-linear.\nThe second way, in which we can use the cor function is to feed it a numeric matrix (or data frame that contains only numeric variables) as the sole function argument x. If we do that, R will correlate each column of the matrix (or data frame) with all other columns - including itself - and create a correlation matrix. Here is an example.\n\n# create three numeric vectors\nv1 = 1:10     # just the numbers from 1 to 10\nv2 = sqrt(v1) # the square root of v1\nv3 = log(v1)  # the natural logarithm of v1\n\n# generate a numeric 10x3 matrix from the\n# three vectors using the cbind function\nm1 = cbind(v1, v2, v3)\n\n# compute the correlation matrix for m1\ncor(x = m1)\n\nRunning this code will lead R to return a numeric matrix containing the correlations in the console. Since we fed the cor function a matrix with three columns, the output will be a \\(3\\times3\\) matrix (see below).\n\n\n\n          v1        v2        v3\nv1 1.0000000 0.9891838 0.9516624\nv2 0.9891838 1.0000000 0.9861685\nv3 0.9516624 0.9861685 1.0000000\n\n\n\nThree things about the correlation matrix are noteworthy:\n\nIn the diagonal of the matrix, each correlation is 1. This makes sense because in the diagonal, we correlate each variable with itself.\nThe number above the diagonal mirror those below it. This make sense, too. The correlation of v1 and v2 is the same as the correlation of v2 and v1 (the order of the variables does not matter when computing a correlation).\nSpecific to our example, correlations between the three variables are close to perfect even though their relationships are non-linear. This shows how good linear approximations may be in some cases even thought the the assumption of a linear relationship is technically wrong."
  },
  {
    "objectID": "inference/inference.html#rank-order-correaltions",
    "href": "inference/inference.html#rank-order-correaltions",
    "title": "Correlations",
    "section": "Rank order correaltions",
    "text": "Rank order correaltions\nThe default correlation coefficient we can compute with the cor function is the product-moment correlation (as defined formally above). However, the cor function also allows us to compute rank order correlations if we so desire. Rank order correlations are robust against outliers, which makes them preferable to product-moment correlations in some situations (they are also sometimes referred to as non-parametric correlations).\nIn order to change the type of correlation coefficient, we can specify the function argument method when calling the cor function. This argument has a default called “pearson”, which computes the product-moment correlation. If we instead change it to “kendall” or “spearman”, R will instead compute Kendall’s \\(\\tau\\) or Spearman’s \\(\\rho\\), both of which are rank order correlations."
  },
  {
    "objectID": "inference/inference.html#how-to-handle-missing-data",
    "href": "inference/inference.html#how-to-handle-missing-data",
    "title": "Correlations",
    "section": "How to handle missing data",
    "text": "How to handle missing data\nWhen we want to compute correlation coefficients using the cor function, we need to make sure that there are no missing values in the objects we feed the function. Otherwise, R will return NA whenever one of the contribution observations is NA. In order to compute valid correlation coefficient, we need to use the function argument use. The default value is “everything”, nut if some of our observations are NA we don’t want to use everything. We have two options: “complete.obs” and “pairwise.complete.obs”.\nIf we want to compute the correlation of two variables, both options do the same: they remove all cases in which there is a missing value before computing the correlation coefficient.\nIf we want R to compute correlation matrix for three or more variables instead, the two values for use differ slightly. Using “complete.obs” will prompt R to remove all cases with at least one NA in any of the variables. This will ensure equal sample sizes for all computed correlation coefficient but may result in an unnecessary loss of data. For example, when a person has a missing in only one variable, we can still use their data to compute correlations between the remaining variables.\nIf we use “pairwise.complete.obs” instead, R will only exclude cases with an NA for the computation of those correlation coefficients which involve the missing response. That means, we use as many observations as possible to compute each correlation at the risk of creating slight imbalances between correlation coefficients regarding their underlying sample."
  },
  {
    "objectID": "inference/inference.html#testing-for-significant-correlations",
    "href": "inference/inference.html#testing-for-significant-correlations",
    "title": "Correlations",
    "section": "Testing for significant correlations",
    "text": "Testing for significant correlations\nSo far, we have only computed correlation coefficients. However, most of the time, we will also want to know whether the correlations in our data are so strong that we can reject the Null hypothesis that there is no linear relationship between the variables.\nWe can test for statistical significance using the function cor.test. This function takes two numeric vectors of the same length as function arguments x and y. It also automatically removes cases with NAs.\nJust as with the cor function, we can specify the type of correlation coefficient we want to test for significance using the function argument method. As with the cor function, the default is “pearson”, but we can change it to “kendall” or “spearman” if we want.\nSince direction matters when dealing with correlations, we can also specify the type of our alternative hypothesis using the function argument alternative. The default is “two.sided”, which tests whether the correlation is different from zero. We can change this argument to “greater” or “less” to test the directional hypotheses that r is positive or that it is negative, respectively.\nLet’s look at an example, in which we ant to run a two-tailed significant test on a product-moment correlation. Here is what the code would look like:\n\n# create two numeric vectors\nv1 = 1:10     # just the numbers from 1 to 10\nv2 = exp(v1)  # e to the power of v1 (because why not)\n\n# test of significant correlation of v1 and v2\ncor.test(x = v1, y = v2)\n\nRunning this code will prompt R to return a lot if information in the console. Here is that the output looks like:\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  v1 and v2\nt = 2.9082, df = 8, p-value = 0.01964\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.159019 0.927748\nsample estimates:\n      cor \n0.7168704 \n\n\n\nFrom this output, we can tell:\n\nthe type of correlation we tested, namely the product-moment correlation (makes sense because this default,a nd we did not change it)\nwhat data the correlation test is based on (v1 and v2)\nthat our test was two-tailed because our alternative hypothesis was that \\(r\\) is non-zero (also a default we did not change)\nthe test statistics of the underlying \\(t\\)-test, that is, the empirical \\(t\\) value, its degrees of freedom, and the \\(p\\)-value\nthe correlation coefficient and a 95% confidence interval (we could change the confidence level to something else using the conf.level argument; 0.95 is the default)\n\nIn the example above, the \\(p\\)-value of the test justifies rejecting the Null hypothesis.\nLet’s look at another example, in which we run a one-tailed test of significance on Spearman’s \\(\\rho\\), testing whether the correlation is greater than zero. Code as follows:\n\n# create two numeric vectors\nv1 = c(11, 14, 15, 10, 9, 4, 7, 8, 17, 6)\nv2 = c(12, 15, 17, 18, 5, 6, 9, 14, 8, 11)\n\n# test of significant correlation of v1 and v2\ncor.test(x = v1, y = v2, method = \"spearman\",\n         alternative = \"greater\")\n\nThe output in the console looks slightly different, mainly because we tested a rank order correlation (see below).\n\n\n\n\n    Spearman's rank correlation rho\n\ndata:  v1 and v2\nS = 102, p-value = 0.1395\nalternative hypothesis: true rho is greater than 0\nsample estimates:\n      rho \n0.3818182 \n\n\n\nAs we can see, R again tells us which type of correlation we tested, and our alternative hypothesis looked like. Since we tested Spearman’s \\(\\rho\\), the underlying statistical test is not a \\(t\\)-test (remember that this is a non-parametric correlation). Accordingly, R will show us a different test statistic names \\(S\\) and the corresponding \\(p\\)-value. Finally, R will tell us the correlation coefficient, but there will be no 95% confidence interval (R cannot compute it without assuming an underlying parametric function).\nIn this second example, Spearman’s \\(\\rho\\) is not significantly greater than zero. Thus, we have to concede that the result is uninformative."
  },
  {
    "objectID": "inference/inference1.html",
    "href": "inference/inference1.html",
    "title": "The t-Test",
    "section": "",
    "text": "The \\(t\\)-test is a parametric test designed to test whether the mean of a (approximately) normally distributed continuous variable \\(X\\) with unknown variance \\(\\sigma^2\\) differs from a certain fixed value \\(\\mu\\), for example, zero.\nThe test statistic \\(t\\) is formally defined as follows:\n\\(t = \\frac{\\bar{X}-\\mu}{\\frac{\\hat{\\sigma}} {\\sqrt{n}}}\\)\nHere, \\(\\bar{X}\\) is the empirically determined sample mean of the variable \\(X\\), \\(\\mu\\) is the assumed population parameter under \\(H_0\\), \\(\\hat{\\sigma}\\) is the estimate of the population standard deviation derived from the sample, and \\(n\\) is the size of the sample the test is based on.\nLike many statistical test, the \\(t\\)-test is a signal-to-noise ratio. The numerator represents the signal (i.e., the effect we are interested in) while the denominator represents the noise (based on the sampling variance).\nThe \\(t\\)-test comes in three flavours:\nThe only difference between the three types of \\(t\\)-tests is the definition of \\(\\bar{X}\\) and \\(\\sigma^2\\). For one-sample \\(t\\)-tests \\(X\\) is the variable we are measuring, and \\(\\sigma^2\\) is its variance.\nFor the other two types of \\(t\\)-test, \\(X\\) is the difference between two variables \\(Y\\) and \\(Z\\). Depending on the type of \\(t\\)-test, \\(Y\\) and \\(Z\\) are either two variables measured in the same group or one variable measured in two independent groups. In both cases, \\(\\sigma^2\\) is the variance of the difference \\(Y-Z\\).\nWe can run all three types of \\(t\\)-tests in R using the function t.test(). This function is quite flexible. It has several function arguments that we can use to specify the type of \\(t\\)-test and how our \\(H_0\\) and \\(H_1\\) should look like. Let’s have a look at the most important function arguments:\nNow that we have a rough understanding of what we can do with the t.test() function, let’s give it a spin."
  },
  {
    "objectID": "inference/inference1.html#the-one-sample-t-test",
    "href": "inference/inference1.html#the-one-sample-t-test",
    "title": "The t-Test",
    "section": "The one-sample t-test",
    "text": "The one-sample t-test\nFor starters, we will consider the most simple case, namely the one-sample \\(t\\)-test. For example, we could test whether the mean of a vector of numbers differs from zero.\n\nWe can generate some normally distributed data for our examples using the rnorm() function. We simply need to feed the function three arguments: the size of the simulated sample (n), the true mean (mean), and the true standard deviation (sd). Calling the rnorm() function will then result in R producing a pseudo-random sample drawn from the specified normal distribution.\nIf we want to create a reproducible pseudo-random sample, we can call the function set.seed() before calling rnorm(). If we feed the set.seed() function an arbitrary number, the following pseudo-random draw will always be the same.\n\nHere is what the R code would look like:\n\n# create some normally distributed data using the rnorm function.\n# use set.seed to make sure the outcome of the data simulation is reproducible\nset.seed(1234) \n\n# simulate 50 data points drawn from a normal distribution with mean = 0.50 and sd = 1\nv1 = rnorm(n = 50, mean = 0.50, sd = 1)\n\n# run a one-sample t-test to test if the mean of v1 differs from zero\nt.test(x = v1)\n\nAs we can see, the code for the two-tailed one-sample t-test is very simple. Now let’s have a look at what appears in the console if we run the code above.\n\n\n\n\n    One Sample t-test\n\ndata:  v1\nt = 0.37508, df = 49, p-value = 0.7092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2045796  0.2984736\nsample estimates:\n mean of x \n0.04694699 \n\n\n\nThe output of the \\(t\\)-test function is actually a list (we can easily verify that by saving the output of the test as a new object). R displays the information contained in that list in an easily readable fashion in the console. In particular, R will tell us:\n\nthe type of \\(t\\)-test we ran (one-sample \\(t\\)-test)\nthe empirical \\(t\\)-value\nthe test’s degrees of freedom (\\(n-1\\))\nthe \\(p\\)-value\nthe type of \\(H_1\\)\nthe mean of \\(X\\) and a 95% confidence interval around it\n\nIn the example above, the \\(p\\)-value is greater than the conventional significance level of .05. Accordingly, we cannot accept \\(H_1\\) and have to retain \\(H_0\\).\nLet’s now consider a second example, in which we want to test whether the mean of a numeric vector exceeds 50.\n\n# generate normally distributed data (n = 100) with true mean = 60 and and sd = 10 (make the draw reproducible using set.seed)\nset.seed(2345)\nv2 = rnorm(n = 100, mean = 60, sd = 10)\n\n# run a one-tailed one-sample t-test testing whether the mean of v2 exceeds 50\nt.test(x = v2, mu = 50, alternative = 'greater')\n\nLet’s have a look at the output in the console:\n\n\n\n\n    One Sample t-test\n\ndata:  v2\nt = 11.147, df = 99, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is greater than 50\n95 percent confidence interval:\n 58.77243      Inf\nsample estimates:\nmean of x \n 60.30786 \n\n\n\nThe output states correctly that we tested the a directional alternative hypothesis and what value we tested against (\\(H_1: \\mu &gt; 50\\)). Due to the directional nature of the test, the 95% confidence interval around the mean is one-tailed, too (its upper limit is “Inf”, meaning infinite).\nOf course, R also tells us the \\(t\\)-value, degrees of freedom, and \\(p\\)-value of the test. This time, we can reject \\(H_0\\) and accept \\(H_1\\), accordingly."
  },
  {
    "objectID": "inference/inference1.html#the-paired-t-test",
    "href": "inference/inference1.html#the-paired-t-test",
    "title": "The t-Test",
    "section": "The paired t-test",
    "text": "The paired t-test\nLet’s now turn to cases, in which we want to test for the difference between two variables measured in the same group. This could be two variables measured at the same time, or the same variable measured at two times.\nIn order to run a paired \\(t\\)-test, we need to feed the t.test() function two numerical vectors of the same length as its x and y arguments. R will pair each element of the first vector with the corresponding element of the second vector. Therefore, we need to ensure that the data for both vectors are in the same order (usually not a problem if we use data frames).\nLet’s look at an example, in which we want to test whether the stress level of 100 simulated people decreases between two measurement occasions T1 and T2.\n\nSince we want to simulate correlated data, using the rnorm() function won’t help us. However, there is a useful function called mvrnorm() in the MASS package. This function allows us to generate data from multivariate normal distributions.\nWe can feed the mvrnorm() the following arguments:\n\nn: the number of observations\nmu: a vector of length \\(k\\) containing the means of the generated variables\nSigma: the \\(k\\times k\\) variance-covariance matrix\n\nThe function output will be a \\(n \\times k\\) matrix with each column containing the simulated data of one variable.\n\nHere is the code for the simulated data:\n\n# load the package MASS for its function mvrnorm\nlibrary(MASS)\n\n# generate a data frame containing data for 100 simulated people who reported their stress levels twice\n# In this example, we will set the true stress level at T1 to 0.50, the one at T2 to 0.10, and the correlation of the to measurements to r = 0.50\n# we also use set.seed for reproducibility\nset.seed(3456)\nmy_df = data.frame(\n  ID = 1:100,\n  stress = mvrnorm(\n    n = 100,\n    mu = c(0.50, 0.10),\n    Sigma = matrix(c(1, 0.50, \n                     0.50, 1), nrow = 2)\n  )\n)\n\n# assign better names to the two stress variables\nnames(my_df)[2:3] = c('stress_T1', 'stress_T2')\n\nWe can now run the paired \\(t\\)-test. Since we want to run a one-tailed test, we need to think carefully about how to specify our alternative hypothesis. Our hypothesis states that stress at T2 should be lower than stress at T1. If we enter stress at T1 first, R will compute \\(X\\) by subtracting T2 stress levels from T1 stress levels. If our hypothesis is correct, the difference of means should be positive. Therefore, we need to define the alternative hypothesis as “greater” (if we were to enter stress at T2 first, we would need to specify it as “less”, accordingly. Here is what the code looks like:\n\n# test whether there is a significant decrease in stress form T1 to T2\nt.test(x = my_df$stress_T1, \n       y = my_df$stress_T2, \n       alternative = 'greater',\n       paired = T)\n\nHere is the console output:\n\n\n\n\n    Paired t-test\n\ndata:  my_df$stress_T1 and my_df$stress_T2\nt = 3.5033, df = 99, p-value = 0.0003458\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 0.2114204       Inf\nsample estimates:\nmean difference \n      0.4019068 \n\n\n\nR will now inform us that we ran a paired \\(t-test\\) (this is correct as we set the paired argument to TRUE; otherwise, R would have computed and independent samples \\(t\\)-test).\nIt also tells us which alternative hypothesis we tested, namely that the difference between T1 and T2 stress levels was positive (greater than 0). Furthermore, R reports the mean difference, the one-tailed 95% confidence interval around it.\nFinally, it shows is the empirical \\(t\\)-value, degrees of freedom (\\(n-1\\), similar to the one-sample test), and the \\(p\\)-value. In this example, stress was significantly lower at T2than at T1, allowing us to accept \\(H_1\\).\n\nThe statistically well-versed reader may interject that a paired-sample \\(t\\)-test is the same as a one-sample \\(t\\)-test on the difference of the two variables involved. That is technically correct!\nWe can easily verify it by computing a new variables as the difference between the two variables we want to compare. First, we run a paired-sample \\(t\\)-test by feeding the t.test() function both variables. Then, we run a one-sample \\(t\\)-test and feed it only the new difference variable. The results of the two tests will be identical, ceteris paribus."
  },
  {
    "objectID": "inference/inference1.html#independent-samples-t-test",
    "href": "inference/inference1.html#independent-samples-t-test",
    "title": "The t-Test",
    "section": "Independent samples t-test",
    "text": "Independent samples t-test\nWe will now turn to the last version of \\(t\\)-test, the \\(t\\)-test for independent samples. As with the paired \\(t\\)-test, we need to feed the t.test() function two numeric vectors containing the data we want to compare. However, since the data are assumed to be independent, the two vectors need not be of the same length, and the order of the elements does not matter.\nFirst, we are going to simulate a new data frame containing the data from 100 participants who were randomly assigned to two experimental conditions labelled “experimental” and “control” for lack of imagination. The only measured variable of interest is the dependent variable “dv”.\n\n# generate data for 100 participants, half of which \n# were assigned to an experimental condition while\n# the other half was assigned to the control group\n# we use set.seed for reproducible simulation\nset.seed(4567)\n\nmy_df2 = data.frame(\n  ID = 1:100,\n  condition = rep(c('experimental', 'control'), each = 50),\n  dv = c(\n    rnorm(n = 50, mean = 0.50, sd = 1), # experimental group data\n    rnorm(n = 50, mean = 0, sd = 1)     # control group data\n  )\n)\n# The function rep repeats all elements of the vector\n# a number of time specified by the argument 'each'\n\nNow that we have a proper simulated data set, we can run the independent samples \\(t\\)-test. Let’s assume we want to test whether the groups differ, that is, our \\(H_1\\) is unspecific, and we run a two-tailed \\(t\\)-test on the data. Here is what the code looks like:\n\n# test the hypothesis that the mean of dv differs\n# between the experimental and the control group\nt.test(x = my_df2$dv[my_df2$condition == 'experimental'],\n       y = my_df2$dv[my_df2$condition == 'control'],\n       paired = FALSE, var.equal = FALSE)\n\nTechnically, we do not need to specify the paired and var.equal argument because FALSE is the default for both. Let’s have a look at the output.\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  my_df2$dv[my_df2$condition == \"experimental\"] and my_df2$dv[my_df2$condition == \"control\"]\nt = 3.7368, df = 94.704, p-value = 0.0003186\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.3261383 1.0654884\nsample estimates:\nmean of x mean of y \n0.7206913 0.0248780 \n\n\n\nR tells us that we ran a Welch two-sample \\(t\\)-test. That is R’s way of letting us know that it corrected the test’s degrees of freedom for unequal variances. The standard \\(t\\)-test for independent samples has \\(n-2\\) degrees of freedom, so in our example, we would expect 98. However, due to slight differences in the empirically obtained variances between the two groups, the correction scales the degrees of freedom down to 94.704 (if you see a \\(t\\)-test with fractional degrees of freedom somewhere you now know what happened). The rest of the information is similar to what we saw in the previous examples. In our case, the data support accepting \\(H_1\\). If, for some reason, we do not want R to correct for unequal variances, we can set the var.equal argument to TRUE. Depending on how strong the inequality of variances is, this might have an impact on the \\(p\\)-value. Let’s give it a try with the data above.\n\n# test the hypothesis that the mean of dv differs\n# between the experimental and the control group\nt.test(x = my_df2$dv[my_df2$condition == 'experimental'],\n       y = my_df2$dv[my_df2$condition == 'control'],\n       paired = FALSE, var.equal = TRUE)\n\nHere is the console output:\n\n\n\n\n    Two Sample t-test\n\ndata:  my_df2$dv[my_df2$condition == \"experimental\"] and my_df2$dv[my_df2$condition == \"control\"]\nt = 3.7368, df = 98, p-value = 0.0003133\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.3262991 1.0653276\nsample estimates:\nmean of x mean of y \n0.7206913 0.0248780 \n\n\n\nAs we can see, R now states that we ran a “two sample \\(t\\)-test”. The degrees of freedom amount to the expected 98, and the \\(p\\)-values differs slightly from that of the Welch \\(t\\)-test.\nIf we run an independent samples \\(t\\)-test and our grouping variable (in our example the variable “condition”) has only two levels, we can use an alternative syntax containing a formula.\nAn R formula is a social type of R object that requires a specific syntax. Generally speaking this syntax looks like this:\n\n# generic formula stating that y is a function of x\ny ~ x\n\nWhat this means is that \\(y\\) is a function of \\(x\\). Usually, R accepts only a single variable to the left of the tilde operator (~), whereas we can combine multiple variables on it’s right side. We will return to formulae later when we discuss linear models. For now the simple version is sufficient.\nIn case of an independent \\(t\\)-test, we can feed the t.test() function a formula containing the variable names as well as another function argument called data, which requires a data frame as input. Here, we can tell R which data frame our variables are contained in.\nCaveat: If we use a formulae for the \\(t\\)-test, we cannot define the paired argument and, thus, must omit it.\nLet’s have a look at the alternative syntax for our test above.\n\n# test the hypothesis that the mean of dv differs\n# between the experimental and the control group\nt.test(dv ~ condition, data = my_df2,\n       var.equal = FALSE)\n\nAs we can see below, the output is the same as for the initial Welch \\(t\\)-test above. Which type of syntax we use depends on our personal preferences.\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  dv by condition\nt = -3.7368, df = 94.704, p-value = 0.0003186\nalternative hypothesis: true difference in means between group control and group experimental is not equal to 0\n95 percent confidence interval:\n -1.0654884 -0.3261383\nsample estimates:\n     mean in group control mean in group experimental \n                 0.0248780                  0.7206913 \n\n\n\nCongratulations, you are now able to do \\(t\\)-tests of all flavours in R!"
  },
  {
    "objectID": "inference/inference3.html",
    "href": "inference/inference3.html",
    "title": "Correlations",
    "section": "",
    "text": "If we want to test whether there is a linear relation between two continuous variables \\(X\\) and \\(X\\), we can compute the correlation coefficient \\(r\\) as a measure of the strength of that relation. The correlation coefficient is defined as follows:\n\\(r_{x,y} = \\frac{cov(x,y)}{\\sigma_x\\sigma_y} = \\frac{\\frac{1}{N}\\sum_{i=1}^{N}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sigma_x\\sigma_y}\\)\nAs we can see, the correlation coefficient is the standardised covariance of \\(X\\) and \\(Y\\). The standardisation (dividing the covariance by the product of the standard deviations of \\(X\\) and \\(Y\\)) ensures that - unlike the covariance - the correlation coefficient is limited to the range from -1 to 1. Here, -1 indicates a perfect negative linear relationship, 1 indicates a perfect positive linear relationship, and 0 means that there is no linear relationship between \\(X\\) and \\(Y\\)."
  },
  {
    "objectID": "inference/inference3.html#computing-correlations-in-r",
    "href": "inference/inference3.html#computing-correlations-in-r",
    "title": "Correlations",
    "section": "Computing correlations in R",
    "text": "Computing correlations in R\nWe can compute the correlation coefficient \\(\\rho\\) in R using the function cor(). We can use the cor() function in two ways:\nThe first way is to feed the function two numerical vectors of the same length as two separate function arguments, x and y. R will then compute the correlation between them. Here is what the syntax looks like:\n\n# create a numeric vector\nv1 = -10:10\n\n# create a second numeric vector (square of v1)\nv2 = v1^2\n\n# compute the correlation of v1 and v2\ncor(x = v1, y = v2)\n\nIf we run this code, R will return the correlation coefficient a single number in the console (see blow).\n\n\n\n[1] 0\n\n\n\nIn our example, the correlation is zero despite \\(Y\\) being a function of \\(X\\) (\\(Y = X^2\\)). The reason is that the relationship of \\(X\\) and \\(Y\\) is non-linear.\nThe second way, in which we can use the cor() function is to feed it a numeric matrix (or data frame that contains only numeric variables) as the sole function argument \\(X\\). If we do that, R will correlate each column of the matrix (or data frame) with all other columns - including itself - and create a correlation matrix. Here is an example.\n\n# create three numeric vectors\nv1 = 1:10     # just the numbers from 1 to 10\nv2 = sqrt(v1) # the square root of v1\nv3 = log(v1)  # the natural logarithm of v1\n\n# generate a numeric 10x3 matrix from the\n# three vectors using the cbind function\nm1 = cbind(v1, v2, v3)\n\n# compute the correlation matrix for m1\ncor(x = m1)\n\nRunning this code will lead R to return a numeric matrix containing the correlations in the console. Since we fed the cor() function a matrix with three columns, the output will be a \\(3\\times3\\) matrix (see below).\n\n\n\n          v1        v2        v3\nv1 1.0000000 0.9891838 0.9516624\nv2 0.9891838 1.0000000 0.9861685\nv3 0.9516624 0.9861685 1.0000000\n\n\n\nThree things about the correlation matrix are noteworthy:\n\nIn the diagonal of the matrix, each correlation is 1. This makes sense because in the diagonal, we correlate each variable with itself.\nThe number above the diagonal mirror those below it. This make sense, too. The correlation of v1 and v2 is the same as the correlation of v2 and v1 (the order of the variables does not matter when computing a correlation).\nSpecific to our example, correlations between the three variables are close to perfect even though their relationships are non-linear. This shows how good linear approximations may be in some cases even thought the the assumption of a linear relationship is technically wrong."
  },
  {
    "objectID": "inference/inference3.html#rank-order-correaltions",
    "href": "inference/inference3.html#rank-order-correaltions",
    "title": "Correlations",
    "section": "Rank order correaltions",
    "text": "Rank order correaltions\nThe default correlation coefficient we can compute with the cor() function is the product-moment correlation (as defined formally above). However, the cor() function also allows us to compute rank order correlations if we so desire. Rank order correlations are robust against outliers, which makes them preferable to product-moment correlations in some situations (they are also sometimes referred to as non-parametric correlations).\nIn order to change the type of correlation coefficient, we can specify the function argument method when calling the cor() function. This argument has a default called “pearson”, which computes the product-moment correlation. If we instead change it to “kendall” or “spearman”, R will instead compute Kendall’s \\(\\tau\\) or Spearman’s \\(\\rho\\), both of which are rank order correlations."
  },
  {
    "objectID": "inference/inference3.html#how-to-handle-missing-data",
    "href": "inference/inference3.html#how-to-handle-missing-data",
    "title": "Correlations",
    "section": "How to handle missing data",
    "text": "How to handle missing data\nWhen we want to compute correlation coefficients using the cor() function, we need to make sure that there are no missing values in the objects we feed the function. Otherwise, R will return NA whenever one of the contribution observations is NA. In order to compute valid correlation coefficient, we need to use the function argument use. The default value is “everything”, nut if some of our observations are NA we don’t want to use everything. We have two options: “complete.obs” and “pairwise.complete.obs”.\nIf we want to compute the correlation of two variables, both options do the same: they remove all cases in which there is a missing value before computing the correlation coefficient.\nIf we want R to compute correlation matrix for three or more variables instead, the two values for use differ slightly. Using “complete.obs” will prompt R to remove all cases with at least one NA in any of the variables. This will ensure equal sample sizes for all computed correlation coefficient but may result in an unnecessary loss of data. For example, when a person has a missing in only one variable, we can still use their data to compute correlations between the remaining variables.\nIf we use “pairwise.complete.obs” instead, R will only exclude cases with an NA for the computation of those correlation coefficients which involve the missing response. That means, we use as many observations as possible to compute each correlation at the risk of creating slight imbalances between correlation coefficients regarding their underlying sample."
  },
  {
    "objectID": "inference/inference3.html#testing-for-significant-correlations",
    "href": "inference/inference3.html#testing-for-significant-correlations",
    "title": "Correlations",
    "section": "Testing for significant correlations",
    "text": "Testing for significant correlations\nSo far, we have only computed correlation coefficients. However, most of the time, we will also want to know whether the correlations in our data are so strong that we can reject the Null hypothesis that there is no linear relationship between the variables.\nWe can test for statistical significance using the function cor.test(). This function takes two numeric vectors of the same length as function arguments x and y. It also automatically removes cases with NAs.\nJust as with the cor() function, we can specify the type of correlation coefficient we want to test for significance using the function argument method. As with the cor() function, the default is “pearson”, but we can change it to “kendall” or “spearman” if we want.\nSince direction matters when dealing with correlations, we can also specify the type of our alternative hypothesis using the function argument alternative. The default is “two.sided”, which tests whether the correlation is different from zero. We can change this argument to “greater” or “less” to test the directional hypotheses that \\(r\\) is positive or that it is negative, respectively.\nLet’s look at an example, in which we want to run a two-tailed significance test on a product-moment correlation. Here is what the code would look like:\n\n# create two numeric vectors\nv1 = 1:10     # just the numbers from 1 to 10\nv2 = exp(v1)  # e to the power of v1 (because why not)\n\n# test of significant correlation of v1 and v2\ncor.test(x = v1, y = v2)\n\nRunning this code will prompt R to return a lot if information in the console. Here is that the output looks like:\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  v1 and v2\nt = 2.9082, df = 8, p-value = 0.01964\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.159019 0.927748\nsample estimates:\n      cor \n0.7168704 \n\n\n\nFrom this output, we can tell:\n\nthe type of correlation we tested, namely the product-moment correlation (makes sense because this default,a nd we did not change it)\nwhat data the correlation test is based on (v1 and v2)\nthat our test was two-tailed because our alternative hypothesis was that \\(r\\) is non-zero (also a default we did not change)\nthe test statistics of the underlying \\(t\\)-test, that is, the empirical \\(t\\) value, its degrees of freedom, and the \\(p\\)-value\nthe correlation coefficient and a 95% confidence interval (we could change the confidence level to something else using the conf.level argument; 0.95 is the default)\n\nIn the example above, the \\(p\\)-value of the test justifies rejecting the Null hypothesis.\nLet’s look at another example, in which we run a one-tailed test of significance on Spearman’s \\(\\rho\\), testing whether the correlation is greater than zero. Code as follows:\n\n# create two numeric vectors\nv1 = c(11, 14, 15, 10, 9, 4, 7, 8, 17, 6)\nv2 = c(12, 15, 17, 18, 5, 6, 9, 14, 8, 11)\n\n# test of significant correlation of v1 and v2\ncor.test(x = v1, y = v2, method = \"spearman\",\n         alternative = \"greater\")\n\nThe output in the console looks slightly different, mainly because we tested a rank order correlation (see below).\n\n\n\n\n    Spearman's rank correlation rho\n\ndata:  v1 and v2\nS = 102, p-value = 0.1395\nalternative hypothesis: true rho is greater than 0\nsample estimates:\n      rho \n0.3818182 \n\n\n\nAs we can see, R again tells us which type of correlation we tested, and our alternative hypothesis looked like. Since we tested Spearman’s \\(\\rho\\), the underlying statistical test is not a \\(t\\)-test (remember that this is a non-parametric correlation). Accordingly, R will show us a different test statistic named \\(S\\) and the corresponding \\(p\\)-value. Finally, R will tell us the correlation coefficient, but there will be no 95% confidence interval (R cannot compute it without assuming an underlying parametric function).\nIn this second example, Spearman’s \\(\\rho\\) is not significantly greater than zero. Thus, we have to concede that the result is uninformative."
  },
  {
    "objectID": "inference/inference5.html",
    "href": "inference/inference5.html",
    "title": "The two-factorial ANOVA",
    "section": "",
    "text": "The two-factorial analysis of variance (ANOVA) in an extension of the one-factorial ANOVA. It allows us to test how the mean of a continuous variable differs as a function of two categorical variables \\(A\\) and \\(B\\). In a two-factorial ANOVA, we can not only test for the independent effects of the two variables on the outcome, but also a possible interaction of the two variables \\(A \\times B\\). Here, interaction means that the magnitude and/or direction of the effect of one of the variables depends on the level of the second variable.\nA two-factorial design requires that the two variables \\(A\\) and \\(B\\) are “crossed”. That means that each level of the first factor \\(A\\) must be combined with each level of the second second factor \\(B\\). If \\(A\\) has three levels and \\(B\\) has two levels, our design has a total of 6 cells, and we would refer to it as a \\(3 \\times 2\\)-design.\nIf we ensure, in addition, that all cells of our design have the same sample size, we call our design orthogonal. Orthogonality is not required for running a two-factorial ANOVA, but it comes with an advantage: equal sample sizes make the ANOVA robust against violating the assumption of equal variances."
  },
  {
    "objectID": "inference/inference5.html#general-logic-of-the-two-factorial-anova",
    "href": "inference/inference5.html#general-logic-of-the-two-factorial-anova",
    "title": "The two-factorial ANOVA",
    "section": "General logic of the two-factorial ANOVA",
    "text": "General logic of the two-factorial ANOVA\nThe model underlying the two-factorial ANOVA looks like this:\n\\[\\mu_{jk} = \\mu+\\alpha_j+\\beta_k+\\gamma_{jk}\\] Here, \\(\\mu_{jk}\\) is the true mean or a combination of the \\(j\\)th level of factor \\(A\\) and the \\(kth\\) level of factor \\(B\\), \\(\\mu\\) is the true grand mean (i.e., the population mean across all groups), \\(\\alpha_j\\) is the true effect of the \\(j\\)th level of factor \\(A\\), \\(\\beta_k\\) is the true effect of the \\(k\\)th level of factor \\(B\\), and \\(\\gamma_{jk}\\) is the true effect of combining the \\(j\\)th level of \\(A\\) with the \\(k\\)th level of \\(B\\) that goes beyond the independent contributions of the two factors (\\(\\alpha_j\\) and \\(\\beta_k\\)).\nJust as with the one-factorial ANOVA, its two-factorial cousin partitions the variance of the outcome variable \\(\\sigma_x^2\\) into signal and noise. The signal is the variance that stems from differences between groups, and the noise is the variance that is due to differences within the groups. Here, too, we can simplify the partition of variances by partitioning the sums of squares instead. As with the one-factorial ANOVA, we can state that:\n\\[SS_{total} = SS_{between} + SS_{within}\\]\nThe difference to the one-factorial ANVOA is that we can now partition the signal (the \\(SS_{between}\\)) even further. Specifically, we can partition the variability between groups into variability that is due to the first factor \\(A\\), variability that is due to the second factor \\(B\\), and variability that results from the interaction of the two factors \\(A \\times B\\). We can state this formally as follows:\n\\[SS_{between} = SS_A + SS_B + SS_{A \\times B}\\]\nLet’s have a closer look at the sums of squares using a simplest possible example, namely the \\(2 \\times 2\\)-design, in which factor \\(A\\) has \\(J = 2\\) levels, and factor \\(B\\) also has \\(K=2\\) levels. We can think of this design as a squares divided into four parts (see below):\n\n\n\n\n\n\n\n\n\nIn each of the four cells of this \\(2 \\times 2\\) design, there are \\(n_{jk}\\) many observations of the outcome variable \\(x\\), where \\(n_{jk}\\) is the sample size in the cell formed by the \\(j\\)th level of \\(A\\) and the \\(k\\)th level of \\(B\\). We denote individual observations as \\(x_{ijk}\\), which refers to the \\(i\\)th observation in the cell formed b the \\(j\\)th level of \\(A\\) and the \\(k\\)th level of \\(B\\).\nWhen we want to partition the sum of squares in a two-factorial design, we first need to compute the total sum of squares. We do so, by collapsing across all cells of our design and summing the squared deviations of all observations from the grand mean \\(\\bar{x}\\) (i.e., the mean computed across all observations irrespective of which cell they stem from). We can formalise this as:\n\\[SS_{total} = \\sum_{j=1}^{J} \\sum_{k=1}^{K} \\sum_{i=1}^{n_{jk}} (x_{ijk}-\\bar{x})^2\\] The next step is computing \\(SS_{between}\\) by pretending that there is no variance within groups. Computing the \\(SS_{between}\\) works similar to the one-factorial ANOVA. That means, we replace each observation \\(x_{ijk}\\) with the mean of its cell \\(\\bar{x}_{jk}\\) before computing the sum of the squared deviations from the grand mean \\(\\bar{x}_{jk}\\).\n\\[SS_{between} = \\sum_{j=1}^{J} \\sum_{k=1}^{K} n_{jk} \\times (\\bar{x}_{jk} - \\bar{x})^2\\] Computing the \\(SS_{within}\\) is also similar to the one-factorial ANOVA. We pretend that there is no variation between groups by replacing the grand mean \\(\\bar{x}\\) with the cell means \\(\\bar{x}_{jk}\\) in the formula for the total sum of squares.\n\\[SS_{within} = \\sum_{j=1}^{J} \\sum_{k=1}^{K} \\sum_{i = 1}^{n_{jk}}  (x_{ijk} - \\bar{x}_{jk})\\]\nSo far, so good. We have now partitioned \\(SS_{total}\\) into the \\(SS_{between}\\) and the \\(SS_{within}\\). However, since we have a \\(2 \\times 2\\)-design, we are not merely interested in whether there is substantial variation between the groups, but also where this variation originates. Thus, we need to partition the sums of squares further.\nWe first compute \\(SS_A\\), that is, the variation that is solely due to differences in factor \\(A\\). We do so by pretending that a) there is no factor \\(B\\), and b) there is no variation within cells. The latter is necessary because \\(SS_A\\) is part of \\(SS_{between}\\). The good thing is that we already know how to do it, namely by replacing individual observations with a mean score. But how do we pretend that there is no factor \\(B\\)? Effectively, we collapse across all levels of \\(B\\) and combine them into one big cell for each level of \\(A\\). Here is what our \\(2 \\times 2\\)-design would look like in our minds now:\n\n\n\n\n\n\n\n\n\nWe now denote the individual observations as \\(x_{ij\\cdot}\\). The little dot reminds us that there was originally another factor \\(B\\) across which we have now collapsed the cells of our design. To obtain \\(SS_A\\), we now need to replace all individual observations with their respective cell means respective cell means \\(\\bar{x}_{j\\cdot}\\) and then compute the sum of their squared deviations from the grand mean \\(\\bar{x}\\) .\n\\[SS_A = \\sum_{j=1}^{J} n_{j\\cdot}(\\bar{x}_{j\\cdot}-\\bar{x})^2   \\] Here, \\(n_{j\\cdot}\\) refers to the number of observations for the \\(j\\)th level of factor \\(A\\) (irrespective of the level of factor \\(B\\)).\nWe can now compute the \\(SS_B\\) in a similar fashion, namely by pretending that a) there is no factor \\(A\\), and b) there is no variation within cells. Our imagined design now looks like this:\n\n\n\n\n\n\n\n\n\nWe note denote individual observations as \\(x_{i\\cdot k}\\) to emphasize that we do not consider which level of \\(A\\) an observation stems from. Accordingly, the cell means are now denoted as \\(\\bar{x}_{\\cdot k}\\). The \\(SS_B\\) are formally defined as:\n\\[SS_B = \\sum_{k=1}^{K} n_{\\cdot k}(\\bar{x}_{\\cdot k}-\\bar{x})^2   \\]\nFinally, we need to compute the variability that is due to the interaction of \\(A\\) and \\(B\\). The simplest way to do so is to subtract the newly computed \\(SS_A\\) and \\(SS_B\\) from \\(SS_{between}\\):\n\\(SS_{A \\times B} = SS_{between} - SS_A - SS_B\\)\nIf we run a two-factorial ANOVA, we no longer need the \\(SS_{between}\\) because its parts, \\(SS_A\\), \\(SS_B\\), and \\(SS_{a \\times B}\\) contain all the variation between groups. The next step is to test each component of the between-group variation for statistical significance. The logic is similar to that of the one-factorial ANOVA, that is, we test for significant using an \\(F\\)-statistic.\nSince we have three possible sources of between-group variability, we will run three tests: one for the main effect of factor \\(A\\), one for the main effect of factor \\(B\\), and one for the interaction effect \\(a \\times B\\). For each of these tests, we need to compute the mean squares of the effect we are interested in and divide it by the \\(MS_within\\). We can obtain \\(MS_A\\), \\(MS_B\\), \\(MS_{A \\times B}\\), and \\(MS_within\\) by dividing the respective sum of squares by its degrees of freedom (see below).\n\\[MS_A = \\frac{SS_A}{J-1}\\] \\[MS_B = \\frac{SS_B}{K-1}\\]\n\\[MS_{A\\times B} = \\frac{SS_{A\\times B}}{(J-1)(K-1)}\\] \\[MS_{within} = \\frac{SS_{within}}{N-J\\times K}\\]\n\nThe main effect of factor A\nLet’s first look at the test for the main effect of \\(A\\). The \\(F\\)-statistic looks as follows:\n\\[\\frac{MS_A}{MS_{wihtin}} \\sim F_{J-1;N-J\\times K}\\] We use it to test the following hypotheses:\n\\(H_{0_A}: \\alpha_j = 0 \\quad \\forall j\\)\n\\(H_{1_A}: \\lnot H_{0_A}\\)\nRemember that \\(\\alpha_j\\) is the true effect of the \\(j\\)th level of \\(A\\), which means that:\n\\(\\mu_{j\\cdot} = \\mu+\\alpha_j\\)\nIn other words, when we add \\(\\alpha_j\\) to the overall population mean \\(\\mu\\) (the true value of \\(\\bar{x}\\)), we obtain the expectancy of the mean for the \\(j\\)th level of \\(A\\) (collapsed over all levels of \\(B\\)).\n\n\nThe main effect of factor B\nRegarding the main effect of \\(B\\), this is what the \\(F\\)-statistic look:\n\\[\\frac{MS_B}{MS_{wihtin}} \\sim F_{K-1;N-J\\times K}\\] The corresponding hypotheses are:\n\\(H_{0_B}: \\beta_k = 0 \\quad \\forall k\\)\n\\(H_{1_B}: \\lnot H_{0_B}\\)\nAgain, \\(\\beta_k\\) is the true effect of the \\(k\\)th level of B. We can state that:\n\\(\\mu_{\\cdot k} = \\mu + \\beta_k\\)\nThis means that adding \\(\\beta_k\\) to the true population mean \\(\\mu\\) will result in the expected mean of the \\(k\\)th level of factor \\(B\\) (collapsed across all levels of \\(A\\)).\n\n\nThe interaction of A and B\nThe \\(F\\)-statistic for the interaction effect \\(A \\times B\\) is defined as follows:\n\\[\\frac{MS_{A \\times B}}{MS_{wihtin}} \\sim F_{(J-1)(K-1);N-J\\times K}\\]\nWe use this \\(F\\)-statistic to test the final hypothesis, which is very similar to the main effect hypotheses:\n\\(H_{0_{A \\times B}}: \\gamma_{A \\times B} = 0 \\quad \\forall j,k\\)\n\\(H_{1_{A \\times B}}: \\lnot H_{0_{A\\times B}}\\)\nAs mentioned above, \\(\\gamma_{jk}\\) is the effect that combining the \\(j\\)th level of \\(A\\) and the \\(k\\)th level of \\(B\\) has on the population mean \\(\\mu\\) beyond the respective main effects \\(\\alpha_j\\) and \\(\\beta_k\\). We define \\(\\gamma_{A \\times B}\\) such that:\n\\[\\mu_{jk} = \\mu + \\alpha_j + \\beta_k + \\gamma_{jk} = 0\\]\nThis means that when we look at individual cells of our design instead of collapsing across rows or columns, we can obtain the respective true cell mean \\(\\mu_{jk}\\) by taking the true grand mean \\(\\mu\\) and adding not only \\(\\alpha_j\\) and \\(\\beta_k\\) but also \\(\\gamma_{jk}\\)."
  },
  {
    "objectID": "inference/inference5.html#running-a-two-factorial-anova-in-r",
    "href": "inference/inference5.html#running-a-two-factorial-anova-in-r",
    "title": "The two-factorial ANOVA",
    "section": "Running a two-factorial ANOVA in R",
    "text": "Running a two-factorial ANOVA in R\nWhen we want to run a two-factorial ANOVA in R, we can use the same package and function we used for the one-factorial case. That is, we use the aov_ez() function of the package *afex*. Before we can run the analysis, we first need data from a two-factorial design.\nLet’s say, we have data from an experimental \\(2 \\times 2\\)-design in which we manipulated two factors: job demands (low vs. high) and autonomy (low. vs. high). The dependent variable is the experienced level of job stress. Let’s further assume that we have gathered data from 50 participants in each of the four cells of our design and stored it in a data frame called my_df. Here is an except of the data.\n\n\n  ID demands control stress\n1  1     low     low    112\n2  2     low    high     79\n3  3    high     low    134\n4  4    high    high     76\n5  5     low     low    114\n6  6     low    high    129\n\n\nThe difference to a one-factorial ANOVA is that we need to feed the aov_ez() function a vector containing two variable names as its between argument (in the one-factorial case it was a single character value). Here is what the syntax looks like:\n\n# load library afex\nlibrary(afex)\n\n# run a two-factorial ANOVA with demands and control as the \n# independent variables and stress as the dependent variable\nmy_ANOVA = aov_ez(id = 'ID', between = c('demands', 'control'), \n       dv = 'stress', data = my_df)\n\n# display the results of the ANOVA\nmy_ANOVA\n\nRunning this code will create an ANOVA object called “my_ANOVA” in the environment and then display the results of the analysis in the console. Here is what the output looks like:\n\n\n\nContrasts set to contr.sum for the following variables: demands, control\n\n\nAnova Table (Type 3 tests)\n\nResponse: stress\n           Effect     df    MSE         F  ges p.value\n1         demands 1, 196 455.82 43.92 *** .183   &lt;.001\n2         control 1, 196 455.82 20.02 *** .093   &lt;.001\n3 demands:control 1, 196 455.82    4.15 * .021    .043\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\n\nAs we can see, the output is an ANOVA table. R tells us that the tests are based on type-3 sums of squares and tells us what the outcome variable was in our analysis (stress). The table itself contains three effects: a main effect of demands (the first variable we entered as a between-subjects factor), a main effect of control (the second between-subjects factor), and the interaction of the two variables denoted by “demands:control” (in R, the colon often indicates an interaction effect).\nFor each effect, R displays the numerator and denominator degrees of freedom, the \\(MS_within\\) (they are necessarily equal for all three effects), the \\(F\\)-value, an estimate of the effect size \\(\\eta^2\\), and the \\(p\\)-value. In our example, all three effects are statistically significant. Statistically speaking, we can now reject all of the Null hypotheses, that is, we would conclude that the level of demands has an effect on stress, that the level of control has an effect on stress, and that there is an interaction of the two variables."
  },
  {
    "objectID": "inference/inference5.html#disentangling-effects-in-a-two-factorial-anova",
    "href": "inference/inference5.html#disentangling-effects-in-a-two-factorial-anova",
    "title": "The two-factorial ANOVA",
    "section": "Disentangling effects in a two-factorial ANOVA",
    "text": "Disentangling effects in a two-factorial ANOVA\nSimilar to the one-factorial ANOVA, the tests we run in a two-factorial ANOVA are unspecific in the sense that they do not tell us which means differ from one another. However, disentangling effect in two-factorial ANOVAs is non-trivial. Which effects we need to disentangle and how we go about it depends on a) whether there is a significant interaction and b) how many levels our factors have.\n\nInteractions and the interpretation of main effects\nLet’s first consider what difference it makes whether there is a significant interaction. In order to do that, we first need to consider what a main effect is in a two-factor ANOVA. Here, we must distinguish between the statistical main effect as returned by the ANOVA and an actual main effect in the psychological sense. We speak of an actual main effect if the order of the levels of one factor holds across all levels of the other factor. For example, in a \\(2 \\times 2\\)-design, we would speak of a main effect of factor \\(A\\) if the first level of \\(A\\) was associated with higher (or lower) scores than its second level, irrespective of whether we look at the first or the second level of factor \\(B\\).\nHere is the problem: because the statistical main effect of a factor in an ANOVA is computed by collapsing across the levels of the other factor, there are different patterns that produce a significant mean difference but do not satisfy the condition described in the previous paragraph. When can those pattern occur? The answer is: when there is an interaction of the two factors.\nWhat does this mean for us? First for the easy case in which there is no interaction. If the interaction effect is not significant, we cannot accept \\(H_{1_{A\\times B}}\\). That means we retain \\(H_{0_{A\\times B}}\\) for now (without believing that it is true because the non-significant finding is uninformative). Retaining \\(H_{0_{A\\times B}}\\) implies that if there are any significant main effects in the ANOVA, we can interpret them as is and disentangle them similar to effects in a one-factorial ANOVA.\nNow let’s assume that we found a significant interaction effect. This is where things become difficult. Let’s look at a few graphs to understand this issue better. All of the situations below constitute cases where an ANOVA would show a significant interaction an two significant main effects However, not all of these situations permit us to interpret the data in terms of actual main effects.\n In this first case, we can see that there is an interaction because the lines to not run parallel. We can also see that there are main effects of factors \\(A\\) and \\(B\\). The main effect of factor \\(A\\) becomes evident from the marginal means \\(\\bar{x}_{j\\cdot}\\), which we can derive by averaging across the blue and orange points for each level of \\(A\\). As we can see from the right side of the graph, there is a considerable distance between the two means \\(\\bar{x}_{1\\cdot}\\) and \\(\\bar{x}_{2\\cdot}\\). This difference is the main effect of factor \\(A\\) in the \\(2\\times 2\\) ANOVA. We can also see from the left side of the graph that the two marginal means \\(\\bar{x}_{\\cdot 1}\\) and \\(\\bar{x}_{\\cdot 2}\\) differ (we can obtain them by taking the averages of the same-coloured points). This is the main effect of factor \\(B\\).\nImportantly, despite the interaction effect, the order of the factor levels does not change. We call this an ordinal interaction. Irrespective of the level of factor \\(B\\), we obtain higher values when the level of factor \\(A\\) is \\(j=2\\) instead of \\(j=1\\). Similarly, irrespective of the level of factor \\(A\\), values are higher when factor \\(B\\)’s level is \\(k=2\\) instead of \\(k=1\\). Therefore, we can interpret the statistical main effects as actual main effects and state that the level of our dependent variables is generally higher for one level of factor \\(A\\) than for the other. Similarly, we can state that the values are generally higher for on level of factor \\(B\\) than for the other.\nSo far, so good. Now lets’ look at the next case.\n\nIn this scenario, an ANOVA would also show an interaction effect (non-parallel lines) and two main effects (evidenced by differences between the marginal means). The difference to the previous scenario is the effect of factor \\(A\\) when we fix factor \\(B\\) at level \\(k=2\\). This effect is a flat line. Although the values differ slightly, the pattern for factor \\(B\\) remains similar with \\(k=2\\) yielding higher values of the dependent variable than \\(k=1\\) irrespective of the level of factor \\(A\\). We call this type of interaction semi-disordinal because order is only maintained for one factor and not inverted for the other. How does this change our interpretation of the main effect?\nWe would still state that the values are generally higher for on level of factor \\(B\\) than for the other. However, we cannot state the same for factor \\(A\\). As we can see, whether one level of \\(A\\) is associated with higher levels of the dependent variable in conditional on the level of factor \\(B\\). We see an effect fo factor \\(A\\) when we fix \\(B\\) at level \\(k=1\\), but if we fix it at \\(k=2\\) instead, there is no longer a difference between the two levels of factor \\(A\\). Therefore, we cannot interpret the data as showing an actual main effect of factor \\(A\\) even though this effect may show up in the ANOVA.\nLet’s now look at the third scenario.\n\nAs in the previous scenarios, an ANOVA would show a significant interaction (non-parallel lines) and two main effects (see the marginal means). This time, both factor’s main effects are conditional on the level of the other factor. Factor \\(A\\) only has an effect on the dependent variable if we fix factor \\(B\\) at level \\(k=1\\), and factor B only has an effect when factor \\(A\\)’s level is \\(j=2\\). In this case, we cannot interpret either of the two significant main effects in the ANOVA as actual main effect. Note that although order is maintained nor neither factor, we would still consider this a semi-disordinal interaction because there is no inversion of the factor’s order.\nNow for the grand finale:\n Again, an ANOVA would tell us that there is a significant interaction and two significant main effects. However, as we can see, the notion of main effects is maximally misleading here. If we look at factor \\(A\\), we will see that its effect can be positive (in the sense that it leads to higher levels of the dependent variable), or it can be negative. Whether the effect is positive or negative depends on the level of factor \\(B\\). The same is true for factor \\(B\\). Whether this factor has a positive or negative effect on the level of the dependent variable depends on the level of factor \\(A\\). We call this a disordinal interaction because the order of factor levels (in terms of which has the higher values of the dependent variable) reverses for each factor depending on the levels of the other factor. This means that - similar to the previous scenario - we cannot interpret the statistical main effects to be actual main effects.\n\nThe bottom line here is that we should only bother disentangling actual main effects and not any main effect that is statistically significant in a 2-factorial ANOVA.\nThis means that we should disentangle main effects only if there is no evidence of an interaction or if the interaction is ordinal.\n\n\n\nDisentangling main effects\nLet’s first consider the disentangling of main effects (assuming that there is no significant interaction or an ordinal interaction). When a factor has only two levels, the case is clear, because the difference must be between those two levels.\nIf we have three or more levels, we need to disentangle the main effect in the same way we disentangled them in a one-factorial case. That means we can run pairwise comparisons or custom contrasts on that factor’s marginal means (i.e., collapsing across all levels of the other factor) using the emmeans() function from the package *emmeans*.\nIn our example from far above, the factor “demands” has only two levels, but we can still use the data to show how the syntax and output would look like:\n\n# load library emmeans\nlibrary(emmeans)\n\n# pairwise comparisons on the demands factor with Bonferroni correction\nemmeans(object = my_ANOVA, specs = 'demands',\n        contr = 'pairwise', adjust = 'bonferroni')\n\nRunning the code above will yield the following console output:\n\n\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n$emmeans\n demands emmean   SE  df lower.CL upper.CL\n low       99.9 2.13 196     95.7      104\n high     119.9 2.13 196    115.7      124\n\nResults are averaged over the levels of: control \nConfidence level used: 0.95 \n\n$contrasts\n contrast   estimate   SE  df t.ratio p.value\n low - high      -20 3.02 196  -6.627  &lt;.0001\n\nResults are averaged over the levels of: control \n\n\n\nSimilar to the one-factorial case, the output will consist of two tables, with the second of them containing the desired information. In this case, we have only one entry here because the factor “demands” has only two levels (however, the logic of the post-hoc comparisons would not differ from the one-factorial case had we more than two levels).\nThere are two few things worth noting: First, when we feed the the emmeans() function a single variable as its specs argument, the output will contain a message stating that we collapsed across the other factor (in our case “control”). Thus, we know that we are talking about the marginal means of the “demands”. Second, when we call the emmeans() function on a main effect in an ANOVA design with at least two factors, R will also return a message informing us that the results of our pairwise comparisons may be misleading due to involvement in potential interactions (we already know this by now).\nSince we have two factors in a \\(2 \\times 2\\)-design, we need to consider that we may have two significant interpretable main effects. Generally speaking, we need to disentangle each significant main effect via its marginal means, granted that the interaction pattern (absent or ordinal interaction) permits it.\n\n\nDisentangling interactions\nIf a \\(2 \\times 2\\)-ANOVA reveals a significant interaction, we need to disentangle it. The reason is that - as we have seen from the different graphs above - there are various patterns that can produce an interaction effect, and we need to identify which one lead to the significant interaction. If the interaction is not significant, there is - of course - no need to disentangle the effect.\nBefore we try to answer the question how to disentangle an interaction effect, it helps to understand better what we actually test when testing an interaction effect. The best way to do that is to have a look at the contrasts underlying the tests of main effects and interaction in a \\(2 \\times 2\\) ANOVA. Note that we need to turn our two-factorial design into a one-factorial design for the purpose of defining the contrasts. The contrasts for the three tests look as follows:\n\n\n\n\n\n\n\n\n\n\n\n\\(j=1\\); \\(k=1\\)\n\\(j=2\\), \\(k=1\\)\n\\(j=1\\); \\(k=2\\)\n\\(j=2\\); \\(k=2\\)\n\n\n\n\nmain effect of \\(A\\)\n1\n-1\n1\n-1\n\n\nmain effect of \\(B\\)\n1\n1\n-1\n-1\n\n\ninteraction effect\n1\n-1\n-1\n1\n\n\n\nLet’s digest this. For the main effect of \\(A\\), we can see that we compare the marginal means of its two levels because we average across the two levels of factor \\(B\\). Likewise, we can see that are comparing the marginal means of \\(B\\) because we average across both levels of \\(A\\). Finally, and most importantly, we can see what the interaction contrast does. It tests whether the difference between the two levels of \\(A\\) when \\(B\\) is fixed at \\(k=1\\) differs from the the same difference when \\(B\\) is fixed at \\(k=2\\). This is equivalent to testing whether the difference between the two levels of \\(B\\) when \\(A\\) is fixed at \\(j=1\\) differ from the difference between the two levels of \\(B\\) when \\(A\\) is fixed at \\(j=2\\).\n\nWe can now see the analogy between the two-factorial ANOVA and the one-factorial ANOVA.\nDisentangling a main effect in a two-factorial ANOVA is like disentangling an effect in a one-factorial ANOVA, the only difference being that we use the respective factor’s marginal means instead of cell means.\nDisentangling the interaction is also similar to disentangling an effect in a one-factorial ANOVA, the only difference being that we use the mean differences between the two factors instead of cell means.\n\nWe can easily verify that using the contrasts specified above leads to the same results as running the ANOVA, provided that we do not adjust the p-values for multiple comparisons. Let’s quickly recap the ANOVA results from our simulated experiment on job demands and control. Here is the ANOVA output.\n\n\n\nAnova Table (Type 3 tests)\n\nResponse: stress\n           Effect     df    MSE         F  ges p.value\n1         demands 1, 196 455.82 43.92 *** .183   &lt;.001\n2         control 1, 196 455.82 20.02 *** .093   &lt;.001\n3 demands:control 1, 196 455.82    4.15 * .021    .043\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\n\nNow let’s run a custom contrast analysis on the ANOVA object using emmeans() and compare it to the ANOVA results. Here is the syntax:\n\n# analyse contrasts for main effects and interactions in a 2x2 \n# ANOVA without correcting for type-I error inflation\nemmeans(object = my_ANOVA, specs = c('demands', 'control'),\n        contr = list(\n          demands = c(1,-1,1,-1),\n          control = c(1,1,-1,-1),\n          interaction = c(1,-1,-1,1)),\n        adjust = 'none')\n\nHere is the output:\n\n\n\n$emmeans\n demands control emmean   SE  df lower.CL upper.CL\n low     low      103.6 3.02 196     97.6      110\n high    low      129.7 3.02 196    123.8      136\n low     high      96.2 3.02 196     90.3      102\n high    high     110.1 3.02 196    104.1      116\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate   SE  df t.ratio p.value\n demands        -40.0 6.04 196  -6.627  &lt;.0001\n control         27.0 6.04 196   4.475  &lt;.0001\n interaction    -12.3 6.04 196  -2.037  0.0430\n\n\n\nWhen we compare the ANOVA results to the results of the custom contrast analysis, we can see that there is only one difference. The ANOVA uses \\(F\\)-statistics whereas the contrasts are analysed using \\(t\\)-statistics. The \\(F\\)-values reported in the ANOVA are the exact squares of the corresponding \\(t\\)-values, and while the \\(p\\)-values of the two main effects are too small to compare them visually between the two analyses, we can see that the \\(p\\)-value of the interaction effect is the same.\nWhat do we do with this knowledge? First of all, we know now what we can state once an interaction in a two-factorial ANOVA is significant: there is a difference of differences. Put differently, we know now that the magnitude of the effects of \\(A\\) and \\(B\\) differ depending on which level of the other factor we look at. This knowledge in mind, we can now start to disentangle the interaction by looking at the simple effects.\nA simple effect is the main effect of one factor when fixing the other factor at a specific level. In a \\(2 \\times 2\\)-ANOVA, there are four simple effects:\n\nthe effect of \\(A\\) when \\(B\\) is fixed at \\(k=1\\)\nthe effect of \\(A\\) when \\(B\\) is fixed at \\(k=2\\)\nthe effect of \\(B\\) when \\(A\\) is fixed at \\(j=1\\)\nthe effect of \\(B\\) when \\(A\\) is fixed at \\(j=2\\)\n\nIn order to understand how a significant interaction effect came about, we need to inspect all four of these simple effects. It may come as no surprise that we will do so using custom contrasts. Here is what the contrast weights would look like:\n\n\n\n\n\n\n\n\n\n\n\n\\(j=1\\); \\(k=1\\)\n\\(j=2\\), \\(k=1\\)\n\\(j=1\\); \\(k=2\\)\n\\(j=2\\); \\(k=2\\)\n\n\n\n\nsimple effect of \\(A\\) for \\(k=1\\)\n1\n-1\n0\n0\n\n\nsimple effect of \\(A\\) for \\(k=2\\)\n0\n0\n1\n-1\n\n\nsimple effect of \\(B\\) for \\(j=1\\)\n1\n0\n-1\n0\n\n\nsimple effect of \\(B\\) for \\(j=2\\)\n0\n1\n0\n-1\n\n\n\nAs we an see, simple effects are a subset of the pairwise comparisons. Now let’s have a look at the syntax for the simple effect analysis:\n\n# analysis of the simple effects using custom contrasts\nemmeans(object = my_ANOVA, specs = c('demands', 'control'),\n        contr = list(\n          demands_for_low_control = c(1, -1, 0, 0),\n          demands_for_high_control = c(0, 0, 1, -1),\n          control_for_low_demands = c(1, 0, -1, 0),\n          control_for_high_demands = c(0, 1, 0, -1)\n        ))\n\nHere is what the output looks like:\n\n\n\n$emmeans\n demands control emmean   SE  df lower.CL upper.CL\n low     low      103.6 3.02 196     97.6      110\n high    low      129.7 3.02 196    123.8      136\n low     high      96.2 3.02 196     90.3      102\n high    high     110.1 3.02 196    104.1      116\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                 estimate   SE  df t.ratio p.value\n demands_for_low_control    -26.16 4.27 196  -6.127  &lt;.0001\n demands_for_high_control   -13.86 4.27 196  -3.246  0.0014\n control_for_low_demands      7.36 4.27 196   1.724  0.0863\n control_for_high_demands    19.66 4.27 196   4.604  &lt;.0001\n\n\n\nAs we can see, all but one of the simple effects are significant. Since the output also shows an estimate of the contrast value for each of the simple effect contrasts, we can now interpret the interaction pattern.\nHere is a valid interpretation: there is an interaction because the effect of job demands on stress is greater when control is low than when it is high.\nAnother valid interpretation: there is an interaction because control has an effect on stress when job demands are high, but this effect is weaker when demands are low to the extant that we cannot say, based on the data, whether it still exists.\nAs we can see, we can interpret the interaction from the viewpoint of both factors. Which interpretation we need to choose depends on the specific nature of our interaction hypothesis.\n\n\nDisentangling interactions in two-factorial ANOVAs with more than two levels\nAs soon as our design has at least one factor with three or more levels, we need one more step to make sense of the interaction. Let’s consider - for the sake of simplicity - the case of a \\(2 \\times 3\\)-ANOVA that yields a significant interaction effect. Now, an analysis of the simple effects is not straightforward because one factor has three levels. While we can easily inspect the simple effect of the two-level factor for all three levels of the other factor, we do not know which pairwise comparison to focus on for the three-level factor.\nTo narrow things down, we need to remember what exactly an interaction test does: to test for differences between differences. In a \\(2 \\times 3\\)-design, this boils down to computing the difference between the two levels of first factor and testing whether this difference (the effect of factor \\(A\\)) differs as a function of the second factor (the one with three levels). Since we now have three levels of factor \\(B\\), there are three possible pairwise comparisons for the effect of \\(A\\). The effect of \\(A\\) could differ between the first and second level of \\(B\\), between its first and third level, or between the second and third level. Of course, it is also possible that two or even all three of these pairwise comparisons point toward differences.\nWhy is this pairwise comparisons of differences relevant? It allows us to decompose the overall interaction effect into smaller parts that we already know how to handle, namely \\(2 \\times 2\\)-interactions. Once we know, which of the three possible \\(2 \\times 2\\)-interactions in our \\(2 \\times 3\\)-design is significant, we can then proceed by disentangling them in the way described above, namely by inspecting the simple effects."
  },
  {
    "objectID": "inference/inference7.html",
    "href": "inference/inference7.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "In linear regression, we “predict” one variable \\(Y\\) (the criterion) from one or more other variables \\(X\\) (the predictors), given that data on both \\(Y\\) and \\(X\\) is available in the same sample (think of regression as a within-subjects approach to inferential statistics). If we say predict in the context of regression, we mean this in a strictly statistical sense, meaning that the regression analysis itself does not allow any statements about the potential causal patterns in the data.\nPrediction simply means that we use information from the \\(X\\) variables to make the best possible guess about the corresponding values of \\(Y\\). The best possible guess, in this case, is the one that minimizes the prediction error across the whole range of observations. That is, we may still be quite far off for individual values of \\(Y\\) when using this best guess, but in the long run (or across the board) it will be our best option.\nLinear regression is conceptually close to correlation analysis as it assumes a strictly linear relationship between \\(X\\) and \\(Y\\). In fact, in cases with only one predictor variable, correlation and linear regression convey the exact same information, albeit being presented in a slightly different manner."
  },
  {
    "objectID": "inference/inference7.html#the-basics-of-linear-regression",
    "href": "inference/inference7.html#the-basics-of-linear-regression",
    "title": "Simple Linear Regression",
    "section": "The basics of linear regression",
    "text": "The basics of linear regression\nThe core idea of linear regression is that we can express the criterion variable \\(Y\\) as a linear function of the predictor variables \\(X\\). Let us first consider the simple case with one predictor. The general equation is:\n\\[Y = b_0 + b_1\\times X + \\epsilon\\]\nHere, \\(b_0\\) is some constant denoting the intercept of the linear function, and \\(b_1\\) is its slope. We call \\(b_1\\) a regression weight. It indicates how well we can predict \\(Y\\) from \\(X\\) (think of \\(b_1\\) as a measure of the covariation of \\(X\\) and \\(Y\\)). Finally, \\(\\epsilon\\) is the prediction error, also called the residual. We assume that:\n\\[\\epsilon \\sim N(0, \\sigma^2)\\]\nThat is, the residual is assumed to have a mean of zero and some nonzero variance. We consider \\(\\epsilon\\) to be unsystematic, which means that it is not part of our actual prediction.\n\nTreating \\(\\epsilon\\) as unsystematic is an oversimplification in most regression models. The reason is that the residual term contains not only the true unsystematic measurement error but also systematic variability of \\(Y\\) that is not captured in our \\(X\\) variables. In other words, \\(\\epsilon\\) contains both unsystematic and unexplained variability of \\(Y\\).\n\nLet’s call the prediction of \\(Y\\) from our predictor variable \\(X\\) by a different name, namely \\(\\hat{Y}\\). Since \\(\\epsilon\\) is not part of this prediction, we can state:\n\\[\\hat{Y} = b_0 + b_1\\times X\\] This also means that:\n\\[\\epsilon = Y-\\hat{Y} \\]\nThe trick to linear regression is to choose the parameters of the regression model, \\(b_0\\) and \\(b_1\\), so that they minimise the residual \\(\\epsilon\\). We do that using the method of least squares, which is why linear regression is sometimes referred to as ordinary least squares (OLS) regression. To understand the method of least squares, it help to move from the variable-level formulation of the regression model to the level of observations.\n\\[y_i = b_0 + b_1 \\times x_i + \\epsilon_i\\]\nWhat this means is that our prediction of the magnitude of \\(Y\\) for the \\(i\\)th observation is based on the corresponding value of \\(X\\), that is, \\(x_i\\) and our prediction error for that specific observation \\(\\epsilon_i\\). The method of least squares states that the best combination of parameters \\(b_0\\) and \\(b_1\\) is the one that minimises the sum of the squared residuals \\(\\epsilon_i\\).\nIt can be shown that the residual is minimised for:\n\\[b_1 = \\frac{COV(X,Y)}{\\sigma_x^2} = r_{XY} \\times \\frac{\\sigma_Y}{\\sigma_X}\\] And:\n\\[b_0 = \\bar{Y} - b_1 \\times \\bar{X}\\]\nAs we can see from the choice of the optimal \\(b_1\\), it is a linear transformation of the covariance of \\(X\\) and \\(Y\\), which means that we can also express it as a linear transformation of their correlation. Since linear transformations only change the units of measurement while leaving the strength of the linear relationship untouched, we can see that the linear regression with a single predictor variable \\(X\\) contains the exact same information as the correlation, but it is expressed differently.\nSpecifically, the expression we use in linear regression allows us to predict specific values \\(\\hat{y}_i\\) whereas the correlation tells us how many standard deviations \\(Y\\) increases if \\(X\\) is increased by one standard deviation. With that in mind, we can now state that we would expect \\(\\hat{Y}\\) to take the value \\(b_0\\) if \\(X\\) were zero. We can also say that \\(\\hat{Y}\\) increases by \\(b_1\\) units if we increase \\(X\\) by one unit.\n\nTesting regression coefficients for significance\nWe can test the fixed parameters of a regression model (i.e., the intercept and the regression weights of the predictor variables \\(X\\)) for statistical significance using \\(t\\)-statistics. For each of the parameters the following is true\n\\[\\frac{b_i - \\beta_i}{SE_{b_i}} \\sim t_{N-k}\\]\nHere, \\(b_i\\) is the parameter of interest, \\(\\beta_i\\) is the expected value under \\(H_0\\) (typically zero, but we can can technically test against non-zero values if we wanted to), and \\(SE_{b_i}\\) is the standard error of that parameter. The degrees of freedom of the resulting \\(t\\)-distribution equal the sample size \\(N\\) minus the number of the estimated parameters \\(k\\).\nWe are not going to go into the formula for the standard error of the regression coefficients here because they are a) somewhat complicated and b) R computes them for us. Instead, we will turn to one more important statistic used in regression models, namely the coefficient of determination \\(R^2\\):\n\\[R^2 = \\frac{S_{\\hat{Y}}^2}{S_{Y}^2}\\]\nHere, \\(S_{\\hat{Y}}^2\\) is our estimate of the variance of the model predictions \\(\\hat{Y}\\), whereas \\(S_{Y}^2\\) is our estimate of the variance of the criterion variable \\(Y\\). In the case with only one predictor, the coefficient of determination is the squared correlation coefficient between predictor \\(X\\) and criterion \\(Y\\). In the case with multiple predictor variables we, therefore, also refer to \\(R\\) the multiple correlation coefficient. In any case, \\(R^2\\) tells us which proportion of the variance of the criterion \\(Y\\) we can account for with the predictions of our regression model \\(\\hat{Y}\\).\nWe already know from the chapters on ANOVA that such ratios of variances can also be expressed as ratios of sums of squares. In the case of linear regression, it looks like this:\n\\[R^2 = \\frac{S_{\\hat{Y}}^2}{S_{Y}^2} = \\frac{\\frac{1}{N-k}\\sum_{i=1}^{N}(\\hat{y}_i - \\bar{\\hat{y}}_i)^2}{\\frac{1}{N-k}\\sum_{i=1}^{N}(y_i - \\bar{y}_i)^2}\\]\nWe can simplify this equation by dropping the \\(\\frac{1}{N-k}\\) which results in:\n\\[R^2 = \\frac{\\sum_{i=1}^{N}(\\hat{y}_i - \\bar{\\hat{y}}_i)^2}{\\sum_{i=1}^{N}(y_i - \\bar{y}_i)^2} = \\frac{SS_{regression}}{SS_{total}}\\]\nNow that we know that we are dealing with sums of squares again, we can easily compute the residual sum of squares (or error sum of squares):\n\\[SS_{residual} = SS_{total} - SS_{regression}\\]\n:::{.alert .alert-success} If at this point you are wondering why we are back to sums of squares even though we are not dealing with ANOVAs anymore: regression and ANOVA are the same. :::\nOr, to be precise: they are both special cases of the general linear model. We use ANOVAs when our predictors are categorical variables, but we could easily set up the same model as a regression model. The output of the respective analysis and the specific parameter tests we run may differ, but the underlying information is the same. :::\nBack to \\(R^2\\): we can test for significance of \\(R^2\\) using an \\(F\\)-test based on the mean squares not unlike the ones we used in ANOVAs. It can be shown that:\n\\[\\frac{MS_{regression}}{MS_{error}} =  \\frac{\\frac{SS_{regression}}{k-1}}{\\frac{SS_{error}}{N-k}} \\sim F_{k-1; N-k}\\]\nThis \\(F\\)-test tells us whether our regression model as a whole - that is, irrespective of the statistical significance of its individual regression weights - allows for an above-chance prediction of the criterion variable.\nAdding more predictor variables to a regression model will necessarily increase the proportion of variance explained by the model \\(R^2\\) even if the new parameter is not statistically significant individually. To counter this problem, we can adjust \\(R2\\) and essentially “punish” a model for containing too many weak predictors. Here is the formula for the adjusted \\(R^2\\):\n\\[R_{adjusted}^2 = 1-\\frac{\\frac{SS_{error}}{df_{error}}}{\\frac{SS_{total}}{df_{total}}} = 1- \\frac{MS_{error}}{MS_{total}}\\]\n\n\nRunning a regression analysis in R\nWe can run regression analyses in R using the lm() function (linear model). The function has several arguments. We will use only the following two:\n\nformula (required): a formula type object telling the function which variable to predict and which combination of variables to use as predictors\ndata (optional): a data frame containing the variables we feed into the formula; if we do not specify this argument, R will assume that the variables we fed into the formula argument exist as objects in our environment\n\n\n\nExcurse: formula type objects\nBefore we delve into examples of linear regressions, we need to have a look at the syntax of formula type R objects. We already know that these objects use the ~ operator (tilde) and have the general form:\n\n# general syntax of a formula\ny ~ x\n\nThat means, we define \\(y\\) as a function of \\(x\\). Of course, a formula can be much more complex than that because we can use combinations of multiple variables on the right hand side. In regression models, the formula is generally an additive combination of predictors, each of which then receives its own regression weight. We can combine multiple predictors using the + operator.\nThe first thing we need to know now is that a regression formula in R typically omits the intercept although the lm() function models it. This is purely a convenience feature. The actual formula looks like this:\n\n# formula explicating that the model contains an intercept\ny ~ 1 + x\n\nHere, the \\(1\\) represents the intercept. If we omit it, R will read the function as if we had written it, meaning that the function will estimate the intercept \\(b_0\\) both when we add the \\(1\\) in our formula and when we leave it out.\n\nIn some situations, we might want to force a regression through the origin. In other words, we want the intercept of the model to be zero. In those cases, we can replace the \\(1\\) in our regression model with a \\(0\\), so R will know that the intercept must be 0 in the model.\n\n\n\nBack to regressions in R\nNow that we have a rough understanding of formula type objects in R, we can go back to running linear regressions in R. Let’s first create some data for the simple case with one predictor variable. Let’s assume we gathered data from 40 people on two variables, love of cats and love of dogs. Both variables are measured on scales ranging from -3 (can’t stand them) to +3 (love them). We will use a linear regression to predict participant’s attitude toward cats from their attitude toward dogs.\n\nThis data example is very prototypical for psychology in the sense that it violates two of the assumptions of linear regression, namely that the criterion \\(y\\) is continuous and measured on an interval scale (meaning that the increase from, say, 1 to 2 is equal to the increase from, say, 4 to 5).\nRating scales, as they are often used in psychological research, do not satisfy these conditions. Neither is the scale continuous (in fact,it is restricted to a few discrete values), nor can we say for certain whether the steps between scale points mark equal psychological distances.\nIn practice, we generally dismiss this violating of assumptions and pretend that it is not an issue. For those who find that unsatisfying, it may be worthwhile to use a different type of regression model that is better suited to this kind of data, namely ordinal regression.\n\nLet’s first have a brief look at the fictional data (stored in a data frame called df1):\n\n\n  ID love_dogs love_cats\n1  1        -2         1\n2  2         1        -2\n3  3         2         0\n4  4        -3        -3\n5  5         1        -1\n6  6         1        -1\n\n\nWe can now call the lm() function to run an ordinary least squares regression of participants’ love for cats from their love for dogs. Similar to ANOVAs, we will define the regression model as an object. Here is what the syntax might look like:\n\n# regression of participants' love for cats on their love for dogs\nmodel1 = lm(formula = love_cats ~ 1 + love_dogs,\n            data = df1)\n\nOnce we run that code, an object called “model1” will appear in our environment. R will inform us that this object is a list of 12. We can now call the object’s name and have a look at what R outputs in the console:\n\n\n\n\nCall:\nlm(formula = love_cats ~ 1 + love_dogs, data = df1)\n\nCoefficients:\n(Intercept)    love_dogs  \n    -0.4223       0.1245  \n\n\n\nAs we can see, the output is relatively sparse. We can see the function call we entered as well as estimates of the two regression parameters \\(b_0\\) (intercept) and \\(b_1\\) (the regression weight for love of dogs).\nWhile we now know the parameter values of the linear regression, this information is slightly underwhelming. If we want more information, particularly about the statistical significance of the parameters, we need to feed the regression model into another function called summary(). The summary() function takes a fitted model as its sole argument. Here is the syntax:\n\n# display detailed output for the regression model\nsummary(model1)\n\nHere is what the console output looks like when we call the summary() function on our regression model:\n\n\n\n\nCall:\nlm(formula = love_cats ~ 1 + love_dogs, data = df1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4532 -1.3287 -0.4532  1.4223  3.5468 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -0.4223     0.3007  -1.404    0.168\nlove_dogs     0.1245     0.1729   0.720    0.476\n\nResidual standard error: 1.674 on 38 degrees of freedom\nMultiple R-squared:  0.01346,   Adjusted R-squared:  -0.0125 \nF-statistic: 0.5185 on 1 and 38 DF,  p-value: 0.4759\n\n\n\nAs we can see, R now displays substantially more information. In addition to the function call, we can now also see information on the distribution of the residuals (quartiles). For each of the fixed parameters of the model, we can now see the estimate, its standard error, the resulting \\(t\\)-value, and the \\(p\\)-value of a two-tailed \\(t\\)-test of the parameter against zero. Below the parameter estimates, there is some more information on the model, the most important of which are the \\(R^2\\) and adjusted \\(R^2\\) values as well as the results of the \\(F\\)-test testing whether \\(R^2\\) is greater than zero.\nIn our example, love of dogs is not a significant predictor of participant’s love for cats. The model as a whole also does not explain a significant proportion of the variance in participants’ love for cats indicated by the non-significant \\(F\\)-test. This is not surprising because the model only contains one predictor, love for dogs, and this predictor is not significantly related to the criterion. As we can see, the \\(p\\)-value for the \\(t\\)-test of the single predictor (love for dogs) is exactly equal to the \\(F\\)-test of the whole model since love for dogs is the only variable that can possibly explain variability in the love for cats in our data set."
  },
  {
    "objectID": "inference/inference7.html#categorical-predcitors-in-simple-linear-regression",
    "href": "inference/inference7.html#categorical-predcitors-in-simple-linear-regression",
    "title": "Simple Linear Regression",
    "section": "Categorical predcitors in simple linear regression",
    "text": "Categorical predcitors in simple linear regression\nSo far, we have considered the case in which both \\(X\\) and \\(Y\\) are (sort of) continuous or variables. While \\(Y\\) must be a continuous variable in linear regression, the same is not true for the predictor \\(X\\). We can also predict \\(Y\\) from categorical variables with two or more levels. If we do so, we need to slightly change the way in which we interpret the results of the model output.\n\nDichotomous predictors\nIn the most simple case, a categorical predictor has two levels (e.g., treatment vs. control). If we want to use a dichotomous predictor in a linear regression, we need to assign values to its two levels. Which values we choose is technically irrelevant, but some are more sensible when it comes to interpreting the model. The two most common ways to code dichotomous predictors are:\n\neffect coding: the predictors levels are centred around 0 (e.g., -1 vs. 1 or -0.5 vs. 0.5)\ndummy coding: the predictor is coded as a binary variable (0 vs. 1)\n\nWhen running a regression using a categorical predictor in R, we need to consider the type of the predictor variable. If we use a character string or a factor as a binary predictor, R will automatically use dummy coding. If the predictor variable is numeric and different from the desired coding scheme, we need to recode it manually.\n\nCaveat: Unless we define a predictor as a factor with an ordered structure (by defining levels and labels in the desired order), R will order the levels alphabetically, with the first level being treated as the reference category.\n\nLet’s now briefly look at an example of a simple linear regression with a dichotomous predictor. Here is some made up data (stored in df2), in which the predictor is either dummy-coded (because the variable is a factor with two levels) or effect coded (-0.5 vs. 0.5):\n\n\n  ID cond_dummy cond_effect score\n1  1    control        -0.5  5.59\n2  2  treatment         0.5 12.20\n3  3    control        -0.5  8.55\n4  4  treatment         0.5  9.05\n5  5    control        -0.5 10.17\n6  6  treatment         0.5  8.58\n\n\nWe will first look at the dummy-coded version of the predictor by using the labelled factor as the predictor (the variable named ‘cond_dummy’). Here is what the syntax looks like:\n\n# regression of score on the condition variable\nmodel2 = lm(formula = score ~ cond_dummy, data = df2)\n\nRunning the code above yields the following output:\n\n\n\n\nCall:\nlm(formula = score ~ cond_dummy, data = df2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0977 -1.1347 -0.4077  0.7375  5.4223 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           7.4077     0.3401  21.780  &lt; 2e-16 ***\ncond_dummytreatment   1.4893     0.4810   3.096  0.00302 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.863 on 58 degrees of freedom\nMultiple R-squared:  0.1419,    Adjusted R-squared:  0.1271 \nF-statistic: 9.588 on 1 and 58 DF,  p-value: 0.003016\n\n\n\nWhen we use dummy-coding, the intercept of the model represent the mean of the reference category (in our example the control group). This means, we can test whether the mean of the reference group differs from zero using the intercept of the model. The treatment effect tells us how much the mean value changes when we move from the control group to the treatment group, that is, it shows us the mean difference between the two groups and tells us whether this mean difference is significant.\n\nTesting the significance of mean differences between two groups…doesn’t that sound familiar? That is what we previously used the independent samples \\(t\\)-test for.\nIn fact, if we ran an independent samples \\(t\\)-test on the data and forced it to assume equal variances, it would show the exact same \\(t\\)-value, degrees of freedom, and \\(p\\)-value as the test of the slope in our regression model.\nWe would get slightly different results when using the default Welch \\(t\\)-test because it adjusts the degrees of freedom downward to correct for unequal variances, thus yielding a somewhat larger \\(p\\)-value.\n\nLet’s now see what happens to the output if we use the effect-coded version of the condition variable as a predictor instead of the dummy-coded version.\n\n\n\n\nCall:\nlm(formula = score ~ cond_effect, data = df2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0977 -1.1347 -0.4077  0.7375  5.4223 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   8.1523     0.2405  33.898  &lt; 2e-16 ***\ncond_effect   1.4893     0.4810   3.096  0.00302 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.863 on 58 degrees of freedom\nMultiple R-squared:  0.1419,    Adjusted R-squared:  0.1271 \nF-statistic: 9.588 on 1 and 58 DF,  p-value: 0.003016\n\n\n\nWe should notice a few things: first, the regression weight \\(b_1\\) and its associated test statistics (\\(t\\)-value, \\(p\\)-value) are identical. The same goes for the coefficient of determination \\(R^2\\) and it associated \\(F\\)-test. The only thing that differs is the intercept of the model and its associated test statistics. This makes sense, intuitively, because we simply shifted the predictor to the “left” by half a unit (from 0 vs. 1 to -0.5 vs. 05). Other than in the dummy-coded model, the intercept no longer reflects the mean of the reference group but the overall mean. The underlying information, namely the correlation between \\(X\\) and \\(Y\\), is unchanged, however. Accordingly, the strength of the relationship between \\(X\\) and \\(Y\\) as well as its statistical significance cannot differ between the two versions of the regression model.\n\n\nPredictors with more then two levels\nWhile dichotomous predictors are easy to handle (particularly when we dummy-code them), things can get complicated when categorical predictors have three or more levels. What R does in these cases is create \\(j-1\\) contrasts, where \\(j\\) is the number of categories of the predictor \\(X\\). These contrasts are coded in a specific way. Similar to dummy-coded predictors, the first level s treated as the reference category, and each other level is compared to that reference category.\nLet’s look at an example with a four-level predictor. Here, we predict the daily average sun hours in Northern Ireland from the four seasons (data are made up, of course).\n\n\n  obs season sun_hours\n1   1 spring      0.90\n2   2 summer      2.76\n3   3 autumn      1.03\n4   4 winter      0.02\n5   5 spring      1.64\n6   6 summer      2.50\n\n\nHere is the syntax for the regression model:\n\n# regresison model predicting sun hours from seasons\nmodel3 = lm(formula = sun_hours ~ season, data = df3)\n\nLet’s have a look at the console output:\n\n\n\n\nCall:\nlm(formula = sun_hours ~ season, data = df3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9780 -0.1840 -0.0420  0.0975  1.2710 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    1.3080     0.1207  10.839 6.90e-13 ***\nseasonsummer   1.6310     0.1707   9.557 2.05e-11 ***\nseasonautumn  -0.3860     0.1707  -2.262   0.0298 *  \nseasonwinter  -1.6620     0.1707  -9.739 1.25e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3816 on 36 degrees of freedom\nMultiple R-squared:  0.9134,    Adjusted R-squared:  0.9062 \nF-statistic: 126.5 on 3 and 36 DF,  p-value: &lt; 2.2e-16\n\n\n\nIn the data, seasons are coded such that spring is the first level, followed by summer, autumn, and winter. Therefore, the regression model treats spring as the reference category. The intercept tells us that spring in Northern Ireland has an average of 1.3 sun hours per day, which is significantly different from having zero sun. The regression weight for summer indicates that the sun shines for an additional 1.6 hours in summer compared with spring. Likewise, the regression weights for autumn and winter tell us that the daily sun hours are lower in autumn than in spring and also lower in winter than spring. Adding one of the regression weights to the intercept yields the mean hours of sunshine for that season (in the case of winter, we the model predicts a daily average of minus 0.3 sun hours in Northern Ireland, which seems about right).\nImportantly, the model does not tell us whether average daily sun hours differ between summer and autumn, summer and winter, or autumn and winter. If we are interested in those differences, we need to run the model using a different reference category (for example by changing the factor levels and labels of the predictor variable accordingly).\n\nYou may have noticed that despite having only one predictor variable, the model now contains four parameters, the intercept and three regression weights. This is also reflected in the degrees of freedom (we lose one for each parameter).\nTechnically, this means that we have now entered the realm of multiple linear regression."
  },
  {
    "objectID": "inference/inference9.html",
    "href": "inference/inference9.html",
    "title": "Multi-level models",
    "section": "",
    "text": "In the real world (and in some psychological experiments), data is embedded in a hierarchical structure. If we want to apply a regression approach to hierarchical data, we need to consider this structure in our models.\nHierarchical data structures consist of at last two levels at which the observation of variables is possible. Per convention, we define the most fine-grained level of observation as level 1. This is also the level at which we measure the criterion. Since Level 1 is the most fine-grained one it also has the highest number of observations. At level 2, we can cluster the level 1 data into several distinct groups with group membership defined by level 2 variables. We can then cluster level 2 data into distinct groups at level 3, and so on.\nHere are some examples:\nOne common aspect of those examples is that the criterion is always measured at level 1. For example, a school does not have a grade of its own. Instead, the grades are assigned to students. Likewise, a team in an organisation does not experience stress, the people working in it do. That is not to say that we cannot compute an average school grade or team stress level, but the data we need to compute such scores are always gathered at level 1.\nAnother common aspect of hierarchical data structure is that we can measure predictor variables at each level. Let’s consider the example ‘experiences stress’ from above:\nWe could reasonably postulate hypotheses linking any (or all) of the variables above to the criterion (experienced stress). However, the critical question is how to approach a test of these hypotheses in an appropriate way. The answer is: by using multi-level models.\nMulti-level models (also known as hierarchical linear models, mixed effects models, or nested-data models) are regression models that take the hierarchical structure of the data into account by partitioning the variance of the criterion accordingly. As such, they are not unlike repeated measures and mixed ANOVAs where the variance of the observed variable is partitioned into variance between and variance within participants. In fact, these ANOVAs are special types of multi-level models, which means that multi-level models are a more general and, thus, more flexible method to deal with hierarchical data."
  },
  {
    "objectID": "inference/inference9.html#why-considering-the-data-structrue-is-important",
    "href": "inference/inference9.html#why-considering-the-data-structrue-is-important",
    "title": "Multi-level models",
    "section": "Why considering the data structrue is important",
    "text": "Why considering the data structrue is important\nThe first question we might ask ourselves is why we should bother considering the data structure in the first place. Why not just aggregate the data and run an ordinary linear regression model using the aggregate scores as the criterion? The answer is twofold.\nFirst, depending on our hypotheses aggregation may not be an option. For example, if we wanted to predict school grades from pupils’ cognitive styles (level 1 predictor), the gender of their teachers (a level 2 predictor) and the type of school (level 3 predictor), aggregating the data at the school level won’t work unless we have very good reason to believe that an aggregation of the predictors works without losing explanatory power.\nSecond, and more importantly, we may fall pray to the ecological fallacy. This fallacy means that we draw incorrect conclusion about lower-level predictor-criterion relations when analysing the data at an aggregate level. To illustrate this fallacy, lets look at an example.\n\n\n\n\n\n\n\n\n\nIn the example above, we are predicting a criterion from a predictor variable. As we can see, the correlation between the two variables is positive. Now let’s look at the same data again. This time, we will consider that there are five discount groups in our data.\n\n\n\n\n\n\n\n\n\nWe now get a completely different picture. Within each of the five groups, there is a strong negative correlation of the predictor and the criterion. In our case, neglecting the data structure implies an effect in the opposite direction of the effects we observe within each group. This extreme form of the ecological fallacy is also known as Simpson’s Paradox.\nNow, does the fact that we obtain different results when considering instead of neglecting the data structure mean that one of the analyses is true while the other is wrong? Not really. When analysing data across groups (i.e., by neglecting group membership), the correlation we observe is a statistical truth, that is, the relation between predictor and criterion exists. However, when we try to make sense of why it occurs, we need to consider the data structure. Why do we get a positive correlation when, in fact, the relation is negative within each group? The answer is: because the groups differ systematically. In other words, the positive correlation between predictor and criterion is a result of group differences (a level 2 effect), whereas at level 1 the relation is negative.\nWhat the example above should make clear is that considering the hierarchical nature of our data provides us with a clearer picture of how our predictors relate to the criterion of interest. This, in turn, will allow us to draw more appropriate inferences about our data. The question now is: how?"
  },
  {
    "objectID": "inference/inference9.html#the-basics-of-multi-level-modelling",
    "href": "inference/inference9.html#the-basics-of-multi-level-modelling",
    "title": "Multi-level models",
    "section": "The basics of multi-level modelling",
    "text": "The basics of multi-level modelling\nThe core idea of multi-level modelling is, put very simply, to predict level 1 criteria from predictors at various levels. To this end, the variance of the predictor is partitioned into variance at level 1, variance at level 2, and so on. In order to understand how that works, we first need to understand the difference between fixed effects and random effects.\nFixed effects are our model’s predictors. We estimate the strength of their relationship with the criterion as regression weights. As we already know there regression weights indicate how the criterion changes when statistically controlling for the other predictors.\nRandom effects are unsystematic (or unexplained) variances. They represent the variability of a parameter assuming a normal distribution. Random effects can be used to model the variability of the criterion but also the variability of fixed effects.\nWe already know a model containing fixed and random effects, namely the simple linear regression. Let’s recap it basic formula:\n\\[Y = b_0 + b_1X_1 + \\epsilon\\] With:\n\\[\\epsilon \\sim N(0, \\sigma^2)\\]\nHere, the parameters \\(b_0\\) and \\(_b1\\) are fixed effects. They are the basis for our model prediction \\(\\hat{Y}\\). In contrast, the residual \\(\\epsilon\\) is a random effect. While we compute the difference between our model prediction \\(\\hat{Y}\\) and the actual criterion \\(Y\\) for each observation, we summarise the residual \\(\\epsilon\\) via a single parameter, namely its variance.\n\nFrom simple linear regressionn to multi-level modelling\nLet’s start with the most simple regression model, which we will call a single-level intercept only model.\n\\[Y_i = b_0 + \\epsilon_i\\].\nIn this model, we have only two effects: one fixed effect, namely the intercept \\(b_0\\), and one random effect, the residual \\(\\epsilon\\). According,y \\(Y_i\\) represent the level of \\(Y\\) for the \\(i\\)th observation, and \\(\\epsilon_i\\) is the specific deviation of the \\(i\\)th observation from the model prediction (i.e., the intercept). The model assumes that all the variance of \\(Y\\) is unsystematic and that the unsystematic differences \\(\\epsilon_i\\) originate on level 1 (the only level in this model).\nLet’s now assume that we know that there is a variable on a higher level (level 2) that allows grouping our level 1 data. There are two ways to handle such a situation. We could either treat the grouping variable as a categorical fixed effect, modelling its relationship with the criterion via a regression weight in a single-level model. Alternatively, we could treat it as a random effect in a two-level model. In our example, we would include a random effect for the intercept of the model (our sole fixed effect).\n\nThe decision whether to model the effects of a grouping variable as a fixed or a random effect depends on what we are interested in. If we want to examine the influence of a certain grouping variable, we usually model it as a fixed effect.\nAs a rule of thumb, we should model something as a fixed effect if we would sue the same grouping (same variable, same levels) in a replication of our study. This includes, experimental manipulations, studying gender effects, etc.\nIf,in contrast, the grouping variable itself is a random sample (e.g., a sample of schools or a sample of teams in an organisation), we should model it as a random effect. In such cases, we are no interested in how switching from one category to another affects the criterion. Likewise, we would not insist on studying the exact same groups in a replication, but would instead draw another random sample (e.g. a different set of schools or different teams).\n\nLet’s now look at the most simple two-level model, which assumes that the intercept of the intercept-only model varies between groups formed by a level 2 grouping variable. Note that we will use a slightly different syntax. In this syntax, we will put the levels at which variables or parameters vary in parentheses. This makes it a little easier to parse the indices of the regression parameters as we will see below. Here is the model equation:\n\\[Y_{(ij)} = b_{0(j)} + \\epsilon_{(ij)}\\] Where:\n\\[b_{0(j)} = c_{0(0)} + u_{0(j)}\\] Which means that:\n\\[ Y_{(ij)} = c_{0(0)} + u_{0(j)} + \\epsilon_{(ij)} \\] With:\n\\[u_{0(j)} \\sim N(0, \\sigma_{u_{0(j)}}^2)\\] and \\[\\epsilon_{(ij)} \\sim N(0, \\sigma_{\\epsilon_{(ij)}}^2)\\]\nThe notation using parentheses in the indices allows us to differentiate two pieces of information for the model parameters: a) the parameter name, that is, which effect they correspond to (e.g., \\(b_0\\) to represent the intercept and \\(b_1\\) the effect of the first predictor) and b) whether that parameter is a constant (indicated by the value 0 in parentheses) or whether it varies randomly, indicated by the letter corresponding to the level at which the parameter varies (e.g., \\(j\\) if the parameter varies randomly at level 2). Accordingly, the index of the parameter \\(b_{0(j)}\\) reveals that it is the model intercept (the parameter \\(b_0\\)), and that it varies between the level-2 units (indicated by the \\(j\\) in parentheses).\nIn the model \\(Y_{ij}\\) is the criterion value of the \\(i\\)th observation in the \\(j\\)th group. The model intercept \\(b_{0(j)}\\) is split into two components, the mean intercept across all \\(j\\) groups \\(c_{0(0)}\\) (the grand mean, which no longer varies between groups, hence the zero in parentheses), and a random component, the random intercept \\(u_{0(j)}\\). This random component \\(u_{0(j)}\\) indicates how the intercept of the \\(j\\)th level 2 unit deviates from the fixed intercept \\(c_{0(0)}\\). However, since we model \\(u_{0(j)}\\) as a random effect, we only estimate its variance.\nLet’s now turn to the final parameter, the residual \\(\\epsilon_{(ij)}\\). In this multi-level model, the intercept works a bit differently than in a single-level model. It indicates how the \\(i\\) observations in the \\(j\\)th group differ from that group’s intercept, which we get by adding \\(c_{0(0)}\\) (the fixed intercept) and \\(u_{0(j)}\\) (the \\(j\\)th group’s deviation from the fixed intercept). What this means is that the residual \\(\\epsilon_{(ij)}\\) no longer represents the deviations between the model prediction and the actual criterion values, but instead denotes the (unexplained) variance within each group.\nLet’s look at an example and visualise it to get a better grasp of the differences between the two models. Let’s assume we have data from 9 participants (level 1) nested in three groups (level 2). We have one observation per participant. If we modelled the data with a single-level intercept only model, this is how it would look.\n\n\n\n\n\n\n\n\n\nWe can see, that the residuals \\(\\epsilon_i\\) tell us how each participant’s score deviates from the intercept \\(b_0\\). now let’s look at the multi-level version of the model.\n\n\n\n\n\n\n\n\n\nAs we can see, we can now construe not only the fixed intercept \\(c_{0(0)}\\) but also individual intercept for each of the three groups, which we denote as \\(c_{0(j)}\\). The residual \\(\\epsilon_{(ij)}\\) now tells us how a participant’s score deviates from the intercept of that participant’s group.\nThe crucial difference between the two models is that single-level model only tells us that there is unexplained variance in the criterion, whereas the multi-level model also tells us at which level this unexplained variance originates.\nA multi-level model containing only a fixed and a random intercept (as the one in our example) is also known as the variance component model. By comparing the two random effects, we can infer which proportion of the variance of \\(Y\\) originates between versus within the level 2 groups. We can do so by computing the intra-class correlation (ICC).\n\\[ICC = \\frac{\\sigma_{u_{0(j)}}^2} {\\sigma_{u_{0(j)}}^2 + \\sigma_{\\epsilon_{ij}}^2}\\] The larger the variance between groups, indicated by the variance of the random intercept \\(\\sigma_{u_{0(j)}}^2\\), the larger the ICC. Thus, a larger ICC means that values within groups are more similar, which means they are more highly correlated.\n\n\nMulti-level models with random intercepts (and fixed slopes)\nThe variance component model is usually not very interesting because it contains no predictors. In a multi-level model, we can add predictors at each level. We will, for now, only focus on fixed slopes of level-1 variables. As a general rule, a fixed effect in a multi-level model will reduce the unexplained variance at its level. That is, a level 1 fixed effect will explain some of the residual variance, whereas a fixed effect at level 2 will explain some of the variance captured in the random intercept (in a sense we make some of the variability of the intercepts systematic by adding level 2 fixed effects).\nLet’s look at the model syntax for a two-level model containing a fixed slope for a level-1 predictor.\n\\[Y_{(ij)} = b_{0(j)} + b_{1(0)}X_{(ij)} + \\epsilon_{(ij)}\\] With:\n\\[b_{0(j)} = c_{0(0)} + u_{0(j)}\\] Yielding:\n\\[Y_{(ij)} = c_{0(0)} + u_{0(j)} + b_{1(0)}X_{1(ij)} + \\epsilon_{(ij)}\\]\nLet’s try to make sense of the notation. Again, \\(Y_{(ij)}\\) is the criterion value of the \\(i\\)th observation in the \\(j\\)th group. The index of the model intercept \\(b_{0(j)}\\) tells us a) that it is the intercept (parameter \\(b_0\\))and b) that it varies between the \\(j\\) groups (\\(j\\) in parentheses). We can partition this effect into its fixed component \\(c_{0(0)}\\), its index telling us that it no longer varies between groups (thus the 0 in parentheses), and a random component \\(u_{0(j)}\\). The index of this random component tells us that is belongs to the intercept and varies between the \\(j\\) groups.\nThe name of the next parameter \\(b_{1(0)}\\) reveals that this is regression weight of the first predictor (\\(b_1\\)) and that it does not vary between groups (indicated by the zero in parentheses). We can further see that \\(b_{1(0)}\\) is is a level-1 fixed effect because there is a value of the accompanying predictor \\(X_{1(ij)}\\) for each of the \\(i\\) observations within each of the \\(j\\) groups (the \\(ij\\) in parentheses tells us that \\(X_1\\) varies at both levels).\nBelow is a visualisation of what a random intercept model with a single level-1 predictor could look like.\n\n\n\n\n\n\n\n\n\nIn the figure above, the bold black line represents the fixed part of the model. The intercept of this regression line is \\(c_{0(0)}\\), and its slope is \\(b_{1_(0)}\\). We can also see that the model has random intercepts for each team. This means that the regression line is shifted upward or downwards depending on which team we look at. Since the slope for the predictor “job demands” does not vary at level 2, it is equal for all teams as indicated by the regression lines running parallel. What this means is that while members of different teams seem to report different overall levels of stress, the increase in stress as job demands increase is equal for members of all teams.\n\nA fixed will usually explain some of the unexplained variance at its own level. For example, in a two-level model, including a level-1 predictor as a fixed effect will reduce the residual \\(\\epsilon_{(ij)}\\). The variance of the model intercepts might also change slightly when the average magnitude of the level-1 predictor or its relationship with the criterion differ between level-2 units (and be it due to chance).\nIf that reminds you of mixed ANOVAs where we separated effects into within-subject and between-subject effects, that is no coincidence. A mixed ANOVA can be considered a special case of multi-level model.\n\n\n\nFixed effects at higher levels\nIn the previous example, the model included a fixed effect of a level-1 predictor. Of course, we could also include a predictor at level 2 (or at higher levels if the model contained them). Let’s assume we want to run a random intercept model with one fixed effect at level 1 and another at level 2. Here is what the model would look like:\n\\[Y_{(ij)} = b_{0(j)} + b_{1(0)}X_{1(ij)}+b_{2(0)}X_{2(j)}+\\epsilon_{(ij)}\\] Which we can rewrite as to disentangle the fixed and random component of the intercept:\n\\[Y_{(ij)} = c_{0(0)} + u_{0(j)} + b_{1(0)}X_{1(ij)}+b_{2(0)}X_{2(j)}+\\epsilon_{(ij)}\\] The model is the same as the one from the previous section with the exception of the newly added predictor \\(X_{2(j)}\\) and its associated fixed slope \\(b_{2(0)}\\). The notation tells us that \\(X_2\\) varies only between the \\(j\\) groups and is constant for all \\(i\\) levels within each group. Furthermore, we can see that the slope for \\(X_2\\) does not vary randomly (indicated by the zero in parentheses in the parameter’s index). This makes sense because we do not have a higher level at which it could vary.\nThis model is interesting for one specific reason. It models differences between groups as part random (via the random intercepts) and part systematic (via the fixed effect of \\(X_2\\)). The parameter for the random intercepts we estimate (\\(\\sigma^2_{u_0(j)}\\)), thus, contains all the differences between the \\(j\\) groups that the fixed effect \\(b_{2(0)}\\) does not account for.\n\nIf we compare the random-intercept model containing fixed effects at both levels to a model excluding the level-2 fixed effect, we will observe that their residuals remain similar. However, the random variance of the intercept will be lower in the model including the fixed level-2 effect. Again, this shows that a fixed effect will explain some of the unexplained variance at its own level. In other words, a predictor that varies only between groups (and is constant within groups) cannot explain the unexplained variability within groups.\n\n\n\nMulti-level models with random intercepts and slopes\nWhen we run multi-level models, we are not restricted to modelling random intercepts. We can also treat the slopes of our predictors as random effects, given that the respective predictor is not measured at the highest level. Let’s consider a model with a single level-1 predictor that contains random intercepts as well as random slopes for that predictor. Here is what the model looks like:\n\\[Y_{(ij)} = b_{0(j)} + b_{1(j)}X_{1(ij)} + \\epsilon_{(ij)}\\] Here:\n\\[b_{0(j)} = c_{0(0)} + u_{0(j)}\\] And:\n\\[b_{1(j)} = c_{1(0)} + u_{1(j)}\\] As always, we assume that the random effects follow a normal distribution with mean zero. We can rewrite the model as follows:\n\\[Y_{(ij)} = c_{0(0)} + u_{0(j)} + c_{1(0)}X_{1(ij)} + u_{1(j)}X_{1(ij)} + \\epsilon_{(ij)}\\] We already know what \\(Y_{(ij)}\\), \\(c_{0(0)}\\), \\(u_{0(j)}\\), and \\(\\epsilon_{(ij)}\\) denote. That leaves the two parameters related to the level-1 predictor \\(X_1\\). Here, \\(c_{1(0)}\\) is the fixed effect of \\(X_1\\), which we can interpret as the average slope. The corresponding random effect, \\(u_{1(j)}\\) indicates how the slopes differ from the fixed slope for \\(j\\) groups. As with all random effects, we compute the deviations of the group slopes for the fixed slope, but we estimate \\(u_{1(j)}\\) as a single parameter via the variance of the group slopes. Below is a visualisation of a random slope model.\n\n\n\n\n\n\n\n\n\nFrom the figure above, we can see that while job demands are positively associated with stress for people in all teams, the strength of this association is subject to some variation, meaning that the slope is steeper for members of some teams than others.\n\nThe figure above should remind you of the interaction effects of continuous and categorical predictors in multiple linear regression. Those interaction effects essentially resulted in different regression lines for each category.\nEven though multiple-regression is based on a purely between-subjects design, this analogy holds. If you will, we can think of random slopes as unspecific interaction effects between the respective predictor and an modeled variable at a higher level (of course, part of the variability in slopes can also be due to chance alone).\n\n\n\nCross-level interactions\nThe final type of effect we need to talk about in the context of multi-level models is a cross-level interaction. Cross-level interactions are interactions of predictors located at difference levels. For example, we could model the interaction of a level-1 predictor and the level-2 grouping variable in a two-level model. Let’s look at the model formulation for a two-level model containing a cross-level interaction:\n\\[Y_{(ij)} = b_{0(j)} + b_{1(j)}X_{1(ij)} + b_{2(0)}X_{2(j)} + b_{3(0)}X_{1(ij)}X_{2(j)}+\\epsilon_{(ij)}\\] We can rewrite this model as follows:\n\\[Y_{(ij)} = c_{0(0)} + u_{0(j)} + c_{1(0)}X_{1(ij)} + u_{1(j)}X_{1(ij)} + b_{2(0)}X_{2(j)} + b_{3(0)}X_{1(ij)}X_{2(j)}+\\epsilon_{(ij)}\\] As we can see, the model contains random intercepts but also random slopes for the level-1 predictor \\(X_{1(ij)}\\). Besides the fixed effects of the two predictors, we also have a fixed effect for their interaction, namely \\(b_{3(0)}\\). This interaction term is similar to interactions in regular regression models in that it is simply the product of the two variables. Similar to within-between interactions in a mixed ANOVA, the cross-level interaction indicates that the strength of the relationship between the level-1 predictor \\(X_{1(ij)}\\) and the criterion \\(Y_{(ij)}\\) depends on the magnitude (or category) of the level-2 predictor \\(X_{2(j)}\\).\n\nIncluding a cross-level interaction in a multi-level model with random slopes for the lower-level predictor involved in the interaction allows us to explain some of the unspecific interaction effect modelled via the random slopes. Just as adding main effects allows us to explain some of the unexplained variance at their level, adding cross-level interactions allow us to explain some of the random variation of the slopes.\n\n\nTheoretically, it is possible to model random slopes for cross-level interactions. The only requirement is that the higher-levelled predictor is not located at the model’s highest level. For example, we can model random slopes for a cross-level interaction of a level-1 predictor and a level-2 predictor is the model has at least three levels. Those random slopes would then represent an unspecific three-way interaction with the possible moderating variable residing on the third level."
  },
  {
    "objectID": "inference/inference9.html#running-multi-level-models-in-r",
    "href": "inference/inference9.html#running-multi-level-models-in-r",
    "title": "Multi-level models",
    "section": "Running multi-level models in R",
    "text": "Running multi-level models in R\nR offers several packages that allow computing multi-level models. We will use the lme4 package as it is perhaps the most commonly used multi-level package. This package has one drawback: it does not compute \\(p\\)-values (the reason is that the authors of the package noticed that there it is non-trivial to compute the correct degrees of freedom for the \\(t\\)-distributions used to test for statistical significance that holds when certain conditions such as balanced design are not met). Therefore, we will use an additional package called lmerTest that computes the degrees of freedom and, consequently, the \\(p\\)-values using a Satterthwaite-correction. Since lmerTest has lme4 as a dependency, installing lmerTest will also and install lme4, and loading lmerTest via the **library()* * function will also load lme4. However, because lme4 is the real workhorse, we will frequently refer to it in the following sections.\nThe way will will use the lmer() function for now requires us to define two arguments (similar to the regular regression function lm()):\n\nformula (required): a formula type object telling the function which variable to predict and what parameters to include in the model.\ndata (optional): a data frame containing the data; in theory, we could omit defining the argument if all of our variables existed as separate objects in our environment, but using data frames is much preferable.\n\n\nFormulae for multi-level models\nThe first thing we need to understand is how the syntax for multi-level model formulae works in R. For fixed effects, the formula is similar to that of regular regression models, so we need to focus on the random effects. Generally speaking, random effects are written in parentheses. Within the parentheses, we use a pipe operator (|) to separate the parameters we want to model random effects for (left-hand side of the pipe) and the grouping variable(s) by which the effects should vary (right-hand side of the pipe). Let’s look at a few basic examples:\n\n## Examples of multi-level formulae using lme4-syntax in a two-level model\n## with \"group\" as the level-2 grouping variable and one level-1 predictor X\n\n# variance component model (intercept-only)\ny ~ 1 + (1|group) \n\n# random intercept model with one predictor X1\ny ~ 1 + X + (1|group) \n\n# random slope model with correlated random intercepts and slopes\ny ~ 1 + X + (1 + X|group)\n\n# random slope model with uncorrelated random intercepts and slopes\ny ~ 1 + X + (1 + X||group)\n\nNow that we have a basic understanding of how the formulae work in the lme4 package, we can turn to the function we will use: lmer(). Since we will use lme4 in conjunction with lmerTest, it is important to know that lmerTest has a function called lmer() that overrides the original lmer() function of lme4. This is intentional and will allow us to write code as if we were using lme4 while also providing us with \\(p\\)-values in the model output.\nLet’s now write our first multi-level model in R. As always, we first need some hierarchical data. We will make up some data from a fictional experiment, in which several participants worked on a number of trials each. The data includes performance as a dependent variable, a continuous independent variable at level 1 indicating the stimulus intensity, and a level-two independent variable indicating task difficulty. Let’s have a look at an excerpt of the data (stored in df1):\n\n\n  ID difficulty trial intensity performance\n1  1  difficult     1         8       30.15\n2  1  difficult     2         9       35.55\n3  1  difficult     3        15       34.26\n4  1  difficult     4        10       29.65\n5  1  difficult     5        10       29.80\n6  1  difficult     6        15       34.00\n\n\nFrom that little excerpt, we can see that difficulty is a level-2 predictor as it is constant for a participant. In contrast intensity varies on the trial level. We can now start building different models. The first model, we will look at is the variance component model. Here is the syntax:\n\n# load the required libraries (lmerTest automatically loads lme4)\nlibrary(lmerTest)\n\n# define the variance component model (intercept only)\nmod1 = lmer(performance ~ 1 + (1|ID), data = df1)\n\nRunning this line of code will create a new object called mod1 in the environment (the object type is labelled as “Formal class lmerModLmerTest, but we do not need to concern ourselves with that). Similar to regular regression models, simply calling the object’s name will not yield a very informative output. instead, we need to feed the model into the summary() function as its sole argument.\n\n# display the the multi-level model\nsummary(mod1)\n\nRunning the code above will produce a detailed output of the model (see below):\n\n\n\n\nAttache Paket: 'lmerTest'\n\n\nDas folgende Objekt ist maskiert 'package:lme4':\n\n    lmer\n\n\nDas folgende Objekt ist maskiert 'package:stats':\n\n    step\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ 1 + (1 | ID)\n   Data: df1\n\nREML criterion at convergence: 5384.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1994 -0.6355 -0.0100  0.6315  3.5175 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 60.29    7.764   \n Residual             20.06    4.479   \nNumber of obs: 900, groups:  ID, 30\n\nFixed effects:\n            Estimate Std. Error     df t value Pr(&gt;|t|)    \n(Intercept)   38.791      1.425 29.000   27.21   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe top of the output contains some general information such as that the \\(t\\)-tests of the fixed effect parameters are computed using Satterthwaite’s method (this is what we use lmerTest for), and that the model parameters were estimates using REML (restricted maximum likelihood). We can usually ignore this information.\nTwo parts of the output are noteworthy. The first is the section labelled “Random effects”. here, R shows us two random effects. The first is a random intercept that varies between participants, indicated by the grouping variable for that random effect being “ID”. The second is the residual. Since these are random effects, we estimate them as a variance or standard deviation (R neatly computes both for us). In the example above, we can see that there is somewhat more variability within participants (i.e., at level 1) than between participants (at level 2).\nThe second part of interest is the “Fixed effects” table. Here, we can see the fixed regression parameters along with their standard error and \\(t\\)-value. Since we are using lmerTest, the output also contains the degrees of freedom and a \\(p\\)-value. In this model, we have only one fixed effect, namely the fixed intercept. As we can see, it is significantly different from zero.\nLet’s now gradually add effects to the model. We will start by adding a fixed slope for the level-1 predictor, then add a fixed slope for the level-2 predictor, then add random slopes for the level-1 predictor and finally include the cross-level interaction. Here is the syntax:\n\n# fixed slope model with random intercepts\nmod2 = lmer(performance ~ 1 + intensity + (1|ID), data = df1)\n\n# random intercept model with fixed slopes for both predictors\nmod3 = lmer(performance ~ 1 + intensity + difficulty + (1|ID), data = df1)\n\n# model with random slopes for the level-1 predictor\nmod4 = lmer(performance ~ 1 + intensity + difficulty + (1 + intensity|ID), data = df1)\n\n# random slopes model with a cross-level interaction\nmod5 = lmer(performance ~ 1 + intensity * difficulty + (1 + intensity|ID), data = df1)\n\nWe can now display each model by feeding it into the summary() function. Let’s have a look at each model’s output and compare it to the preceding model. Here is the output for model 2:\n\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ 1 + intensity + (1 | ID)\n   Data: df1\n\nREML criterion at convergence: 4814.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1988 -0.6494  0.0148  0.6452  3.3583 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 58.41    7.642   \n Residual             10.39    3.223   \nNumber of obs: 900, groups:  ID, 30\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  28.34588    1.44662  33.10797   19.59   &lt;2e-16 ***\nintensity     1.03825    0.03643 869.31076   28.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nintensity -0.253\n\n\n\nLet’s first look at the fixed effects. As we can see, there is now a fixed slope for the level-1 predictor “intensity” in addition to the fixed intercept. We can further see that the fixed slope is significant, meaning that higher stimulus intensity leads to better performance (we can only make this causal inference because our data stems from a fictional experiment and stimulus intensity is a manipulated variable). The fractional degrees of freedom are due to the Satterthwaite-correction.\nLet’s now look at the random effects. If we compare them to the previous model, we will notice that the residual variance has decreased by about 50%, and the variance of the intercepts is also slightly lower. The first makes sense because we have now included a level-1 predictor that should explain some of the level-1 variability. The unexplained level-1 variance captured by the residual is, thus, lower. The fact that the variance of the intercepts also decreased somewhat tells us that the average stimulus intensity is not equal for all participants. To the extent that stimulus intensity is related to the criterion (which it is, in our case), such differences in the average level will reduce some of the unexplained variance of the intercepts.\nLet’s now look at the model including the fixed effect for the level-2 predictor.\n\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ 1 + intensity + difficulty + (1 | ID)\n   Data: df1\n\nREML criterion at convergence: 4782.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2095 -0.6512  0.0152  0.6462  3.3429 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 21.62    4.650   \n Residual             10.39    3.223   \nNumber of obs: 900, groups:  ID, 30\n\nFixed effects:\n                Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)     22.31523    1.26414  33.32054  17.652  &lt; 2e-16 ***\nintensity        1.03892    0.03643 869.82329  28.521  &lt; 2e-16 ***\ndifficultyeasy  12.04787    1.71149  27.99767   7.039 1.17e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) intnst\nintensity   -0.289       \ndifficltysy -0.677 -0.001\n\n\n\nLet’s again focus on the fixed effects first. The new predictor “difficulty” is a factor with two levels, which means that the lmer() function dummy-codes it (similar to the lm() function). From the model output, we can infer that its level “difficult” is being treated as the reference level, and the fixed effect tells us how performance increases if we switch to difficulty “easy”. The effect is statistically significant.\nNow let’s look at the random effects. Since “difficulty” is a level-2 predictor and, thus, constant at level 1 is cannot explain any level-1 variance. We can confirm this by comparing the residual variance of this model to the previous one that excluded the level-2 predictor. Save for some rounding effects, the residual is the same. In contrast, due to the level-2 predictor being significantly related to the criterion, we now know more about why the intercepts for our participants differed. A substantial part of this variance is due to the fact that some participants worked on an easy version of the task while for others the task was difficult. This means that we have now explained some of the previously unexplained variance at level 2. Accordingly, the random effect indicating the variance of the intercepts is much smaller in the new model when compared with its predecessor.\nWe will now look at the model including random slopes.\n\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ 1 + intensity + difficulty + (1 + intensity | ID)\n   Data: df1\n\nREML criterion at convergence: 4723.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.8487 -0.6607  0.0206  0.6497  3.3145 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ID       (Intercept) 17.7825  4.2169        \n          intensity    0.1425  0.3774   -0.28\n Residual              9.2619  3.0433        \nNumber of obs: 900, groups:  ID, 30\n\nFixed effects:\n               Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)    23.73070    1.15541 28.68547  20.539  &lt; 2e-16 ***\nintensity       1.03622    0.07727 27.20176  13.410 1.66e-13 ***\ndifficultyeasy  9.19175    1.55680 27.98708   5.904 2.37e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) intnst\nintensity   -0.303       \ndifficltysy -0.675  0.002\n\n\n\nAs soon as we include random slopes, things become a little messier with the random effects. But let’s first look at the fixed effects. We did not add any new fixed effects, but we can see that the parameter estimates for the ones we already had in the model are slightly different. The most noteworthy change is that the standard error of the fixed slope of “intensity” is larger, and its t-value and degrees of freedom are lower, too. Since the effect is rather strong, it does not change the fact stimulus intensity is a significant predictor of performance.\nWhen we look at the random effects, we should notice three things. First, there is now another random effect indicating the variance (or standard deviation) of the slopes of intensity. Second, we now have a new parameter indicating the correlation of the random intercepts and the random slopes of intensity. This correlation tells us that participants with lower than average intercepts tend to have slightly above average slopes. Finally, adding the random slopes slightly reduced both the residual variance and the variance of the intercepts.\nLet’s now look at the final model including the cross-level interaction.\n\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ 1 + intensity * difficulty + (1 + intensity | ID)\n   Data: df1\n\nREML criterion at convergence: 4712.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9023 -0.6682  0.0338  0.6418  3.3226 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ID       (Intercept) 16.37094 4.0461        \n          intensity    0.07787 0.2791   -0.12\n Residual              9.26467 3.0438        \nNumber of obs: 900, groups:  ID, 30\n\nFixed effects:\n                         Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)               24.8832     1.1674 27.4303  21.315  &lt; 2e-16 ***\nintensity                  0.7802     0.0876 26.0962   8.907 2.16e-09 ***\ndifficultyeasy             6.9351     1.6485 27.2680   4.207 0.000251 ***\nintensity:difficultyeasy   0.5078     0.1235 25.7387   4.112 0.000354 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) intnst dffclt\nintensity   -0.335              \ndifficltysy -0.708  0.237       \nintnsty:dff  0.238 -0.709 -0.332\n\n\n\nAdding the cross-level interaction had several effects. First of all, we now have another significant fixed effect indicating that the effect of stimulus intensity depends on the difficulty of the task. We can interpret the fixed effects in the same fashion as in a regular multiple regression analysis. Since task difficulty is a dummy-coded categorical predictor, the intercept represents the fixed intercept in the reference category (difficult task), and the fixed slope of intensity marks the effect of stimulus intensity in the reference category. The fixed effect of task difficulty states how the fixed intercept of the model changes when we switch difficult tasks (the reference category) to easy tasks. Finally, the cross-level interaction indicates how the fixed slope of stimulus intensity changes if we switch to easy tasks. This in mind, we can now state that the positive effect of stimulus intensity on performance is stronger for easy than for difficult tasks.\nLet’s now look at the random effects. Since we have only added a fixed effect, their number is identical to that of the previous model. Since random slopes indicate unspecific cross-level interaction effects, adding a significant cross-level interaction must decrease the variance of the slopes. This is exactly what we observe: the random slopes of intensity is lower in the final model than in its predecessor. We also notice a slight reduction in the variance of the random slopes. The reason for this reduction is the correlation between random slopes and random intercepts. Were the random effects uncorrelated, the variance of the random intercepts would have remained unchanged save for rounding effects."
  },
  {
    "objectID": "inference/inference9.html#comparing-nested-multi-level-models",
    "href": "inference/inference9.html#comparing-nested-multi-level-models",
    "title": "Multi-level models",
    "section": "Comparing nested multi-level models",
    "text": "Comparing nested multi-level models\nThe five models we computed above are nested models, that is, each of the models is more complex than the previous one because it adds at least one new parameter. We could now ask ourselves whether adding a new parameter make the model better. Just as with regular regression models, “better” means that the model provides a better fit to the data. However, since we partition the unexplained variance in a multi-level model, we cannot compute \\(R^2\\) as a measure of model fit (remember that \\(R^2\\) is determined by a regression model’s residual variance).\nIf we want to compare nested multi-level models, we instead use likelihood-ratio tests. Since multi-level models are estimated using a (restricted) maximum likelihood method, we can compute the likelihood \\(L\\) of the model. From the likelihood \\(L\\) we can compute the model deviance \\(D\\).\n\\[D = -2 \\cdot ln(L)\\] The deviance is a measure of “badness”. A model is better, the lower \\(D\\) is. It can be shown that the difference between two model’s deviances \\(D_1\\) and \\(D_2\\) follows a \\(\\chi^2\\)-distribution with degrees of freedom equalling the difference in model parameters \\(k\\).\n\\(D_2-D_1 \\sim \\chi_{k_{2}-k_{1}}\\)\nThis test logic is conceptually similar to the comparison of nested regression models. The only difference is that the test is based on a different fit-metric and uses a different test statistic. Very conveniently, we can use the same function to compare nested multi-level models that we used to compare nested regular regression models, namely the function anova().\nWe can feed the anova() function two or more multi-level models, and it will compare each model to its predecessor. Just as in the case of regular regression models, we need to make sure that the models are nested and entered in the correct order for the output of the function to make sense.\nThere is one other caveat: the anova() function requires all models to be estimated via the maximum likelihood (ML) method. However, the default method for parameter estimation in lme4 is restricted maximum likelihood (REML). If we feed models estimated via REML into the anova() function, the function will recompute the model using ML instead and then compare the models in the order in which we entered them. The potential problem is that the model parameters may slightly differ between the ML and REML method. If we plan to compare models in terms of their fit, it is, therefore prudent to use ML estimation from the start. We can do so by overwriting the default of the REML argument of the lmer() function. The default value is TRUE leading to REML estimation, but if we set it to FALSE instead, the lmer() function will estimate the model parameters using ML.\nIn our example rerunning the models with ML estimation does not change the overall picture (although in the more complex models, the exact parameter estimate differ slightly). We will therefore not reinvestigate the model outputs for the ML versions. Let’s instead have a look at the model comparisons of our five nested models. Here is the syntax:\n\n# comparison of five nested multi-level models predicting performance in a fictional experiment\nanova(mod1, mod2, mod3, mod4, mod5)\n\nonce we run that code, we will see the following output in the console:\n\n\n\nData: df1\nModels:\nmod1: performance ~ 1 + (1 | ID)\nmod2: performance ~ 1 + intensity + (1 | ID)\nmod3: performance ~ 1 + intensity + difficulty + (1 | ID)\nmod4: performance ~ 1 + intensity + difficulty + (1 + intensity | ID)\nmod5: performance ~ 1 + intensity * difficulty + (1 + intensity | ID)\n     npar    AIC    BIC  logLik -2*log(L)   Chisq Df Pr(&gt;Chisq)    \nmod1    3 5393.3 5407.7 -2693.6    5387.3                          \nmod2    4 4820.6 4839.8 -2406.3    4812.6 574.652  1  &lt; 2.2e-16 ***\nmod3    5 4792.0 4816.1 -2391.0    4782.0  30.563  1  3.232e-08 ***\nmod4    7 4738.5 4772.1 -2362.2    4724.5  57.538  2  3.205e-13 ***\nmod5    8 4726.2 4764.6 -2355.1    4710.2  14.287  1  0.0001569 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nLet’s go through that output. The top half is simply a reiteration of the model formulae for the convenience of the reader. The critical information is contained in the lower half. First, the function tells us for each model how many parameters it contains. Here, we count both fixed and random parameters. For example, the first model has three parameters: the fixed intercept, the residual variance and the variance of the random intercepts. Models 2 and 3 each add one fixed effects, meaning that the number of parameters increases by 1 each. Model 4 has two additional parameters even though we only added the random slopes. The second parameter is the correlation between the random intercepts and slopes that the lmer() function estimates automatically. Finally, the fifth model adds one more fixed effect for a total of 8 parameters.\nThe table then shows several related fit indices that are all based on the log-likelihood. We already know that the deviance \\(D\\) is simply \\(-2 \\cdot ln(L)\\). Akaike’s information criterion (AIC) is defined as:\n\\[AIC = D + 2k\\]\nHere, \\(k\\) is the number of model parameters. Just like \\(D\\), the AIC is a measure of “badness”. Adding two times the number of parameters to the deviation \\(D\\) means that AIC punishes the addition of new parameters. The Bayesian information criterion (BIC) is defined as:\n\\[BIC = k\\cdot ln(N) + D\\] Again, \\(k\\) is the number of parameters, and \\(N\\) is the total number of observations. Once more, lower values indicate a better model fit. Furthermore we can see that the BIC, similar to the AIC, punishes the addition of new model parameters, but the punishment is harsher (as soon as we have 8 or more observations, the multiplier for the model parameters exceeds that of the AIC) as it grows stronger with the total sample size.\nFinally, the output of the model comparison shows us the \\(\\chi^2\\)-distributed difference of the model deviations in the column labelled “Chisq” along with the degrees of freedom and the \\(p\\)-value for the test of differences in model fit. In our case, each model outperforms its predecessor in terms of model fit. The likelihood ration test comes in very handy when we want to test whether adding a random effect makes our model better because, other than fixed effects, the random effects do not come with a significance test on their own."
  },
  {
    "objectID": "inference/inference9.html#a-final-note-on-multi-level-model-formulation",
    "href": "inference/inference9.html#a-final-note-on-multi-level-model-formulation",
    "title": "Multi-level models",
    "section": "A final note on multi-level model formulation",
    "text": "A final note on multi-level model formulation\nAs the example above shows, we have a lot of decisions to make when we decide how to model even relatively simple hierarchical data. Do we include random slopes or do we leave it at random intercepts? Should we include main effects only or do we want to model cross-level interactions? Some of these questions will be answered by the theoretical frameworks we base our studies on, but for others it is not that simple. In particular, few theories make statements about random effects (after all, it is the fixed effects we use to test our hypotheses).\nAs a rule of thumb, it is prudent to include random slopes whenever feasible because the assumption that the regression has the exact same slope for all higher-level units is quite a strong one (we will neglect for a moment that we make this exact assumption frequently when using other statistical techniques such as repeated measures ANOVA). In some cases, however, modelling random slopes is not possible. One reason might be that we do not have sufficient level-1 observations within each level-2 grouping unit or our general sample size is too small to reliably estimate a large number of model parameters. In other cases, even if the sample size is large, a model containing random slopes may not converge. What this means is that the optimisation algorithms used to estimate the model parameters run into problems, and the estimated model parameters may not be trustworthy. In those cases, it is wise to remove parameters until the model converges. Often, removing a random slope does the trick."
  },
  {
    "objectID": "intro/intro2.html",
    "href": "intro/intro2.html",
    "title": "Running code in R",
    "section": "",
    "text": "As mentioned before, the R console is where we execute code and where all the computing is done. Before we start using R scripts, we first need to get a rough understanding of what the R console does. Think of the console as a glorified calculator. You tell it to run a piece of code and then R returns the answer.\nThere are two ways to run code in R: entering it manually in the R console or writing a script and executing part or all of it."
  },
  {
    "objectID": "intro/intro2.html#running-code-via-the-console",
    "href": "intro/intro2.html#running-code-via-the-console",
    "title": "Running code in R",
    "section": "Running code via the console",
    "text": "Running code via the console\nWe can execute code by typing it in the console and hitting “Enter”. We will keep it simple for now. Lets say we want R to compute a few numbers. It could look like this:\n\n\n\nFig 1. Input and output of simple code in the R console\n\n\nAs we can see, each of the commands yields the appropriate answer. In theory, we could enter code that is much more complex, but entering long lines of code quickly becomes a hassle we would prefer to avoid. Also, once we close R we would have to enter the same code again manually, which is not only a lot of (unnecessary) work but also error prone.\n\nNote: Generally speaking, using the console directly to run code is not very sensible, the exception being queries using the help-function (more on the help-function later)."
  },
  {
    "objectID": "intro/intro2.html#running-code-via-r-scripts",
    "href": "intro/intro2.html#running-code-via-r-scripts",
    "title": "Running code in R",
    "section": "Running code via R scripts",
    "text": "Running code via R scripts\nThe most common way to run code in R is to write an R script and then execute it. The great advantage of an R script is that we can save it and re-run it as often as we want without having to enter everything by hand once again. If you are keen about Open Science, you can share the Script as part of your Open Analysis Code policy. Since R is free, researchers who want to look at, re-run, or even copy your code can do so without worrying about license fees.\nFor example, we can write a script containing the same computations we entered manually into the console above. To do so, we first need to create a new (end empty) R script by clicking on “File”, then on “New File” and selecting “R Script”.\n\n\n\nFig 2. Creating a new R script\n\n\nWe can now edit the script. Here, we enter each of the computations we want R to do as a single line of our script. Once all four lines are ready, we select them all (either by marking them with the mouse or by clicking ctrl+a). We then tell RStudio to run the selected code by clicking on the “Run”-button on the top right above the script or by pushing ctrl+Enter.\n\n\n\nFig 3. Running the code contained in the R script\n\n\nAs we can see, the output in the console looks as if we had manually entered the code and pressed “Enter” after each line. We could have obtained the same result by copy-pasting the content of the script into the console and hitting “Enter”.\nIn the image above, our script is yet unsaved as can be seen from its name “untitled1”. We can save it by clicking on the floppy disc Symbol (for those of you who still know what a 3.5” floppy disc is) or by clicking on “File” and then “Save” or “Save as”. Since we have not yet named our script, we will be prompted to select a name and to choose in which folder we want to save the script.\nFor example, we can name this script “my first script”. Once we have done that, the tab above the script shows the name of the script (or the start of the name if we chose a long name) followed by “.R” indicating that this is an R file.\n Also note how the name of the script is now shown in black font as opposed to the red font it had prior to saving. Writing a script’s name in red font is RStudio’s way of telling you that this script has unsaved changes."
  },
  {
    "objectID": "intro/intro2.html#commenting-your-code",
    "href": "intro/intro2.html#commenting-your-code",
    "title": "Running code in R",
    "section": "Commenting your code",
    "text": "Commenting your code\nOne crucial part of an R script are comments. Comments are used to explain the code we write. Commenting our scripts is very important as it provides structure to them and makes the code legible, be it for others who try to understand our code or to ourselves (trust me, you will learn to appreciate a well-commented code if you return to a script you wrote a few months back and cannot remember what you did there).\nWe can write comments by adding a # in front of the text. All text following a # in the same line will be ignored by R when running the script.\n\n\n\nFig 5. Running code with comments\n\n\nWhen running the script the whole code is copied to the console, but only the actual code is being evaluated. All the text that we declared as comments is being ignored.\n\nNote: A good rule of thumb is to have at least one line of comments for every four lines of code. When in doubt, opt for more or more detailed comments. Better to over-explain your code than to risk it being unintelligible."
  },
  {
    "objectID": "intro/intro4.html",
    "href": "intro/intro4.html",
    "title": "Data frames and lists",
    "section": "",
    "text": "Here, we will look at two types of R objects that we will frequently encounter when working R, data frames and lists. Both of these object types are a little more complex than the ones we talked about so far, but with greater complexity comes greater utility."
  },
  {
    "objectID": "intro/intro4.html#data-frames",
    "href": "intro/intro4.html#data-frames",
    "title": "Data frames and lists",
    "section": "Data frames",
    "text": "Data frames\nData frames are R objects specifically designed for people who work with data. Just like matrices, they are two-dimensional objects. The main difference, however, is that data frames can contain different types of elements. To be exact, data frames are a combination of different vectors of the same length.\nThe general idea of a data frame is to view every row as an observation and every column as a variable. Therefore, each column represents a vector, which also means that, column-wise, all elements must be of the same type. The same is not true for the rows of a data frame.\n\nHow to define a data frame\nWe can define data frames by calling the function data.frame() and entering vectors or matrices as function arguments. Here, vectors are treated as column vectors (or nx1 matrices, if you will). The only requirement is that all of the vectors and matrices we want to combine into a data.frame have the same number of rows.\nBelow is very simple example of how we can define a data frame in R. Imagine, we have collected data from 5 participants, which we will identify by the numbers from 1 to 5, and that we asked them to report their gender (open response) and age in years (numeric value).\n\nmy_data_frame = data.frame(\n  c(1, 2, 3, 4, 5),                                       # participant ID\n  c('male', 'female', 'non-binary', 'female', 'female'),  # participant gender\n  c(25, 19, 23, 22, 28)                                   # participant age\n)\n\n\nNote: In this example, the code is distributed across several lines. Technically, we could have written it all in one long line, but doing so makes it very difficult to read the code. Unless we call a function with few or very short function arguments, it is recommended to use one line for each argument.\nAlso note how the function arguments are shifted a bit to the right. This is called indentation. Indentation has no effects on how the code works in R (it does in other programming languages such as Python!). However, it contributes greatly to the legibility of the code by structuring it.\nIt is, therefore, a good idea to get used to organizing longer pieces of code by using multiple lines and proper indentation. Your future self, your collaborators, and third parties, who will read your code when you make it publicly available, will thank you for it.\n\nOnce we execute the piece of code above, the data frame will appear as an object in RStudio’s Environment (top right). We can immediately see that it is a data frame because RStudio describes data frames as \\(x\\) observations of \\(y\\) variables. In our case that is listed as “5 obs. of 3 variables”.\n\nNote: The Environment will show a small light blue button with a white arrowhead to the left of our data frame’s name. If we click this button, RStudio will “unfold” the data frame and show us a list of the vectors it contains as well as the type of the vector.\n\nJust as with matrices, we can have a look at our new object by either entering its name or by clicking on it in the environment. If we do the former, R will return the data frame in the console, whereas the latter will open the data frame in a separate tab next to our scripts. Let’s go with the code version.\n\n\n\n  c.1..2..3..4..5. c..male....female....non.binary....female....female..\n1                1                                                  male\n2                2                                                female\n3                3                                            non-binary\n4                4                                                female\n5                5                                                female\n  c.25..19..23..22..28.\n1                    25\n2                    19\n3                    23\n4                    22\n5                    28\n\n\n\n\n\nAssigning proper variable names\nWe may notice two things: first, the data frame contains exactly the information we specified as function arguments for the data.frame() function; second, each column of our data frame has a very weird name. The reason is that data frames require variable names for each column, and if we do not specify these names, R will just use each vector’s content as a name. This is inconvenient for a number of reasons, so it is desirable to assign proper names to the variables.\nOne way to do so is calling the function names(), which allows us to change the names of a data frame’s variables. The function ‘names’ requires the name of the data frame as a function argument. We then define it as a character vector containing the new variable names. The character vector must contain one element for each column of the data frame. For our data frame, it could look like this:\n\nnames(my_data_frame) = c('ID', 'gender', 'age')\n\nIf we now ask R to show us the data frame in the console (by entering its name as code), it will look much tidier.\n\n\n\n  ID     gender age\n1  1       male  25\n2  2     female  19\n3  3 non-binary  23\n4  4     female  22\n5  5     female  28\n\n\n\nAnother way to assign sensible variable names is to specify them when creating the data frame. We can do so by making slight adjustments to the way we enter vectors as function arguments when calling the data.frame() function. In fact, the code will look as if we define vectors as objects. The only difference is that when doing so within the data.frame() function, the vectors will not be created outside the data frame (i.e., they will not appear as separate objects in the Environment). Here is what the code would look like for our data frame.\n\nmy_data_frame = data.frame(  \n  ID = c(1, 2, 3, 4, 5),                                          # ID\n  gender = c('male', 'female', 'non-binary', 'female', 'female'), # gender\n  age = c(25, 19, 23, 22, 28)                                     # age\n) \n\nThe result is the same as if we had used the names() function.\n\n\n\n  ID     gender age\n1  1       male  25\n2  2     female  19\n3  3 non-binary  23\n4  4     female  22\n5  5     female  28\n\n\n\n\n\nEntering matrices into data frames\nAs mentioned above, we can also enter matrices as function arguments when creating a data frame, as long as these matrices have the correct number of rows. Let’s assume that we additionally collected responses from two Likert scale items (levels 1 to 5), which we would like to enter into our data frame. Our code would then look like this:\n\nmy_data_frame = data.frame(  \n  ID = c(1, 2, 3, 4, 5),                                          # ID\n  gender = c('male', 'female', 'non-binary', 'female', 'female'), # gender\n  age = c(25, 19, 23, 22, 28),                                    # age\n  matrix(c(1, 3, 5, 4, 3, 5, 5, 3, 2, 2), nrow = 5)               # responses \n) \n\nRunning this code creates a data frame with 5 observations of 5 variables in in the environment. One problem is that we cannot name the two new variables contained in the matrix as we did with the vectors. However, when we look at our data frame, we will notice that the variable names do not look as terrible as they did when we entered the unnamed vectors. Instead, the variables will be named “X1” and “X2” (when entering a matrix, R will just assign the names from “X1” to “Xi” for a matrix with \\(i\\) columns).\nStill, we might prefer less obscure names for our two measures. One possibility is to use the names() function as we did above. Another is to create the matrix as a separate object before creating the data frame. This allows us to set proper column names using the function colnames(). This function works very similar to the names() function but is specifically designed for matrices. Here is what the code would look like.\n\n# First, we create our 5x2 matrix as an object\nresponse_matrix = matrix(\n  c(1, 3, 5, 4, 3, 5, 5, 3, 2, 2), \n  nrow = 5\n)\n\n# Next, we assign column names\ncolnames(response_matrix) = c('item1', 'item2')\n\n# Finally, we create the data frame\nmy_data_frame = data.frame(  \n  ID = c(1, 2, 3, 4, 5),                                          # ID\n  gender = c('male', 'female', 'non-binary', 'female', 'female'), # gender\n  age = c(25, 19, 23, 22, 28),                                    # age\n  response_matrix                                                 # responses \n) \n\nThe new variables in our data frame now have proper names. We can check this by calling the name of our data frame and inspecting it in the console (or by clicking on its name in the Environment).\n\n\n\n  ID     gender age item1 item2\n1  1       male  25     1     5\n2  2     female  19     3     5\n3  3 non-binary  23     5     3\n4  4     female  22     4     2\n5  5     female  28     3     2\n\n\n\n\nDefining objects outside a data frame and then simply using their name as a function argument when calling the data.frame() function is not limited to matrices but can also be done for vectors. While there are good reasons for defining objects separately before combining them into a data frame, one downside is that our Environment can get a wee bit cluttered.\nIf we want to tidy up our environment, we can remove objects we no longer need (for example, because we put them into a data frame) using the function rm(). We simply need to use the name of the object we want to remove as the function argument for rm(), and it will be disappear from the Environment."
  },
  {
    "objectID": "intro/intro4.html#lists",
    "href": "intro/intro4.html#lists",
    "title": "Data frames and lists",
    "section": "Lists",
    "text": "Lists\nThe final type of R object we need to know for now is the list. Lists are very flexible containers that we can create using the list() function. Think of lists as multi-purpose storage units. They can contain all other types of R objects (including other lists). For example, we could create a list that contains a single character string, a numeric vector, and a data frame. To do so, we simply call the function list() and enter each object we want to store in the list as a function argument.\n\nmy_value = 'hello'          # a single character value\nmy_vector = c(1,1,2,3,5,8)  # a numeric vector\n\n# The following code creates a list with three elements\nmy_list = list(\n  my_value,\n  my_vector,\n  my_data_frame\n)\n\nrm(my_value, my_vector)   # this removes two of the objects from the environment\n\nAs with other objects, the new list will appear in the Environment. RStudio tells us that this object is a list of 3. Just like for a data frame, there is a small light blue button to its left that allows us to unfold the list and have a look at its contents.\nWe can also click on the list in the environment to have a look at it or have R print the list in the console by calling its name. If we do the latter, the output looks like this:\n\n\n\n[[1]]\n[1] \"hello\"\n\n[[2]]\n[1] 1 1 2 3 5 8\n\n[[3]]\n  ID     gender age item1 item2\n1  1       male  25     1     5\n2  2     female  19     3     5\n3  3 non-binary  23     5     3\n4  4     female  22     4     2\n5  5     female  28     3     2\n\n\n\nAs we can see, the output lists the elements of our list one after another. The first element of the list is preceded by “[[1]]”, the second by “[[2]]”, and so on.\n\nNote: Once we start analysing data in R, we will frequently encounter lists. The reason is that many R functions used in inferential statistics use lists as outputs."
  },
  {
    "objectID": "working/working1.html",
    "href": "working/working1.html",
    "title": "Binary Operators",
    "section": "",
    "text": "We already know ho to define R objects. We will now turn to the question how to work with them. Generally speaking, we can work with R objects by performing operations on them. There are two ways of doing so: one is to use binary operators. Another way to perform operations on R objects is to feed them into functions as function arguments. Here, we will focus on binary operators."
  },
  {
    "objectID": "working/working1.html#prelude---the-operator",
    "href": "working/working1.html#prelude---the-operator",
    "title": "Binary Operators",
    "section": "Prelude - the $ operator",
    "text": "Prelude - the $ operator\nBefore we delve into binary operators, we need to learn about a very useful operator specific to data frames and lists with named elements.\nThis operator is $. It is not a binary operator because it requires only one object.\nSpecifically, the $ operator allows us to extract a named element from a data frame or list.\nTo do so, we need two first write the name of the data frame (or list), immediately follow it with the $ operator, and then follow with the name of the element we want to obtain.\nHere is what the code looks like, generally speaking:\n\ndata_frame$variable_name\n\nHere is an example, in which we obtain a variable called age from a data frame called my_data_frame that, among others, contains this variable.\n\n# create a data frame\nmy_data_frame = data.frame(\n  ID = 1:5,\n  gender = c('m', 'f', 'f', 'nb', 'm'),\n  age = c(32, 66, 37, 41, 38)\n)\n\n# obtain age from the data frame\nmy_data_frame$age\n\nIf we run this code, R will display the information contained in the column labelled age in the console with the output looking as follows:\n\n\n\n[1] 32 66 37 41 38\n\n\n\nWe could also define the extracted column as a separate object and save it in the Environment.\nOne extremely useful feature in R is that we can use the $ operator to create new elements in a data frame or list. For example, we can add a new variable to the data frame from the previous example.\n\n# add a variable to the data frame coding whether \n# the participants is currently employed\nmy_data_frame$employed = c(T, T, F, T, T)\n\nOnce we run the code, R will update the data frame in the Environment. It will now be shown as “5 obs. of 4 variables” instead of the original “5 obs. of 3 variables”. Upon inspection, we can confirm that it now contains the new variable employed as its last column.\nHere is what the data frame would look like in the console:\n\n\n\n  ID gender age employed\n1  1      m  32     TRUE\n2  2      f  66     TRUE\n3  3      f  37    FALSE\n4  4     nb  41     TRUE\n5  5      m  38     TRUE"
  },
  {
    "objectID": "working/working1.html#back-to-binary-operators",
    "href": "working/working1.html#back-to-binary-operators",
    "title": "Binary Operators",
    "section": "Back to binary operators",
    "text": "Back to binary operators\nBinary operators are symbols that R uses to represent a specific operation involving two objects. These operations can be arithmetic or logical. Before we jump into action, we need to have a look at how R’s binary operators look like.\n\nArithmetic binary operators\nR has seven built-in arithmetic binary operators (you will probably not use the last two, but we will include them for the sake of completeness).\n\n\n\n\n\n\n\n\nOperator\nOperation\nWhat R does\n\n\n\n\n+\naddition\ncomputes the sum of two numbers\n\n\n-\nsubtraction\nsubtracts the second number from the first\n\n\n*\nmultiplication\ncomputes the product of two numbers\n\n\n/\ndivision\ndivides first number by the second\n\n\n^\npower\ntakes the first to the power of the second\n\n\n%%\nmodulo\ntakes the remainder of division\n\n\n%/%\ninteger division\ndivision rounded down to whole numbers\n\n\n\n\n\nLogical binary operators\nBesides arithmetic operators, R has several built-in logical binary operators. Logical binary operators also require two objects as arguments. They compare the object to the left of the operator to the object on its right and check if the result of this comparison is TRUE or FALSE. In other words, they return a Boolean value. Here is the list of logical operators:\n\n\n\n\n\n\n\nOperator\nWhat R tests\n\n\n\n\n&lt;\nthe first value is less than the second\n\n\n&lt;=\nthe first value is less than or equal to the second\n\n\n&gt;\nthe first value is greater than the second\n\n\n&gt;=\nthe first value is greater than or equal to the second\n\n\n==\nthe first value is exactly equal to the second\n\n\n!=\nthe first value is not equal to the second\n\n\n\n\n\nUsing binary operators on single values\nThe simplest way to use binary operators is to use them on single values. In the case of arithmetic operators, this comes down to adding, subtracting, multiplying etc. two numbers. As the two arguments for the operators, we can use values we defined as objects prior to the operation, values that we enter as is, or a combination of both. Here are a few examples.\n\na = \"hello\"   # a character value\nb = FALSE     # a Boolean value\nd = 13        # a numeric value\ne = 2         # another numeric value\n\na != \"hello\"  # tests if a is unqeual to the string \"hello\" (this is FALSE)\n\nb == FALSE    # tests if b is the Boolean value FALSE (which is TRUE)\n\nd + 3         # adds 3 to the object d (for a total of 10)\n\nd ^ e         # takes d to the power of e (the result is 169)\n\nThis is what appears in the console:\n\n\n\n[1] FALSE\n\n\n[1] TRUE\n\n\n[1] 16\n\n\n[1] 169\n\n\n\n\nNote In the example above, we named the four objects a, b, d, and e. This was neither an oversight nor an expression of dislike toward the letter c. The simple reason is that there is a function called c(), and it is prudent to avoid giving objects the same name as existing functions.\nTechnically, it is possible to assign an object the name of a function, but it may lead to confusion or problems with the R code. Therefore, it is best avoided.\n\nWe can create more complex operations by combining multiple arithmetic and/or logical operators involving multiple values, and we can save the result by defining it as another object.\n\nNote that R follows the basic rules of arithmetic operations. That is, power takes precedence over multiplication or division, which, in turn, take precedence over addition or subtraction. Just as in school maths, we need to use parentheses to organize our operations accordingly.\nWe can also use parentheses in the same fashion when combining multiple logical operations.\nWhen combining arithmetic and logical operators, the arithmetic operations take precedence over the logical ones.\n\nExample 1:\n\nx = 3   # define a numeric value\ny = 2   # define another value\n\nz = x ^ 2 / (x * y - x)\n\nIn the example above, we defined a numeric value called z. Since power takes precedence over division, R first computes x to the power of 2 (for a total of 9). It then divides 9 by the expression in the parentheses. Within the parentheses, multiplication takes precedence over subtraction, which means that R first computes the product of x and y (which is 6) and then subtracts x for a total of 3. So our code boils down to dividing 9 by 3. We can easily verify that R did that by inspecting the new object z.\n\nz\n\n\n\n\n[1] 3\n\n\n\nExample 2:\n\nx = 3      # define a numeric value\ny = 2      # define another value\nz = 'red'  # define a character value\n\nz != 'red' & (x &gt; 2 | x + y == 7)\n\nIn this example, we test whether the two conditions combined by & (logical AND) are simultaneously TRUE. First, whether the value z equals the string ‘red’, and second, whether at least one of the following two statements combined with the | (logical OR) is TRUE: the value x is greater than 2 OR the sum of x and y equals 7.\nThe first statement is TRUE because z equals the character string ‘red’. The second statement is also true. While the sum of x and y clearly differs from 7 and is, thus, FALSE, x is greater than 2. Since the logical OR only requires one of the statements to be TRUE, the whole statement in parentheses is TRUE. Therefore, executing this code should return the Boolean TRUE, which we can check by inspecting the output in the console.\nIt looks as follows:\n\n\n[1] TRUE\n\n\n\nIn theory, arithmetic operators should only work on numeric values (integer or double). Accordingly, R will complain if at lest one of the objects we use as arguments is a character string. Specifically, it will return an error message in the console stating that we assigned a non-numeric argument to the binary operator.\nSomething similar should happen if we assign at least one Boolean value, that is, a value stating either a logical TRUE or FALSE. Keep in mind, however, that R sometimes changes the type of an object so that it works with an operator or function (this is called coercion). If we use a Boolean value in an arithmetic operation, R will just treat it as a binary numeric variable (FALSE = 0, TRUE = 1).\n\n\n\nUsing binary operators on vectors and matrices\nWe can also use binary operators on vectors or matrices. The exact operation depends on the two objects involved. More specifically, if one of the two arguments of a binary operator is a vector or a matrix, then the operation differs depending on whether the second argument is a single vector or another vector or matrix.\nLets first look at the (simpler) case where one of the two arguments is a vector/matrix and the other is a single value. In this case, the exact same operation is performed on all elements of the vector or matrix. For example adding a single value and a vector/matrix means that the single value is added to each element of the vector/matrix, multiplying the vector/matrix by a number means that each element is multiplied by that number, testing whether a vector/matrix equals a certain character string tests for each element whether it equals that string, and so on. Let’s look at some examples.\n\nv1 = c('red', 'green', 'blue')  # defines a character vector\n\nm1 = matrix(1:9, nrow = 3)      # defines a numeric 3x3 matrix\n\na = 2         # defines a numeric value\n\nv1 == 'red'   # tests for each element of v1 whether it equals the string 'red'\n\na ^ m1        # takes a (the value 2) to the power of each element of m1\n\nm1 &lt; 5        # tests for each element of m1 whether it is smaller than 5   \n\nThis is what appears in the console:\n\n\n\n[1]  TRUE FALSE FALSE\n\n\n     [,1] [,2] [,3]\n[1,]    2   16  128\n[2,]    4   32  256\n[3,]    8   64  512\n\n\n     [,1]  [,2]  [,3]\n[1,] TRUE  TRUE FALSE\n[2,] TRUE FALSE FALSE\n[3,] TRUE FALSE FALSE\n\n\n\nIrrespective of whether we enter the vector/matrix as the first or the second argument of the binary operator, the output has the same size and dimensions. That is, multiplying a vector of length 4 by a number yields a numeric vector of length 4. Multiplying a 3x3 matrix by a number yields another numeric 3x3 matrix. Testing whether a statement is true for a vector of length 7 returns a Boolean vector of length 7, and so on.\nWe can also use binary operators with both arguments being vectors or matrices. As a general rule, we must use objects of equal size in those operations, that is, if the object to the left of the operator is a vector of length 6, then the object to its right must also be a vector of length 6. Likewise, if we enter a 3x4 matrix to the left of the operator, our argument on its right will also have to be a 3x4 matrix.\n\nCaveat: While R complains when we enter two matrices of unequal size as arguments of a binary operator (it will print an error message in the console), the same is not true for vectors. If we sue binary operators on vectors of unequal length, R will simply extend the shorter vector by starting over.\nFor example, when adding a vector containing the numbers from 1 to 3 to another vector containing the numbers from 1 to 5, R will turn the first vector into a vector of length 5 by repeating the first two elements (the resulting vector then has the elements 1, 2, 3, 1, 2).\nThis built-in feature of R can cause problems because situations, in which we actually want to perform binary operations on vectors of unequal length, are rare. Since R will run the code without any complaints, we run the risk of missing bugs in our code. Therefore, it is prudent to double-check the code whenever we use binary operators with two vectors.\n\nWhen using binary operators with two vectors or matrices, R performs an element-wise operation. That means that R pairs the first element of the first vector or matrix with the corresponding element of the second vector or matrix when performing the operation. The second element of the first object is paired with second element of the second object, and so on. Let’s again look at a few examples.\nVectors first.\n\nv1 = c(2, 4, 6, 8)    # defines a numeric vector\n\nv2 = 1:4              # lazy way of creating a vector of length 4\n\nv1 + v2               # performs element-wise addition of the vectors\n\nv1 != v2              # takes each element of v1 and tests whether it is\n                      # different from the corresponding element of v2\n\nThis is what appears in the console:\n\n\n\n[1]  3  6  9 12\n\n\n[1] TRUE TRUE TRUE TRUE\n\n\n\nNow for some matrices.\n\nm1 = matrix(c(2, 4, 6, 8), nrow = 2)    # defines a 2x2 matrix\n\nm2 = matrix(1:4, ncol = 2)              # defines another 2x2 matrix\n\nm1 - m2               # performs element-wise subtraction of the matrices\n\nm1 &lt; m2               # takes each element of m1 tests whether it is less than \n                      # the corresponding element of m2   \n\nThis is what appears in the console:\n\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\n      [,1]  [,2]\n[1,] FALSE FALSE\n[2,] FALSE FALSE\n\n\n\nAs we can see, performing arithmetic operations on matrices using binary operators yields numeric matrices of the same size as the input matrices as result. When using logical binary operators instead, the result will be a Boolean matrix of equal size as the two input matrices.\n\nNote that using the binary operator for multiplication (*) on matrices does not do what we would expect from regular matrix algebra. If we want to do classic matrix multiplication, we need to use a special operator, namely %*%."
  },
  {
    "objectID": "working/working3.html",
    "href": "working/working3.html",
    "title": "Numerical Indexing",
    "section": "",
    "text": "Most R objects consist of multiple elements, the exception being single values (or vectors of length 1 and 1x1 matrices, which technically are single values, too). There will be situations where we would like to view, extract, or change some, but not all elements of an object. For example we might want to remove the first four cases from a data set because they were test runs, or we might be interested in how the 22nd participant in our recent study responded to questions 13 and 14. What we do in those cases is tell R to look for specific elements of an object. That is called indexing.\nThe most generic form of indexing uses brackets. Specifically, we first write the name of the object of interest. In brackets following the object’s name, we define which elements we want R to obtain. We can obtain elements by their referring to their (numerical) position in an object or via logical operations (using either binary operators or functions).\nNumerical indexing means that we tell R in brackets which elements it should obtain by entering the elements’ position within the object. If the object is a vector, we need only provide a single number per element, where we need two coordinates in case our object is two-dimensional (e.g., a matrix or a data frame). Using numerical indexing, we can ask R to obtain a single element but also multiple elements of an R object.\nLets look at a few examples using vectors first."
  },
  {
    "objectID": "working/working3.html#numerical-indexing-of-vectors",
    "href": "working/working3.html#numerical-indexing-of-vectors",
    "title": "Numerical Indexing",
    "section": "Numerical indexing of vectors",
    "text": "Numerical indexing of vectors\n\nv1 = c(5, 7, 11, 22, 3, 1, 19) # create a vector with arbitrary numbers\n\n# obtain the 3rd element of vector v1\nv1[3]\n\n# obtain elements 2 to 5 of v1\nv1[c(2,3,4,5)]\n\n# obtain elements 2 to 5 of v1 (lazy/efficient version)\nv1[2:5]\n\n# obtain elements 2, 5, and 7 of v1\nv1[c(2,5,7)]\n\nUsing the code above yields the following output in the R console (because we did not save the obtained elements of v1 as objects of their own, R will print the result in the console).\n\n\n\n[1] 3\n\n\n[1] 2 3 4 5\n\n\n[1]  2  5 NA\n\n\n[1] 2 2\n\n\n[1] 1 2 3\n\n\n\nA neat trick in R is that we can tell it to obtain all elements of an object except for those we specify in brackets. We can do so by preceding the selection of elements with the operator -.\n\n# obtain all elements of v1 except for the 1st and 2nd (3 to 7)\nv1[-(1:2)]\n\n# obtain all elements of v1 except for those (4 to 6)\n# specified in v2\nv1[-v2]\n\nThe output in the console looks like this:\n\n\n\n[1] 3 4 5 6 7\n\n\n[1] 4 5 6 7\n\n\n\nRemember that data frames are essentially a container for column vectors of equal length. Remember also that we can obtain an existing column (i.e., one variable) of a data frame using the $ operator. We can obtain elements of that variable using numerical indexing just as we did in the examples above. Here is what the syntax looks like.\n\n# create a data frame with some demographic data\nmy_df = data.frame(\n  ID = 1:6,                                   \n  age = c(40, 51, 32, 23, 55, 68),\n  gender = c('m', 'f', 'nb', 'f', 'f', 'm'),\n  employment = c(T, T, T, T, T, F)\n  )\n\n# obtain the first element of the age variable\nmy_df$age[1]\n\n# obtain elements 2:4 of the gender variable\nmy_df$gender[2:4]\n\n# obtain all elements of the employment variable \n# except for the first and the last \nmy_df$employment[-c(1, 6)]\n\nHere is what the output in the console would look like:\n\n\n\n[1] 40\n\n\n[1] \"f\"  \"nb\" \"f\" \n\n\n[1] TRUE TRUE TRUE TRUE\n\n\n\nIn some of the examples above, we used vectors in brackets to tell R which elements of the vector v1 we want to obtain. Generally speaking, we can obtain elements of R objects by indexing objects of the same dimensions in brackets, that is, we can use vectors to obtain elements of one-dimensional objects such as vectors and lists, and matrices to obtain elements of two-dimensional objects such as matrices and data frames."
  },
  {
    "objectID": "working/working3.html#numerical-indexing-of-matrices-and-data-frames",
    "href": "working/working3.html#numerical-indexing-of-matrices-and-data-frames",
    "title": "Numerical Indexing",
    "section": "Numerical indexing of matrices and data frames",
    "text": "Numerical indexing of matrices and data frames\nLet’s now turn to two-dimensional objects. As mentioned above, we need to tell R the coordinates of the elements we want to obtain in the brackets following the object’s name. The coordinates must be separated by a comma, and we specify the rows before the columns (remember the roman-catholics as a mnemonic aid).\nFor the following examples, we fist generate a 4x4 matrix containing the numbers form 1 to 16.\n\nm1 = matrix(1:16, nrow = 4)  # create a numeric 4x4 matrix\n\nThe matrix looks as follows:\n\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\n\nNow let’s look at few examples of numerical indexing using our matrix m1.\n\n# extract the element in the 2nd row and the 3rd column (10)\nm1[2, 3]\n\n# extract all elements that are in rows 1 and 3 \n# and at the same time in columns 4 and 2\nm1[c(1, 3), c(4, 2)]\n\n# extract elements 1 to 3 in the 2nd row (2, 6, and 10)\nm1[2, 1:3]\n\nHere is what we will see in the console when running the code above.\n\n\n\n[1] 10\n\n\n     [,1] [,2]\n[1,]   13    5\n[2,]   15    7\n\n\n[1]  2  6 10\n\n\n\nA look at the console shows that R returns the desired elements as vectors. The reason is that we either asked for a single value (example 1), two elements from very different parts of the original matrix (example 2), or a vector of neighboring values within the original matrix (example 3).\nIt is possible, though, to obtain elements from a matrix so that R returns another matrix (of smaller size). The output will be another matrix if we extract elements from different rows but the same columns (or vice versa). Let’s have a look at a few examples below:\n\n# extract elements 1 to 3 in the 2nd row (2, 6, and 10)\n# and the 4th row (4, 8, 12)\nm1[c(2, 4), 1:3]\n\n# extract the 3rd and 4th elements of columns 1 (3, 4) \n# and 2 (7, 8)\nm1[3:4, 1:2]\n\nA look at the console confirms, again, that the code worked as intended.\n\n\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n     [,1] [,2]\n[1,]    3    7\n[2,]    4    8\n\n\n\nSometimes we might want to extract all elements of certain rows or columns of a two-dimensional R object. Theoretically, we could do so by entering all rows/columns in brackets, but R has an easier way for us to do that: stating nothing! Yes, you read that correctly. If we do not specify the rows or columns in brackets, R will understand that we want all of them. Since in those cases, the elements will either share their row or column positions, the resulting objects will be two-dimensional again (see examples below).\n\n# extract the complete 1st row (1, 5, 9, 13) and \n# 3rd row (3, 7, 11, 15)\nm1[c(1, 3), ]\n\n# extract the complete 3rd (9 to 12) and 4th column\n# (13 to 16)\nm1[, 3:4]\n\nHere is what R prints in the console:\n\n\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n     [,1] [,2]\n[1,]    3    7\n[2,]    4    8\n\n\n\nFinally, as with vectors, we can use the operator - to tell R that we want all elements of a two-dimensional object except for some of them. For example, we could specify that we want to exclude some rows and some columns in brackets. Since we have two dimensions, we can also combine selection of certain rows with the exclusion of columns and vice versa. Here are a few examples.\n\n# remove the first row but obtain all columns \nm1[-1, ]\n\n# obtain rows 1:3, but remove columns 2 and 4\nm1[1:3, -c(2, 4)]\n\nThe output in the console looks like this:\n\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    3    7   11   15\n[3,]    4    8   12   16\n\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    2   10\n[3,]    3   11\n\n\n\nIn the examples above, we always obtained some elements of a matrix. However, the code would work in the same fashion, had the matrices been data frames instead. To show that this is true, lets look at a few examples using he data frame we created above.\n\n# obtain the element that is in the 3rd row\n# and the 2ns column of the data frame\nmy_df[3, 2]\n\n# obtain the first variable of the data frame\n# (yields the same output as my_df$ID would)\nmy_df[, 1]\n\n# obtain rows 2:4 of the data frame\nmy_df[2:4, ]\n\n# obtain all elements in rows 3 to 6 and\n# in all columns except for the first\nmy_df[3:6, -1]\n\nAs we can see from the output, two-dimensional indexing of data frames works as intended.\n\n\n\n[1] 32\n\n\n[1] 1 2 3 4 5 6\n\n\n  ID age gender employment\n2  2  51      f       TRUE\n3  3  32     nb       TRUE\n4  4  23      f       TRUE\n\n\n  age gender employment\n3  32     nb       TRUE\n4  23      f       TRUE\n5  55      f       TRUE\n6  68      m      FALSE"
  },
  {
    "objectID": "working/working3.html#numerical-indexing-of-lists",
    "href": "working/working3.html#numerical-indexing-of-lists",
    "title": "Numerical Indexing",
    "section": "Numerical indexing of lists",
    "text": "Numerical indexing of lists\nFinally, we need to look at how numerical indexing works for lists. At first glance, lists seem to follow the same logic as vectors. They, too, are one-dimensional objects with a certain number of elements. However, indexing of lists uses a slightly different syntax. Specifically, we need to tell R which element or elements of a list we want to obtain using double brackets.\nLet’s first create a simple list to work with. Note how the last element of our list is another list (yes, you can put lists into list, that is how cool R lists are).\n\n# The following code creates a list with four elements\nl1 = list(\n  my_value = 'hello',               # a character value\n  \n  my_vector =  c(1,1,2,3,5,8),      # a numeric vector\n  \n  my_matrix = matrix(NA, nrow = 3,  # a 3x3 matrix of NAs\n                     ncol = 3),      \n  my_list = list(                   # a list of two objects\n    a = c('happy', 'hippo'),\n    b = 42\n  )\n)\n\nHere is what the new list looks like.\n\n\n\n$my_value\n[1] \"hello\"\n\n$my_vector\n[1] 1 1 2 3 5 8\n\n$my_matrix\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n[3,]   NA   NA   NA\n\n$my_list\n$my_list$a\n[1] \"happy\" \"hippo\"\n\n$my_list$b\n[1] 42\n\n\n\nWe can now obtain elements of the list. However, lists are a bit more complicated than the other objects. In fact, there are two ways to obtain elements of a list. The first is using single brackets just as we did it with vectors. If we do that, R will return to us another list containing only the specified elements of the original list.\nIf we instead want to obtain a specific element of a list, we can index that element using double brackets instead of the single ones. Using double brackets will return the element in its original form. It will obtain values as value,s vectors as vector, matrices as matrices and so on. That is, R will not return a list when we use double brackets unless the element we are looking for is another list.\nLet’s look at a few examples using single brackets first (we will do them one by one because the output of a list in the console can be a bit longer).\nWe can obtain a single element.\n\n# obtain a list containing element 2 of the list l1\nl1[2]\n\n\n\n\n$my_vector\n[1] 1 1 2 3 5 8\n\n\n\nWe can obtain multiple elements\n\n# obtain a list containing elements 2 to 4 of the l1\nl1[2:4]\n\n\n\n\n$my_vector\n[1] 1 1 2 3 5 8\n\n$my_matrix\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n[3,]   NA   NA   NA\n\n$my_list\n$my_list$a\n[1] \"happy\" \"hippo\"\n\n$my_list$b\n[1] 42\n\n\n\nWe can exclude some elements using the - operator.\n\n# obtain a list containing all elements of l1 except for the 3rd\nl1[-3]\n\n\n\n\n$my_value\n[1] \"hello\"\n\n$my_vector\n[1] 1 1 2 3 5 8\n\n$my_list\n$my_list$a\n[1] \"happy\" \"hippo\"\n\n$my_list$b\n[1] 42\n\n\n\nLet us now turn to the case where we ant to obtain a specific element of a list. As mentioned above, we will use double brackets to do that. Note that we can only obtain a single element of a list in that fashion. If we try to specify multiple elements in double brackets, R will complain by returning an error message.\n\n# obtain element 2 of the list l1\nl1[[2]]\n\n\n\n\n[1] 1 1 2 3 5 8\n\n\n\nSome elements of our list are indexable elements on their own. When we obtain such an element using double brackets, we can use hierarchical indexing. That is, we can tell R to obtain parts of a part of a list. We can do that by first indexing the element of the list we want to obtain using double brackets and then specifying which element of the obtained element we want R to extract using either single brackets or double brackets depending on whether the desired element is another list or not.\nHere is some example code, in which we extract the first three elements of the second element of our list.\n\n# obtain elements 1 to 3 of the second element of l1\nl1[[2]][1:3]\n\n\n\n\n[1] 1 1 2\n\n\n\nAs a final example (just because we can), lets obtain the second element of the character vector that forms the first element of my_list, which is the last element of l1. In those cases, we need to think backwards (similar to calling multiple functions in one go), that is, we tell R to obtain the fourth element of l1, then to obtain that elements’ first element, and then to obtain that elements’ second element. The code to do so looks as follows:\n\n# obtain the second element of the 1st element of\n# the 4th element of the list l1\nl1[[4]][[1]][2]\n\n\n\n\n[1] \"hippo\"\n\n\n\n\nImportant: Given that indexing of lists is somewhat tricky, you might be wondering why we should bother with it. The answer is that many of the functions we will be using to analyse our data will return lists as their output. These lists can contain tons of information, and we may often require only some of it. Therefore, knowing how to access parts of a list can be extremely handy."
  },
  {
    "objectID": "working/working3.html#saving-and-overwriting-indexed-elements",
    "href": "working/working3.html#saving-and-overwriting-indexed-elements",
    "title": "Numerical Indexing",
    "section": "Saving and overwriting indexed elements",
    "text": "Saving and overwriting indexed elements\nSo far, we have only used numerical indexing to make R show us the elements in the console. However, we can do two more things with the obtained elements:\n\nwe can use them as the definition of a new R object\nwe can overwrite them by defining new values for them\n\nIn other words, we can use them as both the left-hand-side and the right-hand-side argument in object definition.\n\nSaving elements of an object as a new object\nLet’s first look at the case in which we want to save parts of an object as an object of its own. For example, we might be interested in creating a reduced data frame, that contains only some of the variables of the original data frame. We will use the data frame defined above in this example.\n\n# create a reduced data frame that omits the \n# employment variable (4th column)\nreduced_df = my_df[, -4]\n\nRunning the code above will create a new object called reduced_df in the Environment. As we can see, it consists of 6 observations of 3 variables as compared to the original data frames 6 observations of 4 variables (which makes perfect sense since we asked R to remove one variable.\n\n\n\nFig 1. Defining part of a data frame as a new object\n\n\nHere is what the new data frame looks like in the console:\n\n\n\n  ID age gender\n1  1  40      m\n2  2  51      f\n3  3  32     nb\n4  4  23      f\n5  5  55      f\n6  6  68      m\n\n\n\n\n\nOverwriting elements of an object\nLet’s now have a look at the second case, in which we use part of an R object as the left-hand-side argument of object definition. As mentioned above, this corresponds to overwriting the existing values of those elements with new values. If we want to overwrite elements of an object, we must consider two rules:\nFirst, the new values must conform with the type of the object. We should overwrite numeric value only with numeric value, character strings with only with other character strings, and so on. If we overwrite existing values with values of different types, R will not return an error message. Instead, it will coerce the object such that all elements are of equal type. For example, trying to overwrite elements of a numeric or Boolean object with a character string will turn the whole object into a character string. If we overwrite an element of a numeric vector with a Boolean value (TRUE or FALSE), the object will remain numeric, and the new values will be interpreted as either 1 or 0, and so on.\n\nIn case of a data frame, each column works as its own object, that is, messing up one variable in this way will usually not mess up the other variables.\nHowever, if we overwrite a complete row of the data frame, we can potentially mess up the whole data frame, because now we tamper with elements of each column.\n\nSecond, we need to tell R the correct number of new values for the elements we want to overwrite. For example, if we want to overwrite two elements of a vector, we need to define the new values in a vector of length two, and if we want to overwrite the elements of a \\(3\\times2\\) matrix, the new values must be defined either in a \\(3\\times2\\) matrix or as a vector of length 6 (in the latter case, R will simply overwrite the old values with the values contained in the vector by filling columns from top to bottom and then moving to the next column). There is an exception to the second rule: we can always specify a single value as the new value. In this case, R will overwrite all of the old values with this new value.\nLet’s look at a few examples using the original data frame we defined above.\n\n# The first participant entered the wrong age; we need to overwrite it with the correct value\nmy_df$age[1] = 41\n\n# We want to capitalise the letters in our gender variable; overwrite the 3rd column (option A)\nmy_df[,3] = c('M', 'F', 'NB', 'F', 'F', 'M')\n\n# overwrite the 3rd column (option B)\nmy_df$gender = c('M', 'F', 'NB', 'F', 'F', 'M')\n\n# Participant 6 wants to retract their data we need to overwrite the 6th row with NAs\nmy_df[6,] = NA\n\nHere is what the data frame looks like after we have redefined the values as per the code above:\n\n\n\n  ID age gender employment\n1  1  41      M       TRUE\n2  2  51      F       TRUE\n3  3  32     NB       TRUE\n4  4  23      F       TRUE\n5  5  55      F       TRUE\n6 NA  NA   &lt;NA&gt;         NA\n\n\n\n\nNote that R has a slightly different way of showing that a value is NA (not available, i.e., missing) in the console whenever the object type is a character string. Rather than simply showing NA as it does when objects are numeric or Boolean, R will instead show  for character strings. This is purely cosmetic, that is, it has no consequences for the way we write code or how R evaluates it.\nIf we inspect the object using RStudio’s viewer, instead, all NA values will be shown as NA irrespective of their type.\n\n\n\nUsing numerical indexing to overwrite variable names\nRemember the names() function we used previously to assign new names to the variables in a data frame? This function allows us to obtain the current variable names of a data frame. If we call it and feed it the name of a data frame as its function argument, R will return of character vector containing the names of all column contained in the data frame.\nThe names() function also allows us to overwrite the existing variables names if we redefine the object returned by the function call as a character vector of equal length (i.e., one element per column of the data frame). Back then we noticed that using the names() function might become tedious if a data frame contained a lot of variables. However, by using numerical indexing, we can circumvent the need to explicitly rename each variable in a data frame.\nSince the names() function returns a character vector, we can use numerical indexing to pinpoint the variable names we are interested in. For example, we could ask R to tell us the name of the 2nd variable of a data frame or to overwrite the name of the 4th variable. Here is what the code would look like (we will be using the data frame we worked with above):\n\n# obtain the name of the data frame's 2nd variable\nnames(my_df)[2]\n\n# assign a new name to the 4th variable ('employed' instead of 'employment')\nnames(my_df)[4] = 'employed'\n\nRunning the code above prompts R to return the name of the data frame’s 2nd variable in the console:\n\n\n\n[1] \"age\"\n\n\n\nThis is what the data frame looks like after renaming the 4th column.\n\n\n\n  ID age gender employed\n1  1  41      M     TRUE\n2  2  51      F     TRUE\n3  3  32     NB     TRUE\n4  4  23      F     TRUE\n5  5  55      F     TRUE\n6 NA  NA   &lt;NA&gt;       NA\n\n\n\n\nTechnically, we must place the brackets used for indexing directly after the closing parentheses of the function call. Placing the brackets inside the parentheses instead resulted in error messages in previous versions of R. However, in the more recent versions of R, placing the brackets behind the name of the data frame inside the parentheses of the function call will also work. in other words, the two following lines of code produce identical results:\n\n# obtain the names of the first three variables\nnames(my_df)[1:3]\n\n# alternative code that yields the same result\nnames(my_df[1:3])"
  },
  {
    "objectID": "working/working5.html",
    "href": "working/working5.html",
    "title": "R Packages",
    "section": "",
    "text": "Base R comes equipped with a range of useful functions. However, we might frequently find ourselves in situations in which base R does not provide solutions to our problems, or where the provided solutions are cumbersome. There is a solution to this problem: additional R packages."
  },
  {
    "objectID": "working/working5.html#what-are-r-packages",
    "href": "working/working5.html#what-are-r-packages",
    "title": "R Packages",
    "section": "What are R packages?",
    "text": "What are R packages?\nR packages are a collection of R functions and/or R objects (such as data sets) not included in base R. They have been developed and are being maintained by people (often other academics) who believe that their package allows R users to do something that base R cannot do or to make life easier for users compared to using base R alone.\nR is modular in the sense that we can add as many packages to base R as we want. Due to the large R community, chances are that if you look for something specific there is an R package for it.\nMost R packages are available from CRAN (Comprehensive R Archive Network). CRAN is a repository for R packages that ensures that the packages meet certain standards such as being properly documented, interfering as little as possible with other packages, being (largely) platform-independent and so on."
  },
  {
    "objectID": "working/working5.html#installing-r-packages",
    "href": "working/working5.html#installing-r-packages",
    "title": "R Packages",
    "section": "Installing R packages",
    "text": "Installing R packages\nIn order to use an R package, we first need to install it on our computer. Installing an R package means that we download it from CRAN. We do not need to do anything else because R will do the actual installing on its own. Once we have downloaded an R package, our instance of R knows that this package exists on our machine. We can then find functions from that package when searching for functions using the ?? operator.\nWhen using RStudio, we can install R packages in two ways. The first is calling the function install.packages() and feeding it the name of the R package we want to install as a character string (or as a character vector if we want to install multiple packages). Here is what the syntax looks like:\n\n# install an R package\ninstall.packages(\"packageName\")\n\nAlternatively, RStudio allows us to install packages using the Packages tab in the Utility & Help (bottom right) section of the interface. When we go to this tab, R will show us all the packages we have currently installed (see below). If we do not yet have any additional packages, we will only see packages that are part of base R such as base, datasets, and methods.\n\nKlicking on the Install button will open a menu that allows us to select the package we are interested in for installation (see below).\n\nThe first field is called “Install from”. The default for the installation of packages is to install them from CRAN (this is sensible, we are rarely - if at all - going to install packages not contained in CRAN).\nIn the next field, called “Packages”, we can enter the name of the package we want to install. In case we want to install multiple packages, we can separate their names with space or comma. Note that RStudio has an auto-complete feature, here. That is, if we enter one or more letters or numbers, RStudio will list all packages whose names start with (or consist of) what we have entered so far.\nUnder “Install to Library”, RStudio shows the folder, in which our R packages will be saved (it depends solely on where you installed R on your computer). Once, we hit the “Install” button on the bottom of the menu, R will install the selected packages.\n\nOn the bottom of the menu, there is a check-box asking whether RStudio should install dependencies. This box ix checked by default, and we should leave it that way. The reason is that some R packages depend on other R packages to run properly because they use some of the functions those other packages provide. Those other packages might themselves require other packages to run properly.\nInstalling dependencies, thus, is necessary to ensure that we can actually use the functions that an R package offers us.\n\nLet’s now look at an example, in which we are interested in installing the R package psych because it contains some functions that are particularly interesting to psychologists. We can either install it using R code, which would look like this:\n\n# install the R package psych\ninstall.packages('psych')\n\nAlternatively, we can install the package psych by going to the Packages tab of RStudio’s Utility & Help section, and entering its name in the Install menu (see below).\n\nIrrespective of how we tell R to install the psych package, R will now download the package and unpack it in the folder where R keeps all the packages. We can verify that R has installed the new package by inspecting the output in the console. There, R will provide some information, including the statement that the package we wanted to install has been ‘successfully unpacked’. In our example, it looks as follows:\n\n\nInstalling R packages is something we need to do only once (the exception is when there is a major update of R - then we need to reinstall all packages).\nTherefore, it is not a good idea to include the install.packages() function in an R script. If we do, then R will - without need - reinstall packages whenever we run the script.\nThere may be some cases, in which we may want to include the installation of packages in our R script, for example, when we publish our script as part of Open Data and want to make sure that our code is 100% reproducible.\nA common and sensible practice in such cases is to use commenting to include the R code for installing the packages but have R ignore it while running the script. Interested parties who want to run our script, can then simply uncomment the respective lines of code to install the packages."
  },
  {
    "objectID": "working/working5.html#loading-installed-libraries",
    "href": "working/working5.html#loading-installed-libraries",
    "title": "R Packages",
    "section": "Loading installed libraries",
    "text": "Loading installed libraries\nIn order to actually use the objects and/or functions included in a package, installing it is not sufficient. We also need to load the package. Once we load an R package, we can access its content until we close RStudio. This also means that we need to load R packages again after closing and restarting RStudio.\nWe can load R packages using the function library() which takes the name of an installed R package as its function argument either as is or as a character string (calling the package name as is might be preferable because of RStudio’s code completion feature).\nFor example, if we wanted to us the psych package we installed in the example above, we would need to load it using the library() function. The syntax looks as follows:\n\n# load the library psych\nlibrary(psych)\n\n# alternative code that does the same\nlibrary('psych')\n\nRunning either line of code will load the R package. If the package we are interested in has dependencies, loading it will also prompt R to load all other packages necessary for the focal package to run properly. One the package (and potentially its dependencies) are loaded, we can start using its contained functions and/or objects."
  },
  {
    "objectID": "working/working5.html#updating-libraries",
    "href": "working/working5.html#updating-libraries",
    "title": "R Packages",
    "section": "Updating libraries",
    "text": "Updating libraries\nJust as R and RStudio, (most) R packages are regularly updated. Using an outdated version of an R package is usually not a big problem. However, in some cases, it may lead to our code not running properly. This can happen whenever a function depends on functions from base R or another R package that have been modified such they have a different name or that their output format has changed.\nIn some instances, R will let us know that our packages are outdated, namely if a package was built in an older version of R than the one we are currently running on our machine. If we load a library that was built using a previous R version, we will receive a warning in the console upon loading it with the library() function.\n\nJust as error messages, warnings are displayed as red font in the console. However, while error messages tell us that the code we entered did not work, the code does run properly when we receive a warning. Warnings are a way for R to tell us that we may potentially - but not necessarily - run into problems.\n\nRStudio has a quality-of-life feature that allows us to check whether there are updates available for the R packages we have currently installed. In the Packages tab of the Utility & Help section, there is a button called Update (see below).\n\nKlicking on the Update button will open a list of packages for which a more recent version is available. We can then select those packages we would like to update or tell RStudio to update all of them. once we have selected at least one package that requires updating, we can hit the button labelled Install Updates.\nWhat R will do is simply reinstall the package’s current version and overwrite the old version on our computer. In case all of our packages are up to date, RStudio will show us a message stating so."
  },
  {
    "objectID": "working/working5.html#conflicts-between-packages",
    "href": "working/working5.html#conflicts-between-packages",
    "title": "R Packages",
    "section": "Conflicts between packages",
    "text": "Conflicts between packages\nA final thing to consider when using R packages is that sometimes there can be conflicts between them, namely when they contain objects with identical names.\nFor example, both the ggplot2 package, which many people use for plotting in R has a function called alpha() (regulates colour transparency in plots). So does the psych package (computes Cronbach’s \\(\\alpha\\)). If we load packages with such name conflicts, one of them will dominate the other.\nWhen loading a package that contains objects and/or functions with the same name as functions and/or objects of packages already loaded, R will ‘mask’ similarly named objects from already loaded packages. If a function or object is masked, we will not be able to call it in the usual manner because R will instead call the function or object of similar name in the most recently loaded package.\nFor example, if we first load the package psych, and then load ggplot2, R will tell us that the function alpha() has been masked from psych. If we call alpha() now, R will refer to the ggplot2 version of alpha() and assume that we want to manipulate the transparency of a colour. Has we first loaded ggplot2 and then loaded psych, R would instead tell us that alpha() has been masked from ggplot2, and calling the function would allow us to compute Cronbach’s \\(\\alpha\\).\nThe question now is: what if I need both functions or objects sharing the same name? The answer is that there we can specify which package we are calling a function from. This is called namespacing. To do so, we need to precede the function name with the package name and separate the two using double colons. The function of the double colon is not unlike that of the $ operator. It tells R where to look for the function we are interested in. The syntax looks as follows:\n\n# call function from package by namespacing it\npackageName::functionName(arguments)\n\nFor example, when loading both the ggplot2 and psych packages, we can call both functions using the syntax above. Doing so will ensure that our code will run free of bugs no matter in which order we load the packages.\n\n# call function alpha from ggplot2 and set the\n# transparency of the colour 'blue' to 50% (0.5)\nggplot2::alpha('blue', alpha = 0.5)\n\n# call the function alpha from psych and\n# compute Cronbach's alpha for a numeric \n# matrix containing responses to a scale\npsych::alpha(x = data_matrix)\n\n\nFun fact: The notation with double colons also allows us to call functions from packages that are installed on our machine but not currently loaded."
  },
  {
    "objectID": "wrangling/wrangling0.html",
    "href": "wrangling/wrangling0.html",
    "title": "Preface - tidy data",
    "section": "",
    "text": "Data wrangling is a term we use to describe all the work we undertake to get raw data into a format we can use to analyse the data and, ideally, to share the data with others in the spirit of open science. It includes reading the raw data into R, tidying up the data, and computing the variables you need for your analyses. Reading the data into R is often (but not always) the easiest part, and we have already covered how to read the most common file formats (.csv, .sav) into R. That means, we will now focus on tidying up data and transforming it.\nWhat exactly do we mean when we say “tidying up” data? Effectively, this means two things: extracting the information we need from the raw data, which means dropping all information we do not need, and putting the data into a tidy format. We consider data tidy if one row of the data frame corresponds to one observation with each column representing one variable. What constitutes a variable is sometimes not completely unambiguous, namely when we have nested data (e.g., multiple trials per participants, multiple measurements in a longitudinal study, multiple pupils per school class).\nFor example, we might have data from a reaction time experiment where participants worked on ten trials, and each of the ten reaction times is recorded as a variable of its own (e.g., rt_1, rt_2, etc.). Or we might have repeated measures on the same variable where each measurement is represented in a separate column, for example in a clinical randomised control trial study with pre-treatment, post-treatment, and follow-up measurement of the variable of interest (e.g., wellbeing_T1, wellbeing_T2, wellbeing_T3). In those examples, each row of the data set represents one unit of observation such as participants, or patients. If data is organised in this fashion, we call this the wide format.\nFor the purpose of tidy data, we consider the wide format not to be tidy because it spreads out one variable (reaction time or well-being) across multiple columns. Instead, we will consider data tidy if it is in long format where each row corresponds to one observation, even if this means having multiple rows per participant. In long format, we would have one column containing the variable (rt, wellbeing) and a second column coding when it was measured. In case of the reaction time experiment, this variable could be called trial, whereas in the clinical trial it could be called time. The reason why we want data in the long format is that most statistical analyses we can run on nested data will require the long format. While we usually require the data to be in long format, raw data is often available in wide format. Therefore, one part of data wrangling is getting data from wide format into long format.\nTidying up data also includes steps such as removing variables we don’t need, for example timestamps that a survey software created automatically, or dropping cases we need to remove from the data set such as test runs, incomplete data sets, or data from participants who want to retract their data.\nAfter we have tidied up our raw data, we may still need to transform the original data into variables we can work with. Generally speaking, data transformation includes all operations we perform on the original data, irrespective of whether we use binary operators or functions. Typical transformations are log-transformation of reaction times, computing scale means from multiple items of a questionnaire, turning a character variable into a factor, or computing the difference between two variables.\nOnce we have a tidy data set with all the variables we need (and after potentially removing some variables we no longer need after data transformation), we can save the data frame, ideally in a widely accessible format such as csv. In other words, the end product of our data wrangling is the data file we will load into the R script for our analyses, and which we may publish in order to satisfy the need for open data.\nIn this segment, we will learn data wrangling within the tidyverse, a set of R packages designed for data wrangling and visualisation. We have already worked with one of the included packages, namely ggplot2. Here, we will add two more packages to our toolbox, dplyr and tidyr."
  }
]