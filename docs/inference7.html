<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.258">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>QUB-PsyR - Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./images/QUBlogoWsmall.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">QUB-PsyR</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-intro" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Intro</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-intro">    
        <li>
    <a class="dropdown-item" href="./intro1.html">
 <span class="dropdown-text">R and RStudio</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./intro2.html">
 <span class="dropdown-text">Running Code in R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./intro3.html">
 <span class="dropdown-text">Objects and Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./intro4.html">
 <span class="dropdown-text">Data Frames and Lists</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./intro5.html">
 <span class="dropdown-text">Saving and Loading Data</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-working-with-r-objects" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Working with R Objects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-working-with-r-objects">    
        <li>
    <a class="dropdown-item" href="./working1.html">
 <span class="dropdown-text">Binary Operators</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./working2.html">
 <span class="dropdown-text">Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./working3.html">
 <span class="dropdown-text">Numerical Indexing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./working4.html">
 <span class="dropdown-text">Logical Indexing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./working5.html">
 <span class="dropdown-text">R Packages</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-statistical-inference" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Statistical Inference</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-statistical-inference">    
        <li>
    <a class="dropdown-item" href="./inference0.html">
 <span class="dropdown-text">A Primer on Statistical Inference</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference1.html">
 <span class="dropdown-text">The t-test</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference2.html">
 <span class="dropdown-text">The Chi²-test</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference3.html">
 <span class="dropdown-text">Correlations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference4.html">
 <span class="dropdown-text">One-factorial Analysis of Variance</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference5.html">
 <span class="dropdown-text">Two-factorial Analysis of Variance</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference6.html">
 <span class="dropdown-text">Repeated measures and mixed ANOVAs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference7.html">
 <span class="dropdown-text">Linear regression</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Linear Regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In linear regression, we “predict” one variable <span class="math inline">\(Y\)</span> (the <strong>criterion</strong>) from one or more other variables <span class="math inline">\(X\)</span> (the <strong>predictors</strong>), given that data on both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is available in the same sample (think of regression as a within-subjects approach to inferential statistics). If we say predict in the context of regression, we mean this in a strictly statistical sense, meaning that the regression analysis itself does not allow any statements about the potential causal patterns in the data.</p>
<p>Prediction simply means that we use information from the <span class="math inline">\(X\)</span> variables to make the best possible guess about the corresponding values of <span class="math inline">\(Y\)</span>. The best possible guess, in this case, is the one that minimizes the prediction error across the whole range of observations. That is, we may still be quite far off for individual values of <span class="math inline">\(Y\)</span> when using this best guess, but in the long run (or across the board) it will be our best option.</p>
<p>Linear regression is conceptually close to correlation analysis as it assumes a strictly linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In fact, in cases with only one predictor variable, correlation and linear regression convey the exact same information, albeit being presented in a slightly different manner.</p>
<section id="the-basics-of-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="the-basics-of-linear-regression">The basics of linear regression</h2>
<p>The core idea of linear regression is that we can express the criterion variable <span class="math inline">\(Y\)</span> as a linear function of the predictor variables <span class="math inline">\(X\)</span>. Let us first consider the simple case with one predictor. The general equation is:</p>
<p><span class="math display">\[Y = b_0 + b_1\times X + \epsilon\]</span></p>
<p>Here, <span class="math inline">\(\b_0\)</span> is some constant denoting the intercept of the linear function, and <span class="math inline">\(b_1\)</span> is its slope. We call <span class="math inline">\(b_1\)</span> a regression weight. It indicates how well we can predict <span class="math inline">\(Y\)</span> from <span class="math inline">\(X\)</span> (think of <span class="math inline">\(b_1\)</span> as a measure of the covariation of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>). Finally, <span class="math inline">\(\epsilon\)</span> is the prediction error, also called the <strong>residual</strong>. We assume that:</p>
<p><span class="math display">\[\epsilon \sim N(0, \sigma^2)\]</span></p>
<p>That is, the residual is assumed to have a mean of zero and some nonzero variance. We consider <span class="math inline">\(\epsilon\)</span> to be unsystematic, which means that it is not part of our actual prediction.</p>
<div class="alert alert-info">
<p>Treating <span class="math inline">\(\epsilon\)</span> as unsystematic is an oversimplification in most regression models. The reason is that the residual term contains not only the true unsystematic measurement error but also systematic variability of <span class="math inline">\(Y\)</span> that is not captured in our <span class="math inline">\(X\)</span> variables. In other words, <span class="math inline">\(\epsilon\)</span> contains both unsystematic and unexplained variability of <span class="math inline">\(Y\)</span>.</p>
</div>
<p>Let’s call the prediction of <span class="math inline">\(Y\)</span> from our predictor variable <span class="math inline">\(X\)</span> by a different name, namely <span class="math inline">\(\hat{Y}\)</span>. Since <span class="math inline">\(\epsilon\)</span> is not part of this prediction, we can state:</p>
<p><span class="math display">\[\hat{Y} = b_0 + b_1\times X\]</span> This also means that:</p>
<p><span class="math display">\[\epsilon = Y-\hat{Y} \]</span></p>
<p>The trick to linear regression is to choose the parameters of the regression model, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, so that they minimise the residual <span class="math inline">\(\epsilon\)</span>. We do that using the <strong>method of least squares</strong>, which is why linear regression is sometimes referred to as <strong>ordinary least squares (OLS) regression</strong>. To understand the method of least squares, it help to move from the variable-level formulation of the regression model to the level of observations.</p>
<p><span class="math display">\[y_i = b_0 + b_1 \times x_i + \epsilon_i\]</span></p>
<p>What this means is that our prediction of the magnitude of <span class="math inline">\(Y\)</span> for the <span class="math inline">\(i\)</span>th observation is based on the corresponding value of <span class="math inline">\(X\)</span>, that is, <span class="math inline">\(x_i\)</span> and our prediction error for that specific observation <span class="math inline">\(\epsilon_i\)</span>. The method of least squares states that the best combination of parameters <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> is the one that minimises the sum of the squared residuals <span class="math inline">\(\epsilon_i\)</span>.</p>
<p>It can be shown that the residual is minimised for:</p>
<p><span class="math display">\[b_1 = \frac{COV(X,Y)}{\sigma_x^2} = r_{XY} \times \frac{\sigma_Y}{\sigma_X}\]</span> And:</p>
<p><span class="math display">\[b_0 = \bar{Y} - b_1 \times \bar{X}\]</span></p>
<p>As we can see from the choice of the optimal <span class="math inline">\(b_1\)</span>, it is a linear transformation of the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, which means that we can also express it as a linear transformation of their correlation. Since linear transformations only change the units of measurement while leaving the strength of the linear relationship untouched, we can see that the linear regression with a single predictor variable <span class="math inline">\(X\)</span> contains the exact same information as the correlation, but it is expressed differently.</p>
<p>Specifically, the expression we use in linear regression allows us to predict specific values <span class="math inline">\(\hat{y}_i\)</span> whereas the correlation tells us how many standard deviations <span class="math inline">\(Y\)</span> increases if <span class="math inline">\(X\)</span> is increased by one standard deviation. With that in mind, we can now state that we would expect <span class="math inline">\(\hat{Y}\)</span> to take the value <span class="math inline">\(b_0\)</span> if <span class="math inline">\(X\)</span> were zero. We can also say that <span class="math inline">\(\hat{Y}\)</span> increases by <span class="math inline">\(b_1\)</span> units if we increase <span class="math inline">\(X\)</span> by one unit.</p>
<section id="testing-regression-coefficients-for-significance" class="level3">
<h3 class="anchored" data-anchor-id="testing-regression-coefficients-for-significance">Testing regression coefficients for significance</h3>
<p>We can test the fixed parameters of a regression model (i.e., the intercept and the regression weights of the predictor variables <span class="math inline">\(X\)</span>) for statistical significance using <span class="math inline">\(t\)</span>-statistics. For each of the parameters the following is true</p>
<p><span class="math display">\[\frac{b_i - \beta_i}{SE_{b_i}} \sim t_{N-k}\]</span></p>
<p>Here, <span class="math inline">\(b_i\)</span> is the parameter of interest, <span class="math inline">\(\beta_i\)</span> is the expected value under <span class="math inline">\(H_0\)</span> (typically zero, but we can can technically test against non-zero values if we wanted to), and <span class="math inline">\(SE_{b_i}\)</span> is the standard error of that parameter. The degrees of freedom of the resulting <span class="math inline">\(t\)</span>-distribution equal the sample size <span class="math inline">\(N\)</span> minus the number of the estimated parameters <span class="math inline">\(k\)</span>.</p>
<p>We are not going to go into the formula for the standard error of the regression coefficients here because they are a) somewhat complicated and b) R computes them for us. Instead, we will turn to one more important statistic used in regression models, namely the coefficient of determination <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[R^2 = \frac{S_{\hat{Y}}^2}{S_{Y}^2}\]</span></p>
<p>Here, <span class="math inline">\(S_{\hat{Y}}^2\)</span> is our estimate of the variance of the model predictions <span class="math inline">\(\hat{Y}\)</span>, whereas <span class="math inline">\(S_{Y}^2\)</span> is our estimate of the variance of the criterion variable <span class="math inline">\(Y\)</span>. In the case with only one predictor, the coefficient of determination is the squared correlation coefficient between predictor <span class="math inline">\(X\)</span> and criterion <span class="math inline">\(Y\)</span>. In the case with multiple predictor variables we, therefore, also refer to <span class="math inline">\(R\)</span> the <strong>multiple correlation coefficient</strong>. In any case, <span class="math inline">\(R^2\)</span> tells us which proportion of the variance of the criterion <span class="math inline">\(Y\)</span> we can account for with the predictions of our regression model <span class="math inline">\(\hat{Y}\)</span>.</p>
<p>We already know from the chapters on ANOVA that such ratios of variances can also be expressed as ratios of sums of squares. In the case of linear regression, it looks like this:</p>
<p><span class="math display">\[R^2 = \frac{S_{\hat{Y}}^2}{S_{Y}^2} = \frac{\frac{1}{N-k}\sum_{i=1}^{N}(\hat{y}_i - \bar{\hat{y}}_i)^2}{\frac{1}{N-k}\sum_{i=1}^{N}(y_i - \bar{y}_i)^2}\]</span></p>
<p>We can simplify this equation by dropping the <span class="math inline">\(\frac{1}{N-k}\)</span> which results in:</p>
<p><span class="math display">\[R^2 = \frac{\sum_{i=1}^{N}(\hat{y}_i - \bar{\hat{y}}_i)^2}{\sum_{i=1}^{N}(y_i - \bar{y}_i)^2} = \frac{SS_{regression}}{SS_{total}}\]</span> Now that we know that we are dealing with sums of squares again, we can easily compute the residual sum of squares (or error sum of squares):</p>
<p><span class="math display">\[SS_{residual} = SS_{total} - SS{regression}\]</span> :::{.alert .alert-success} If at this point you are wondering why we are back to sums of squares even though we are not dealing with ANOVAs anymore: regression and ANOVA are the same.</p>
<p>Or, to be precise: they are both special cases of the general linear model. We use ANOVAs when our predictors are categorical variables, but we could easily set up the same model as a regression model. The output of the respective analysis and the specific parameter tests we run may differ, but the underlying information is the same. :::</p>
<p>Back to <span class="math inline">\(R^2\)</span>: we can test for significance of <span class="math inline">\(R^2\)</span> using an <span class="math inline">\(F\)</span>-test based o the mean squares not unlike the ones we used in ANOVAs. It can be shown that:</p>
<p><span class="math display">\[\frac{MS_{regression}}{MS_{error}} =  \frac{\frac{SS_{regression}}{k-1}}{\frac{SS_{error}}{N-k}} \sim F_{k-1; N-k}\]</span></p>
<p>This <span class="math inline">\(F\)</span>-test tells us whether our regression model as a whole - that is, irrespective of the statistical significance of its individual regression weights - allows for an above-chance prediction of the criterion variable.</p>
<p>Adding more predictor variables to a regression model will necessarily increase the proportion of variance explained by the model <span class="math inline">\(R^2\)</span> even if the new parameter is not statistically significant individually. To counter this problem, we can adjust <span class="math inline">\(R2\)</span> and essentially “punish” a model for containing too many weak predictors. Here is the formula for the adjusted <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[R_{adjusted}^2 = 1-\frac{\frac{SS_{error}}{df_{error}}}{\frac{SS_{total}}{df_{total}}} = \frac{MS_{error}}{MS_{total}}\]</span></p>
</section>
<section id="running-a-regression-analysis-in-r" class="level3">
<h3 class="anchored" data-anchor-id="running-a-regression-analysis-in-r">Running a regression analysis in R</h3>
<p>We can run regression analyses in R using the <em>lm</em> function (linear model). The function has several arguments. We will use only the following two:</p>
<ul>
<li><em>formula</em> (required): a formula type object telling the function which variable to predict and which combination of variables to use as predictors</li>
<li><em>data</em> (optional): a data frame containing the variables we feed into the formula; if we do not specify this argument, R will assume that the variables we fed into the formula argument exist as objects in our environment</li>
</ul>
</section>
<section id="excurse-formula-type-objects" class="level3">
<h3 class="anchored" data-anchor-id="excurse-formula-type-objects">Excurse: formula type objects</h3>
<p>Before we delve into examples of linear regressions, we need to have a look at the syntax of <strong>formula</strong> type R objects. We already know that these objects use the <strong>~</strong> operator (tilde) and have the general form:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># general syntax of a formula</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>That means, we define <span class="math inline">\(y\)</span> as a function of <span class="math inline">\(x\)</span>. Of course, a formula can be much more complex than that because we can use combinations of multiple variables on the right hand side. In regression models, the formula is generally an additive combination of predictors, each of which then receives its own regression weight. We can combine multiple predictors using the <strong>+</strong> operator.</p>
<p>The first thing we need to know now is that a regression formula in R typically omits the <strong>intercept</strong> although the <em>lm</em> function models it. This is purely a convenience feature. The actual formula looks like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># formula explicating that the model contains an intercept</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, the <span class="math inline">\(1\)</span> represents the intercept. If we omit it, R will read the function as if we had written it, meaning that the function will estimate the intercept <span class="math inline">\(b_0\)</span> both when we add the <span class="math inline">\(1\)</span> in our formula and when we leave it out.</p>
<div class="alert alert-success">
<p>In some situations, we might want to force a regression through the origin. In other words, we want the intercept of the model to be zero. In those cases, we can replace the <span class="math inline">\(1\)</span> in our regression model with a <span class="math inline">\(0\)</span>, so R will know that the intercept must be 0 in the model.</p>
</div>
<p>We can now make the formula even more complex by adding more predictors (see example below).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># formula treating y as an additive function of x1 and x2</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x1 <span class="sc">+</span> x2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the example above, we have two additive effects (think of them as main effects) of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> on <span class="math inline">\(y\)</span>. But what if we wanted to add an interaction as well? In this case, we can combine the two predictors using the <strong>:</strong> operator. If we use this operator, R will compute the interaction term as the product of the two variables and use it as a predictor in the model. Here is what the syntax would look like:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># formula treating y as a function of x1 and x2</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># as well as their interaction</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x1<span class="sc">:</span>x2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the above example, we have the <strong>full model</strong>, that is, it contains all possible effects that a regression model with two predictors can have. However, it is also possible to feed a reduced model into the <em>lm</em> function. For example, we might want to estimate a regression model that entails only the main effect of <span class="math inline">\(x_1\)</span> and the interaction effect, but not the main effect of <span class="math inline">\(x_2\)</span>. In this case, the model would look like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># formula treating y as an additive function of x1 </span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and the interaction of x1 and x2</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x1 <span class="sc">+</span> x1<span class="sc">:</span>x2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>R has a neat way of making regression model formulae more parsimonious, namely using the <strong>*</strong> operator. If we combine tow or more variables with this operator, R will include all main effects and interactions of these variables. Here are some examples:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-overflow-wrap code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">## formula for a full regression model with two predictors</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># this:</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x1<span class="sc">*</span>x2</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># reads as this:</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x1<span class="sc">:</span>x2</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="do">## formlua for a full regression model with three predictors</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># this:</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x1<span class="sc">*</span>x2<span class="sc">*</span>x3</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># reads as this:</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3 <span class="sc">+</span> x1<span class="sc">:</span>x2 <span class="sc">+</span> x1<span class="sc">:</span>x3 <span class="sc">+</span> x2<span class="sc">:</span>x3 <span class="sc">+</span> x1<span class="sc">:</span>x2<span class="sc">:</span>x3</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="do">## formula for a three-predictor models with the </span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="do">## third predictor as a purely additive effect</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># this:</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x1<span class="sc">*</span>x2 <span class="sc">+</span> x3</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># reads as this:</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x1<span class="sc">:</span>x2 <span class="sc">+</span> x3</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As we can see, the <strong>*</strong> operator can make our lives a bit easier whenever models involve multiple predictors. A final thing we need to know about formulae is that we can also perform arithmetic operations other than multiplication to create predictors from the variables in our data frame. One example would be polynomial regression, in which we predict <span class="math inline">\(y\)</span> not only from <span class="math inline">\(x\)</span> but also from exponentiated versions of <span class="math inline">\(x\)</span>. Another example are difference scores as predictors (note that using difference scores as predictors in regression models is not trivial, so great care is needed when making sense of the results).</p>
<p>It is important to note that all arithmetic operations save multiplication require a special syntax in order for R to understand what we are trying to do. Specifically, we need to call a function named <em>I</em> (capital I) within the formula and feed it the mathematical expression we want to use as a predictor in the model as its sole argument. Let’s look at an example for a polynomial regression of the 4th order:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-overflow-wrap code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># formula for a model predicting y from x in </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># a 4th order polynomial regression</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x <span class="sc">+</span> <span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(x<span class="sc">^</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">I</span>(x<span class="sc">^</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And here is what the formula would look like in a model where we want to predict <span class="math inline">\(y\)</span> from the difference between two variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-overflow-wrap code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># formula for a model predicting y from the difference </span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># between x1 and x2</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> <span class="fu">I</span>(x1 <span class="sc">-</span> x2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="back-to-regressions-in-r" class="level3">
<h3 class="anchored" data-anchor-id="back-to-regressions-in-r">Back to regressions in R</h3>
<p>Now that we have a rough understanding of formula type objects in R, we can go back to running linear regressions in R. Let’s first create some data for the simple case with one predictor variable. Let’s assume we gathered data from 40 people on two variables, love of cats and love of dogs. Both variables are measured on scales ranging from -3 (can’t stand them) to +3 (love them). We will use a linear regression to predict participant’s attitude toward cats from their attitude toward dogs.</p>
<div class="alert alert-danger">
<p>This data example is very prototypical for psychology in the sense that it violates two of the assumptions of linear regression, namely that the criterion <span class="math inline">\(y\)</span> is continuous and measured on an interval scale (meaning that the increase from, say, 1 to 2 is equal to the increase from, say, 4 to 5).</p>
<p>Rating scales, as they are often used in psychological research, do not satisfy these conditions. Neither is the scale continuous (in fact,it is restricted to a few discrete values), nor can we say for certain whether the steps between scale points mark equal psychological distances.</p>
<p>In practice, we generally dismiss this violating of assumptions and pretend that it is not an issue. For those who find that unsatisfying, it may be worthwhile to use a different type of regression model that is better suited to this kind of data, namely ordinal regression.</p>
</div>
<p>Let’s first have a brief look at the fictional data (stored in a data frame called <em>df1</em>):</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  ID love_dogs love_cats
1  1        -2         1
2  2         1        -2
3  3         2         0
4  4        -3        -3
5  5         1        -1
6  6         1        -1</code></pre>
</div>
</div>
<p>We can now call the <em>lm</em> function to run an ordinary least squares regression of participants’ love for cats from their love for dogs. Similar to ANOVAs, we will define the regression model as an object. Here is what the syntax might look like:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-overflow-wrap code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># regression of participants' love for cats on their love for dogs</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">=</span> <span class="fu">lm</span>(<span class="at">formula =</span> love_cats <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> love_dogs,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> df1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we run that code, an object called “model1” will appear in our environment. R will inform us that this object is a <strong>list of 12</strong>. We can now call the object’s name and have a look at what R outputs in the console:</p>
<div class="alert alert-warning">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = love_cats ~ 1 + love_dogs, data = df1)

Coefficients:
(Intercept)    love_dogs  
    -0.4223       0.1245  </code></pre>
</div>
</div>
</div>
<p>As we can see, the output is relatively sparse. We can see the function call we entered as well as estimates of the two regression parameters <span class="math inline">\(b_0\)</span> (intercept) and <span class="math inline">\(b_1\)</span> (the regression weight for love of dogs).</p>
<p>While we now know the parameter values of the linear regression, this information is slightly underwhelming. If we want more information, particularly about the statistical significance of the parameters, we need to feed the regression model into another function called <em>summary</em>. The <em>summary</em> function takes a fitted model as its sole argument. Here is the syntax:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-overflow-wrap code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># display detailed output for the regression model</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is hat the console output looks like when we call the <em>summary</em> function on our regression model:</p>
<div class="alert alert-warning">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = love_cats ~ 1 + love_dogs, data = df1)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.4532 -1.3287 -0.4532  1.4223  3.5468 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -0.4223     0.3007  -1.404    0.168
love_dogs     0.1245     0.1729   0.720    0.476

Residual standard error: 1.674 on 38 degrees of freedom
Multiple R-squared:  0.01346,   Adjusted R-squared:  -0.0125 
F-statistic: 0.5185 on 1 and 38 DF,  p-value: 0.4759</code></pre>
</div>
</div>
</div>
<p>As we can see, R now displays substantially more information. In addition to the function call, we can now also see information on the distribution of the residuals (quartiles). For each of the fixed parameters of the model, we can now see the estimate, its standard error, the resulting <span class="math inline">\(t\)</span>-value, and the <span class="math inline">\(p\)</span>-value of a two-tailed <span class="math inline">\(t\)</span>-test of the parameter against zero. Below the parameter estimates, there is some more information on the model, the most important of which are the <span class="math inline">\(R^2\)</span> and adjusted <span class="math inline">\(R^2\)</span> values as well as the results of the <span class="math inline">\(F\)</span>-test testing whether <span class="math inline">\(R^2\)</span> is greater than zero.</p>
<p>In our example, love of dogs is not a significant predictor of participant’s love for cats. The model as a whole also does not explain a significant proportion of the variance in participants’ love for cats indicated by the non-significant <span class="math inline">\(F\)</span>-test. This is not surprising because the model only contains one predictor, love for dogs, and this predictor is not significantly related to the criterion. As we can see, the <span class="math inline">\(p\)</span>-value for the <span class="math inline">\(t\)</span>-test of the single predictor (love for dogs) is exactly equal to the <span class="math inline">\(F\)</span>-test of the whole model since love for dogs is the only variable that can possibly explain variability in the love for cats in our data set.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>