<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.258">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>QUB-PsyR - Simple Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./images/QUBlogoWsmall.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">QUB-PsyR</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-intro" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Intro</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-intro">    
        <li>
    <a class="dropdown-item" href="./intro1.html">
 <span class="dropdown-text">R and RStudio</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./intro2.html">
 <span class="dropdown-text">Running Code in R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./intro3.html">
 <span class="dropdown-text">Objects and Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./intro4.html">
 <span class="dropdown-text">Data Frames and Lists</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./intro5.html">
 <span class="dropdown-text">Saving and Loading Data</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-working-with-r-objects" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Working with R Objects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-working-with-r-objects">    
        <li>
    <a class="dropdown-item" href="./working1.html">
 <span class="dropdown-text">Binary Operators</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./working2.html">
 <span class="dropdown-text">Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./working3.html">
 <span class="dropdown-text">Numerical Indexing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./working4.html">
 <span class="dropdown-text">Logical Indexing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./working5.html">
 <span class="dropdown-text">R Packages</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-statistical-inference" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Statistical Inference</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-statistical-inference">    
        <li>
    <a class="dropdown-item" href="./inference0.html">
 <span class="dropdown-text">A Primer on Statistical Inference</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference1.html">
 <span class="dropdown-text">The t-test</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference2.html">
 <span class="dropdown-text">The Chi²-test</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference3.html">
 <span class="dropdown-text">Correlations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference4.html">
 <span class="dropdown-text">One-factorial Analysis of Variance</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference5.html">
 <span class="dropdown-text">Two-factorial Analysis of Variance</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference6.html">
 <span class="dropdown-text">Repeated measures and mixed ANOVAs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference7.html">
 <span class="dropdown-text">Simple linear regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./inference8.html">
 <span class="dropdown-text">Multiple linear regression</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Simple Linear Regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In linear regression, we “predict” one variable <span class="math inline">\(Y\)</span> (the <strong>criterion</strong>) from one or more other variables <span class="math inline">\(X\)</span> (the <strong>predictors</strong>), given that data on both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is available in the same sample (think of regression as a within-subjects approach to inferential statistics). If we say predict in the context of regression, we mean this in a strictly statistical sense, meaning that the regression analysis itself does not allow any statements about the potential causal patterns in the data.</p>
<p>Prediction simply means that we use information from the <span class="math inline">\(X\)</span> variables to make the best possible guess about the corresponding values of <span class="math inline">\(Y\)</span>. The best possible guess, in this case, is the one that minimizes the prediction error across the whole range of observations. That is, we may still be quite far off for individual values of <span class="math inline">\(Y\)</span> when using this best guess, but in the long run (or across the board) it will be our best option.</p>
<p>Linear regression is conceptually close to correlation analysis as it assumes a strictly linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In fact, in cases with only one predictor variable, correlation and linear regression convey the exact same information, albeit being presented in a slightly different manner.</p>
<section id="the-basics-of-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="the-basics-of-linear-regression">The basics of linear regression</h2>
<p>The core idea of linear regression is that we can express the criterion variable <span class="math inline">\(Y\)</span> as a linear function of the predictor variables <span class="math inline">\(X\)</span>. Let us first consider the simple case with one predictor. The general equation is:</p>
<p><span class="math display">\[Y = b_0 + b_1\times X + \epsilon\]</span></p>
<p>Here, <span class="math inline">\(\b_0\)</span> is some constant denoting the intercept of the linear function, and <span class="math inline">\(b_1\)</span> is its slope. We call <span class="math inline">\(b_1\)</span> a regression weight. It indicates how well we can predict <span class="math inline">\(Y\)</span> from <span class="math inline">\(X\)</span> (think of <span class="math inline">\(b_1\)</span> as a measure of the covariation of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>). Finally, <span class="math inline">\(\epsilon\)</span> is the prediction error, also called the <strong>residual</strong>. We assume that:</p>
<p><span class="math display">\[\epsilon \sim N(0, \sigma^2)\]</span></p>
<p>That is, the residual is assumed to have a mean of zero and some nonzero variance. We consider <span class="math inline">\(\epsilon\)</span> to be unsystematic, which means that it is not part of our actual prediction.</p>
<div class="alert alert-info">
<p>Treating <span class="math inline">\(\epsilon\)</span> as unsystematic is an oversimplification in most regression models. The reason is that the residual term contains not only the true unsystematic measurement error but also systematic variability of <span class="math inline">\(Y\)</span> that is not captured in our <span class="math inline">\(X\)</span> variables. In other words, <span class="math inline">\(\epsilon\)</span> contains both unsystematic and unexplained variability of <span class="math inline">\(Y\)</span>.</p>
</div>
<p>Let’s call the prediction of <span class="math inline">\(Y\)</span> from our predictor variable <span class="math inline">\(X\)</span> by a different name, namely <span class="math inline">\(\hat{Y}\)</span>. Since <span class="math inline">\(\epsilon\)</span> is not part of this prediction, we can state:</p>
<p><span class="math display">\[\hat{Y} = b_0 + b_1\times X\]</span> This also means that:</p>
<p><span class="math display">\[\epsilon = Y-\hat{Y} \]</span></p>
<p>The trick to linear regression is to choose the parameters of the regression model, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, so that they minimise the residual <span class="math inline">\(\epsilon\)</span>. We do that using the <strong>method of least squares</strong>, which is why linear regression is sometimes referred to as <strong>ordinary least squares (OLS) regression</strong>. To understand the method of least squares, it help to move from the variable-level formulation of the regression model to the level of observations.</p>
<p><span class="math display">\[y_i = b_0 + b_1 \times x_i + \epsilon_i\]</span></p>
<p>What this means is that our prediction of the magnitude of <span class="math inline">\(Y\)</span> for the <span class="math inline">\(i\)</span>th observation is based on the corresponding value of <span class="math inline">\(X\)</span>, that is, <span class="math inline">\(x_i\)</span> and our prediction error for that specific observation <span class="math inline">\(\epsilon_i\)</span>. The method of least squares states that the best combination of parameters <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> is the one that minimises the sum of the squared residuals <span class="math inline">\(\epsilon_i\)</span>.</p>
<p>It can be shown that the residual is minimised for:</p>
<p><span class="math display">\[b_1 = \frac{COV(X,Y)}{\sigma_x^2} = r_{XY} \times \frac{\sigma_Y}{\sigma_X}\]</span> And:</p>
<p><span class="math display">\[b_0 = \bar{Y} - b_1 \times \bar{X}\]</span></p>
<p>As we can see from the choice of the optimal <span class="math inline">\(b_1\)</span>, it is a linear transformation of the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, which means that we can also express it as a linear transformation of their correlation. Since linear transformations only change the units of measurement while leaving the strength of the linear relationship untouched, we can see that the linear regression with a single predictor variable <span class="math inline">\(X\)</span> contains the exact same information as the correlation, but it is expressed differently.</p>
<p>Specifically, the expression we use in linear regression allows us to predict specific values <span class="math inline">\(\hat{y}_i\)</span> whereas the correlation tells us how many standard deviations <span class="math inline">\(Y\)</span> increases if <span class="math inline">\(X\)</span> is increased by one standard deviation. With that in mind, we can now state that we would expect <span class="math inline">\(\hat{Y}\)</span> to take the value <span class="math inline">\(b_0\)</span> if <span class="math inline">\(X\)</span> were zero. We can also say that <span class="math inline">\(\hat{Y}\)</span> increases by <span class="math inline">\(b_1\)</span> units if we increase <span class="math inline">\(X\)</span> by one unit.</p>
<section id="testing-regression-coefficients-for-significance" class="level3">
<h3 class="anchored" data-anchor-id="testing-regression-coefficients-for-significance">Testing regression coefficients for significance</h3>
<p>We can test the fixed parameters of a regression model (i.e., the intercept and the regression weights of the predictor variables <span class="math inline">\(X\)</span>) for statistical significance using <span class="math inline">\(t\)</span>-statistics. For each of the parameters the following is true</p>
<p><span class="math display">\[\frac{b_i - \beta_i}{SE_{b_i}} \sim t_{N-k}\]</span></p>
<p>Here, <span class="math inline">\(b_i\)</span> is the parameter of interest, <span class="math inline">\(\beta_i\)</span> is the expected value under <span class="math inline">\(H_0\)</span> (typically zero, but we can can technically test against non-zero values if we wanted to), and <span class="math inline">\(SE_{b_i}\)</span> is the standard error of that parameter. The degrees of freedom of the resulting <span class="math inline">\(t\)</span>-distribution equal the sample size <span class="math inline">\(N\)</span> minus the number of the estimated parameters <span class="math inline">\(k\)</span>.</p>
<p>We are not going to go into the formula for the standard error of the regression coefficients here because they are a) somewhat complicated and b) R computes them for us. Instead, we will turn to one more important statistic used in regression models, namely the coefficient of determination <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[R^2 = \frac{S_{\hat{Y}}^2}{S_{Y}^2}\]</span></p>
<p>Here, <span class="math inline">\(S_{\hat{Y}}^2\)</span> is our estimate of the variance of the model predictions <span class="math inline">\(\hat{Y}\)</span>, whereas <span class="math inline">\(S_{Y}^2\)</span> is our estimate of the variance of the criterion variable <span class="math inline">\(Y\)</span>. In the case with only one predictor, the coefficient of determination is the squared correlation coefficient between predictor <span class="math inline">\(X\)</span> and criterion <span class="math inline">\(Y\)</span>. In the case with multiple predictor variables we, therefore, also refer to <span class="math inline">\(R\)</span> the <strong>multiple correlation coefficient</strong>. In any case, <span class="math inline">\(R^2\)</span> tells us which proportion of the variance of the criterion <span class="math inline">\(Y\)</span> we can account for with the predictions of our regression model <span class="math inline">\(\hat{Y}\)</span>.</p>
<p>We already know from the chapters on ANOVA that such ratios of variances can also be expressed as ratios of sums of squares. In the case of linear regression, it looks like this:</p>
<p><span class="math display">\[R^2 = \frac{S_{\hat{Y}}^2}{S_{Y}^2} = \frac{\frac{1}{N-k}\sum_{i=1}^{N}(\hat{y}_i - \bar{\hat{y}}_i)^2}{\frac{1}{N-k}\sum_{i=1}^{N}(y_i - \bar{y}_i)^2}\]</span></p>
<p>We can simplify this equation by dropping the <span class="math inline">\(\frac{1}{N-k}\)</span> which results in:</p>
<p><span class="math display">\[R^2 = \frac{\sum_{i=1}^{N}(\hat{y}_i - \bar{\hat{y}}_i)^2}{\sum_{i=1}^{N}(y_i - \bar{y}_i)^2} = \frac{SS_{regression}}{SS_{total}}\]</span></p>
<p>Now that we know that we are dealing with sums of squares again, we can easily compute the residual sum of squares (or error sum of squares):</p>
<p><span class="math display">\[SS_{residual} = SS_{total} - SS_{regression}\]</span></p>
<p>:::{.alert .alert-success} If at this point you are wondering why we are back to sums of squares even though we are not dealing with ANOVAs anymore: regression and ANOVA are the same. :::</p>
<p>Or, to be precise: they are both special cases of the general linear model. We use ANOVAs when our predictors are categorical variables, but we could easily set up the same model as a regression model. The output of the respective analysis and the specific parameter tests we run may differ, but the underlying information is the same. :::</p>
<p>Back to <span class="math inline">\(R^2\)</span>: we can test for significance of <span class="math inline">\(R^2\)</span> using an <span class="math inline">\(F\)</span>-test based o the mean squares not unlike the ones we used in ANOVAs. It can be shown that:</p>
<p><span class="math display">\[\frac{MS_{regression}}{MS_{error}} =  \frac{\frac{SS_{regression}}{k-1}}{\frac{SS_{error}}{N-k}} \sim F_{k-1; N-k}\]</span></p>
<p>This <span class="math inline">\(F\)</span>-test tells us whether our regression model as a whole - that is, irrespective of the statistical significance of its individual regression weights - allows for an above-chance prediction of the criterion variable.</p>
<p>Adding more predictor variables to a regression model will necessarily increase the proportion of variance explained by the model <span class="math inline">\(R^2\)</span> even if the new parameter is not statistically significant individually. To counter this problem, we can adjust <span class="math inline">\(R2\)</span> and essentially “punish” a model for containing too many weak predictors. Here is the formula for the adjusted <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[R_{adjusted}^2 = 1-\frac{\frac{SS_{error}}{df_{error}}}{\frac{SS_{total}}{df_{total}}} = 1- \frac{MS_{error}}{MS_{total}}\]</span></p>
</section>
<section id="running-a-regression-analysis-in-r" class="level3">
<h3 class="anchored" data-anchor-id="running-a-regression-analysis-in-r">Running a regression analysis in R</h3>
<p>We can run regression analyses in R using the <em>lm</em> function (linear model). The function has several arguments. We will use only the following two:</p>
<ul>
<li><em>formula</em> (required): a formula type object telling the function which variable to predict and which combination of variables to use as predictors</li>
<li><em>data</em> (optional): a data frame containing the variables we feed into the formula; if we do not specify this argument, R will assume that the variables we fed into the formula argument exist as objects in our environment</li>
</ul>
</section>
<section id="excurse-formula-type-objects" class="level3">
<h3 class="anchored" data-anchor-id="excurse-formula-type-objects">Excurse: formula type objects</h3>
<p>Before we delve into examples of linear regressions, we need to have a look at the syntax of <strong>formula</strong> type R objects. We already know that these objects use the <strong>~</strong> operator (tilde) and have the general form:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># general syntax of a formula</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>That means, we define <span class="math inline">\(y\)</span> as a function of <span class="math inline">\(x\)</span>. Of course, a formula can be much more complex than that because we can use combinations of multiple variables on the right hand side. In regression models, the formula is generally an additive combination of predictors, each of which then receives its own regression weight. We can combine multiple predictors using the <strong>+</strong> operator.</p>
<p>The first thing we need to know now is that a regression formula in R typically omits the <strong>intercept</strong> although the <em>lm</em> function models it. This is purely a convenience feature. The actual formula looks like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># formula explicating that the model contains an intercept</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, the <span class="math inline">\(1\)</span> represents the intercept. If we omit it, R will read the function as if we had written it, meaning that the function will estimate the intercept <span class="math inline">\(b_0\)</span> both when we add the <span class="math inline">\(1\)</span> in our formula and when we leave it out.</p>
<div class="alert alert-success">
<p>In some situations, we might want to force a regression through the origin. In other words, we want the intercept of the model to be zero. In those cases, we can replace the <span class="math inline">\(1\)</span> in our regression model with a <span class="math inline">\(0\)</span>, so R will know that the intercept must be 0 in the model.</p>
</div>
</section>
<section id="back-to-regressions-in-r" class="level3">
<h3 class="anchored" data-anchor-id="back-to-regressions-in-r">Back to regressions in R</h3>
<p>Now that we have a rough understanding of formula type objects in R, we can go back to running linear regressions in R. Let’s first create some data for the simple case with one predictor variable. Let’s assume we gathered data from 40 people on two variables, love of cats and love of dogs. Both variables are measured on scales ranging from -3 (can’t stand them) to +3 (love them). We will use a linear regression to predict participant’s attitude toward cats from their attitude toward dogs.</p>
<div class="alert alert-danger">
<p>This data example is very prototypical for psychology in the sense that it violates two of the assumptions of linear regression, namely that the criterion <span class="math inline">\(y\)</span> is continuous and measured on an interval scale (meaning that the increase from, say, 1 to 2 is equal to the increase from, say, 4 to 5).</p>
<p>Rating scales, as they are often used in psychological research, do not satisfy these conditions. Neither is the scale continuous (in fact,it is restricted to a few discrete values), nor can we say for certain whether the steps between scale points mark equal psychological distances.</p>
<p>In practice, we generally dismiss this violating of assumptions and pretend that it is not an issue. For those who find that unsatisfying, it may be worthwhile to use a different type of regression model that is better suited to this kind of data, namely ordinal regression.</p>
</div>
<p>Let’s first have a brief look at the fictional data (stored in a data frame called <em>df1</em>):</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  ID love_dogs love_cats
1  1        -2         1
2  2         1        -2
3  3         2         0
4  4        -3        -3
5  5         1        -1
6  6         1        -1</code></pre>
</div>
</div>
<p>We can now call the <em>lm</em> function to run an ordinary least squares regression of participants’ love for cats from their love for dogs. Similar to ANOVAs, we will define the regression model as an object. Here is what the syntax might look like:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-overflow-wrap code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># regression of participants' love for cats on their love for dogs</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">=</span> <span class="fu">lm</span>(<span class="at">formula =</span> love_cats <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> love_dogs,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> df1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we run that code, an object called “model1” will appear in our environment. R will inform us that this object is a <strong>list of 12</strong>. We can now call the object’s name and have a look at what R outputs in the console:</p>
<div class="alert alert-warning">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = love_cats ~ 1 + love_dogs, data = df1)

Coefficients:
(Intercept)    love_dogs  
    -0.4223       0.1245  </code></pre>
</div>
</div>
</div>
<p>As we can see, the output is relatively sparse. We can see the function call we entered as well as estimates of the two regression parameters <span class="math inline">\(b_0\)</span> (intercept) and <span class="math inline">\(b_1\)</span> (the regression weight for love of dogs).</p>
<p>While we now know the parameter values of the linear regression, this information is slightly underwhelming. If we want more information, particularly about the statistical significance of the parameters, we need to feed the regression model into another function called <em>summary</em>. The <em>summary</em> function takes a fitted model as its sole argument. Here is the syntax:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-overflow-wrap code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># display detailed output for the regression model</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is what the console output looks like when we call the <em>summary</em> function on our regression model:</p>
<div class="alert alert-warning">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = love_cats ~ 1 + love_dogs, data = df1)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.4532 -1.3287 -0.4532  1.4223  3.5468 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -0.4223     0.3007  -1.404    0.168
love_dogs     0.1245     0.1729   0.720    0.476

Residual standard error: 1.674 on 38 degrees of freedom
Multiple R-squared:  0.01346,   Adjusted R-squared:  -0.0125 
F-statistic: 0.5185 on 1 and 38 DF,  p-value: 0.4759</code></pre>
</div>
</div>
</div>
<p>As we can see, R now displays substantially more information. In addition to the function call, we can now also see information on the distribution of the residuals (quartiles). For each of the fixed parameters of the model, we can now see the estimate, its standard error, the resulting <span class="math inline">\(t\)</span>-value, and the <span class="math inline">\(p\)</span>-value of a two-tailed <span class="math inline">\(t\)</span>-test of the parameter against zero. Below the parameter estimates, there is some more information on the model, the most important of which are the <span class="math inline">\(R^2\)</span> and adjusted <span class="math inline">\(R^2\)</span> values as well as the results of the <span class="math inline">\(F\)</span>-test testing whether <span class="math inline">\(R^2\)</span> is greater than zero.</p>
<p>In our example, love of dogs is not a significant predictor of participant’s love for cats. The model as a whole also does not explain a significant proportion of the variance in participants’ love for cats indicated by the non-significant <span class="math inline">\(F\)</span>-test. This is not surprising because the model only contains one predictor, love for dogs, and this predictor is not significantly related to the criterion. As we can see, the <span class="math inline">\(p\)</span>-value for the <span class="math inline">\(t\)</span>-test of the single predictor (love for dogs) is exactly equal to the <span class="math inline">\(F\)</span>-test of the whole model since love for dogs is the only variable that can possibly explain variability in the love for cats in our data set.</p>
</section>
</section>
<section id="categorical-predcitors-in-simple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="categorical-predcitors-in-simple-linear-regression">Categorical predcitors in simple linear regression</h2>
<p>So far, we have considered the case in which both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are (sort of) continuous or variables. While <span class="math inline">\(Y\)</span> must be a continuous variable in linear regression, the same is not true for the predictor <span class="math inline">\(X\)</span>. We can also predict <span class="math inline">\(Y\)</span> from categorical variables with two or more levels. If we do so, we need to slightly change the way in which we interpret the results of the model output.</p>
<section id="dichotomous-predictors" class="level3">
<h3 class="anchored" data-anchor-id="dichotomous-predictors">Dichotomous predictors</h3>
<p>In the most simple case, a categorical predictor has two levels (e.g., treatment vs.&nbsp;control). If we want to use a dichotomous predictor in a linear regression, we need to assign values to its two levels. Which values we choose is technically irrelevant, but some are more sensible when it comes to interpreting the model. The two most common ways to code dichotomous predictors are:</p>
<ul>
<li>effect coding: the predictors levels are centred around 0 (e.g., -1 vs.&nbsp;1 or -0.5 vs.&nbsp;0.5)</li>
<li>dummy coding: the predictor is coded as a binary variable (0 vs.&nbsp;1)</li>
</ul>
<p>When running a regression using a categorical predictor in R, we need to consider the type of the predictor variable. If we use a character string or a factor as a binary predictor, R will automatically use dummy coding. If the predictor variable is numeric and different from the desired coding scheme, we need to recode it manually.</p>
<div class="alert alert-info">
<p><strong>Caveat</strong>: Unless we define a predictor as a factor with an ordered structure (by defining levels and labels in the desired order), R will order the levels alphabetically, with the first level being treated as the reference category.</p>
</div>
<p>Let’s now briefly look at an example of a simple linear regression with a dichotomous predictor. Here is some made up data (stored in <em>df2</em>), in which the predictor is either dummy-coded (because the variable is a factor with two levels) or effect coded (-0.5 vs.&nbsp;0.5):</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  ID cond_dummy cond_effect score
1  1    control        -0.5  5.59
2  2  treatment         0.5 12.20
3  3    control        -0.5  8.55
4  4  treatment         0.5  9.05
5  5    control        -0.5 10.17
6  6  treatment         0.5  8.58</code></pre>
</div>
</div>
<p>We will first look at the dummy-coded version of the predictor by using the labelled factor as the predictor (the variable named ‘cond_dummy’). Here is what the syntax looks like:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-overflow-wrap code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># regression of score on the condition variable</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">=</span> <span class="fu">lm</span>(<span class="at">formula =</span> score <span class="sc">~</span> cond_dummy, <span class="at">data =</span> df2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Running the code above yields the following output:</p>
<div class="alert alert-warning">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = score ~ cond_dummy, data = df2)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.0977 -1.1347 -0.4077  0.7375  5.4223 

Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)           7.4077     0.3401  21.780  &lt; 2e-16 ***
cond_dummytreatment   1.4893     0.4810   3.096  0.00302 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.863 on 58 degrees of freedom
Multiple R-squared:  0.1419,    Adjusted R-squared:  0.1271 
F-statistic: 9.588 on 1 and 58 DF,  p-value: 0.003016</code></pre>
</div>
</div>
</div>
<p>When we use dummy-coding, the intercept of the model represent the mean of the <strong>reference category</strong> (in our example the control group). This means, we can test whether the mean of the reference group differs from zero using the intercept of the model. The treatment effect tells us how much the mean value changes when we move from the control group to the treatment group, that is, it shows us the mean difference between the two groups and tells us whether this mean difference is significant.</p>
<div class="alert alert-sucess">
<p>Testing the significance of mean differences between two groups…doesn’t that sound familiar? That is what we previously used the independent samples <span class="math inline">\(t\)</span>-test for.</p>
<p>In fact, if we ran an independent samples <span class="math inline">\(t\)</span>-test on the data and forced it to assume equal variances, it would show the exact same <span class="math inline">\(t\)</span>-value, degrees of freedom, and <span class="math inline">\(p\)</span>-value as the test of the slope in our regression model.</p>
<p>We would get slightly different results when using the default Welch <span class="math inline">\(t\)</span>-test because it adjusts the degrees of freedom downward to correct for unequal variances, thus yielding a somewhat larger <span class="math inline">\(p\)</span>-value.</p>
</div>
<p>Let’s now see what happens to the output if we use the effect-coded version of the condition variable as a predictor instead of the dummy-coded version.</p>
<div class="alert alert-warning">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = score ~ cond_effect, data = df2)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.0977 -1.1347 -0.4077  0.7375  5.4223 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   8.1523     0.2405  33.898  &lt; 2e-16 ***
cond_effect   1.4893     0.4810   3.096  0.00302 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.863 on 58 degrees of freedom
Multiple R-squared:  0.1419,    Adjusted R-squared:  0.1271 
F-statistic: 9.588 on 1 and 58 DF,  p-value: 0.003016</code></pre>
</div>
</div>
</div>
<p>We should notice a few things: first, the regression weight <span class="math inline">\(b_1\)</span> and its associated test statistics (<span class="math inline">\(t\)</span>-value, <span class="math inline">\(p\)</span>-value) are identical. The same goes for the coefficient of determination <span class="math inline">\(R^2\)</span> and it associated <span class="math inline">\(F\)</span>-test. The only thing that differs is the intercept of the model and its associated test statistics. This makes sense, intuitively, because we simply shifted the predictor to the “left” by half a unit (from 0 vs.&nbsp;1 to -0.5 vs.&nbsp;05). Other than in the dummy-coded model, the intercept no longer reflects the mean of the reference group but the overall mean. The underlying information, namely the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, is unchanged, however. Accordingly, the strength of the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as well as its statistical significance cannot differ between the two versions of the regression model.</p>
</section>
<section id="predictors-with-more-then-two-levels" class="level3">
<h3 class="anchored" data-anchor-id="predictors-with-more-then-two-levels">Predictors with more then two levels</h3>
<p>While dichotomous predictors are easy to handle (particularly when we dummy-code them), things can get complicated when categorical predictors have three or more levels. What R does in these cases is create <span class="math inline">\(j-1\)</span> contrasts, where <span class="math inline">\(j\)</span> is the number of categories of the predictor <span class="math inline">\(X\)</span>. These contrasts are coded in a specific way. Similar to dummy-coded predictors, the first level s treated as the <strong>reference category</strong>, and each other level is compared to that reference category.</p>
<p>Let’s look at an example with a four-level predictor. Here, we predict the daily average sun hours in Northern Ireland from the four seasons (data are made up, of course).</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  obs season sun_hours
1   1 spring      0.90
2   2 summer      2.76
3   3 autumn      1.03
4   4 winter      0.02
5   5 spring      1.64
6   6 summer      2.50</code></pre>
</div>
</div>
<p>Here is the syntax for the regression model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># regresison model predicting sun hours from seasons</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">=</span> <span class="fu">lm</span>(<span class="at">formula =</span> sun_hours <span class="sc">~</span> season, <span class="at">data =</span> df3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s have a look at the console output:</p>
<div class="alert alert-warning">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = sun_hours ~ season, data = df3)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.9780 -0.1840 -0.0420  0.0975  1.2710 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    1.3080     0.1207  10.839 6.90e-13 ***
seasonsummer   1.6310     0.1707   9.557 2.05e-11 ***
seasonautumn  -0.3860     0.1707  -2.262   0.0298 *  
seasonwinter  -1.6620     0.1707  -9.739 1.25e-11 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.3816 on 36 degrees of freedom
Multiple R-squared:  0.9134,    Adjusted R-squared:  0.9062 
F-statistic: 126.5 on 3 and 36 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</div>
<p>In the data, seasons are coded such that spring is the first level, followed by summer, autumn, and winter. Therefore, the regression model treats spring as the reference category. The intercept tells us that spring in Northern Ireland has an average of 1.3 sun hours per day, which is significantly different from having zero sun. The regression weight for summer indicates that the sun shines for an additional 1.6 hours in summer compared with spring. Likewise, the regression weights for autumn and winter tell us that the daily sun hours are lower in autumn than in spring and also lower in winter than spring. Adding one of the regression weights to the intercept yields the mean hours of sunshine for that season (in the case of winter, we the model predicts a daily average of minus 0.3 sun hours in Northern Ireland, which seems about right).</p>
<p>Importantly, the model does not tell us whether average daily sun hours differ between summer and autumn, summer and winter, or autumn and winter. If we are interested in those differences, we need to run the model using a different reference category (for example by changing the factor levels and labels of the predictor variable accordingly).</p>
<div class="alert alert-info">
<p>You may have noticed that despite having only one predictor variable, the model now contains four parameters, the intercept and three regression weights. This is also reflected in the degrees of freedom (we lose one for each parameter).</p>
<p>Technically, this means that we have now entered the realm of multiple linear regression.</p>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>